# Table of contents

## Quickstart

* [🧭 Documentation Map](README.md)
* [Setting Up](quickstart/setting-up.md)
* [Supported SDKs](quickstart/supported-sdks.md)

## API REFERENCES

* [📒 All Model IDs](api-references/model-database.md)
* [Text Models (LLM)](api-references/text-models-llm/README.md)
  * [Alibaba Cloud](api-references/text-models-llm/Alibaba-Cloud/README.md)
    * [qwen-max](api-references/text-models-llm/Alibaba-Cloud/qwen-max.md)
    * [qwen-plus](api-references/text-models-llm/Alibaba-Cloud/qwen-plus.md)
    * [qwen-turbo](api-references/text-models-llm/Alibaba-Cloud/qwen-turbo.md)
    * [Qwen2-72B-Instruct](api-references/text-models-llm/Alibaba-Cloud/Qwen2-72B-Instruct.md)
    * [Qwen2.5-7B-Instruct-Turbo](api-references/text-models-llm/Alibaba-Cloud/Qwen2.5-7B-Instruct-Turbo.md)
    * [Qwen2.5-72B-Instruct-Turbo](api-references/text-models-llm/Alibaba-Cloud/Qwen2.5-72B-Instruct-Turbo.md)
    * [Qwen2.5-Coder-32B-Instruct](api-references/text-models-llm/Alibaba-Cloud/Qwen2.5-Coder-32B-Instruct.md)
    * [Qwen-QwQ-32B](api-references/text-models-llm/alibaba-cloud/qwen-qwq-32b.md)
    * [Qwen3-235B-A22B](api-references/text-models-llm/alibaba-cloud/qwen3-235b-a22b.md)
    * [qwen3-32b](api-references/text-models-llm/alibaba-cloud/qwen3-32b.md)
    * [qwen3-coder-480b-a35b-instruct](api-references/text-models-llm/alibaba-cloud/qwen3-coder-480b-a35b-instruct.md)
    * [qwen3-235b-a22b-thinking-2507](api-references/text-models-llm/alibaba-cloud/qwen3-235b-a22b-thinking-2507.md)
    * [qwen3-next-80b-a3b-instruct](api-references/text-models-llm/alibaba-cloud/qwen3-next-80b-a3b-instruct.md)
    * [qwen3-next-80b-a3b-thinking](api-references/text-models-llm/alibaba-cloud/qwen3-next-80b-a3b-thinking.md)
    * [qwen3-max-preview](api-references/text-models-llm/alibaba-cloud/qwen3-max-preview.md)
    * [qwen3-max-instruct](api-references/text-models-llm/alibaba-cloud/qwen3-max-instruct.md)
    * [qwen3-omni-30b-a3b-captioner](api-references/text-models-llm/alibaba-cloud/qwen3-omni-30b-a3b-captioner.md)
  * [Anthracite](api-references/text-models-llm/Anthracite/README.md)
    * [magnum-v4](api-references/text-models-llm/Anthracite/magnum-v4.md)
  * [Anthropic](api-references/text-models-llm/Anthropic/README.md)
    * [Claude 3 Haiku](api-references/text-models-llm/Anthropic/claude-3-haiku.md)
    * [Claude 3 Opus](api-references/text-models-llm/Anthropic/claude-3-opus.md)
    * [Claude 3.5 Haiku](api-references/text-models-llm/anthropic/claude-3.5-haiku.md)
    * [Claude 3.5 Sonnet](api-references/text-models-llm/Anthropic/claude-3.5-sonnet.md)
    * [Claude 3.7 Sonnet](api-references/text-models-llm/anthropic/claude-3.7-sonnet.md)
    * [Claude 4 Opus](api-references/text-models-llm/anthropic/claude-4-opus.md)
    * [Claude 4 Sonnet](api-references/text-models-llm/anthropic/claude-4-sonnet.md)
    * [Claude 4.1 Opus](api-references/text-models-llm/anthropic/claude-opus-4.1.md)
    * [Claude 4.5 Sonnet](api-references/text-models-llm/anthropic/claude-4-5-sonnet.md)
    * [Claude 4.5 Haiku](api-references/text-models-llm/anthropic/claude-4.5-haiku.md)
  * [Cohere](api-references/text-models-llm/Cohere/README.md)
    * [command-a](api-references/text-models-llm/cohere/command-a.md)
  * [DeepSeek](api-references/text-models-llm/DeepSeek/README.md)
    * [DeepSeek V3](api-references/text-models-llm/DeepSeek/deepseek-chat.md)
    * [DeepSeek R1](api-references/text-models-llm/DeepSeek/deepseek-r1.md)
    * [DeepSeek Prover V2](api-references/text-models-llm/deepseek/deepseek-prover-v2.md)
    * [DeepSeek Chat V3.1](api-references/text-models-llm/deepseek/deepseek-chat-v3.1.md)
    * [DeepSeek Reasoner V3.1](api-references/text-models-llm/deepseek/deepseek-reasoner-v3.1.md)
    * [Deepseek Reasoner V3.1 Terminus](api-references/text-models-llm/deepseek/deepseek-reasoner-v3.1-terminus.md)
    * [Deepseek Non-reasoner V3.1 Terminus](api-references/text-models-llm/deepseek/deepseek-non-reasoner-v3.1-terminus.md)
    * [DeepSeek V3.2 Exp Thinking](api-references/text-models-llm/deepseek/deepseek-reasoner-v3.2-exp-thinking.md)
    * [DeepSeek V3.2 Exp Non-thinking](api-references/text-models-llm/deepseek/deepseek-reasoner-v3.2-exp-non-thinking.md)
  * [Google](api-references/text-models-llm/Google/README.md)
    * [gemini-2.0-flash-exp](api-references/text-models-llm/Google/gemini-2.0-flash-exp.md)
    * [gemini-2.0-flash](api-references/text-models-llm/google/gemini-2.0-flash.md)
    * [gemini-2.5-flash-lite-preview](api-references/text-models-llm/google/gemini-2.5-flash-lite-preview.md)
    * [gemini-2.5-flash](api-references/text-models-llm/google/gemini-2.5-flash.md)
    * [gemini-2.5-pro](api-references/text-models-llm/google/gemini-2.5-pro.md)
    * [gemma-3](api-references/text-models-llm/google/gemma-3.md)
    * [gemma-3n-4b](api-references/text-models-llm/google/gemma-3n-4b.md)
  * [Meta](api-references/text-models-llm/Meta/README.md)
    * [Llama-3-chat-hf](api-references/text-models-llm/Meta/Llama-3-chat-hf.md)
    * [Llama-3-8B-Instruct-Lite](api-references/text-models-llm/Meta/Meta-Llama-3-8B-Instruct-Lite.md)
    * [Llama-3.1-8B-Instruct-Turbo](api-references/text-models-llm/Meta/Meta-Llama-3.1-8B-Instruct-Turbo.md)
    * [Llama-3.1-70B-Instruct-Turbo](api-references/text-models-llm/Meta/Meta-Llama-3.1-70B-Instruct-Turbo.md)
    * [Llama-3.1-405B-Instruct-Turbo](api-references/text-models-llm/Meta/Meta-Llama-3.1-405B-Instruct-Turbo.md)
    * [Llama-3.2-3B-Instruct-Turbo](api-references/text-models-llm/Meta/Llama-3.2-3B-Instruct-Turbo.md)
    * [Llama-3.3-70B-Instruct-Turbo](api-references/text-models-llm/Meta/Llama-3.3-70B-Instruct-Turbo.md)
    * [Llama-3.3-70B-Versatile](api-references/text-models-llm/meta/llama-3.3-70b-versatile.md)
    * [Llama-4-scout](api-references/text-models-llm/meta/llama-4-scout.md)
    * [Llama-4-maverick](api-references/text-models-llm/meta/llama-4-maverick.md)
  * [MiniMax](api-references/text-models-llm/MiniMax/README.md)
    * [text-01](api-references/text-models-llm/MiniMax/text-01.md)
    * [m1](api-references/text-models-llm/minimax/m1.md)
  * [Mistral AI](api-references/text-models-llm/Mistral-AI/README.md)
    * [codestral-2501](api-references/text-models-llm/Mistral-AI/codestral-2501.md)
    * [mistral-nemo](api-references/text-models-llm/Mistral-AI/mistral-nemo.md)
    * [mistral-tiny](api-references/text-models-llm/Mistral-AI/mistral-tiny.md)
    * [Mistral-7B-Instruct](api-references/text-models-llm/Mistral-AI/Mistral-7B-Instruct.md)
    * [Mixtral-8x7B-Instruct](api-references/text-models-llm/Mistral-AI/Mixtral-8x7B-Instruct-v0.1.md)
  * [Moonshot](api-references/text-models-llm/moonshot/README.md)
    * [kimi-k2-preview](api-references/text-models-llm/moonshot/kimi-k2-preview.md)
  * [NousResearch](api-references/text-models-llm/nousresearch/README.md)
    * [hermes-4-405b](api-references/text-models-llm/nousresearch/hermes-4-405b.md)
  * [NVIDIA](api-references/text-models-llm/NVIDIA/README.md)
    * [llama-3.1-nemotron-70b](api-references/text-models-llm/NVIDIA/llama-3.1-nemotron-70b.md)
  * [OpenAI](api-references/text-models-llm/OpenAI/README.md)
    * [gpt-3.5-turbo](api-references/text-models-llm/OpenAI/gpt-3.5-turbo.md)
    * [gpt-4](api-references/text-models-llm/OpenAI/gpt-4.md)
    * [gpt-4-preview](api-references/text-models-llm/OpenAI/gpt-4-preview.md)
    * [gpt-4-turbo](api-references/text-models-llm/OpenAI/gpt-4-turbo.md)
    * [gpt-4o](api-references/text-models-llm/OpenAI/gpt-4o.md)
    * [gpt-4o-mini](api-references/text-models-llm/OpenAI/gpt-4o-mini.md)
    * [gpt-4o-audio-preview](api-references/text-models-llm/openai/gpt-4o-audio-preview.md)
    * [gpt-4o-mini-audio-preview](api-references/text-models-llm/openai/gpt-4o-mini-audio-preview.md)
    * [gpt-4o-search-preview](api-references/text-models-llm/openai/gpt-4o-search-preview.md)
    * [gpt-4o-mini-search-preview](api-references/text-models-llm/openai/gpt-4o-mini-search-preview.md)
    * [o1](api-references/text-models-llm/OpenAI/o1.md)
    * [o1-mini](api-references/text-models-llm/OpenAI/o1-mini.md)
    * [o3](api-references/text-models-llm/openai/o3.md)
    * [o3-mini](api-references/text-models-llm/OpenAI/o3-mini.md)
    * [o3-pro](api-references/text-models-llm/openai/o3-pro.md)
    * [gpt-4.1](api-references/text-models-llm/openai/gpt-4.1.md)
    * [gpt-4.1-mini](api-references/text-models-llm/openai/gpt-4.1-mini.md)
    * [gpt-4.1-nano](api-references/text-models-llm/openai/gpt-4.1-nano.md)
    * [o4-mini](api-references/text-models-llm/openai/o4-mini.md)
    * [gpt-oss-20b](api-references/text-models-llm/openai/gpt-oss-20b.md)
    * [gpt-oss-120b](api-references/text-models-llm/openai/gpt-oss-120b.md)
    * [gpt-5](api-references/text-models-llm/openai/gpt-5.md)
    * [gpt-5-mini](api-references/text-models-llm/openai/gpt-5-mini.md)
    * [gpt-5-nano](api-references/text-models-llm/openai/gpt-5-nano.md)
    * [gpt-5-chat](api-references/text-models-llm/openai/gpt-5-chat.md)
    * [gpt-5-pro](api-references/text-models-llm/openai/gpt-5-pro.md)
  * [Perplexity](api-references/text-models-llm/perplexity/README.md)
    * [sonar](api-references/text-models-llm/perplexity/sonar.md)
    * [sonar-pro](api-references/text-models-llm/perplexity/sonar-pro.md)
  * [xAI](api-references/text-models-llm/xAI/README.md)
    * [grok-3-beta](api-references/text-models-llm/xai/grok-3-beta.md)
    * [grok-3-mini-beta](api-references/text-models-llm/xai/grok-3-mini-beta.md)
    * [grok-4](api-references/text-models-llm/xai/grok-4.md)
    * [grok-code-fast-1](api-references/text-models-llm/xai/grok-code-fast-1.md)
    * [grok-4-fast-non-reasoning](api-references/text-models-llm/xai/grok-4-fast-non-reasoning.md)
    * [grok-4-fast-reasoning](api-references/text-models-llm/xai/grok-4-fast-reasoning.md)
  * [Zhipu](api-references/text-models-llm/zhipu/README.md)
    * [glm-4.5-air](api-references/text-models-llm/zhipu/glm-4.5-air.md)
    * [glm-4.5](api-references/text-models-llm/zhipu/glm-4.5.md)
    * [glm-4.6](api-references/text-models-llm/zhipu/glm-4.6.md)
* [Image Models](api-references/image-models/README.md)
  * [Alibaba Cloud](api-references/image-models/alibaba-cloud/README.md)
    * [qwen-image](api-references/image-models/alibaba-cloud/qwen-image.md)
    * [qwen-image-edit](api-references/image-models/alibaba-cloud/qwen-image-edit.md)
  * [ByteDance](api-references/image-models/bytedance/README.md)
    * [Seedream 3.0](api-references/image-models/bytedance/seedream-3.0.md)
    * [Seededit 3.0 (Image-to-Image)](api-references/image-models/bytedance/seededit-3.0-image-to-image.md)
    * [Seedream v4 (Text-to-Image)](api-references/image-models/bytedance/seedream-v4-text-to-image.md)
    * [Seedream v4 Edit (Image-to-image)](api-references/image-models/bytedance/seedream-v4-edit-image-to-image.md)
    * [USO (Image-to-Image)](api-references/image-models/bytedance/uso.md)
  * [Flux](api-references/image-models/flux/README.md)
    * [flux-pro](api-references/image-models/flux/flux-pro.md)
    * [flux-pro/v1.1](api-references/image-models/flux/flux-pro-v1.1.md)
    * [flux-pro/v1.1-ultra](api-references/image-models/flux/flux-pro-v1.1-ultra.md)
    * [flux-realism](api-references/image-models/flux/flux-realism.md)
    * [flux/dev](api-references/image-models/flux/flux-dev.md)
    * [flux/dev/image-to-image](api-references/image-models/flux/flux-dev-image-to-image.md)
    * [flux/schnell](api-references/image-models/flux/flux-schnell.md)
    * [flux/kontext-max/text-to-image](api-references/image-models/flux/flux-kontext-max-text-to-image.md)
    * [flux/kontext-max/image-to-image](api-references/image-models/flux/flux-kontext-max-image-to-image.md)
    * [flux/kontext-pro/text-to-image](api-references/image-models/flux/flux-kontext-pro-text-to-image.md)
    * [flux/kontext-pro/image-to-image](api-references/image-models/flux/flux-kontext-pro-image-to-image.md)
    * [flux/srpo/text-to-image](api-references/image-models/flux/flux-srpo-text-to-image.md)
    * [flux/srpo/image-to-image](api-references/image-models/flux/flux-srpo-image-to-image.md)
  * [Google](api-references/image-models/google/README.md)
    * [Imagen 3](api-references/image-models/google/imagen-3.0.md)
    * [Imagen 4 Preview](api-references/image-models/google/imagen-4-preview.md)
    * [Imagen 4 Ultra Preview](api-references/image-models/google/imagen-4-ultra.md)
    * [Imagen 4 Generate](api-references/image-models/google/imagen-4-generate.md)
    * [Imagen 4 Fast Generate](api-references/image-models/google/imagen-4-fast-generate.md)
    * [Imagen 4 Ultra Generate](api-references/image-models/google/imagen-4-ultra-generate.md)
    * [Gemini 2.5 Flash Image](api-references/image-models/google/gemini-2.5-flash-image.md)
    * [Gemini 2.5 Flash Image Edit](api-references/image-models/google/gemini-2.5-flash-image-edit.md)
  * [OpenAI](api-references/image-models/OpenAI/README.md)
    * [DALL·E 2](api-references/image-models/OpenAI/dall-e-2.md)
    * [DALL·E 3](api-references/image-models/OpenAI/dall-e-3.md)
    * [gpt-image-1](api-references/image-models/openai/gpt-image-1.md)
  * [RecraftAI](api-references/image-models/RecraftAI/README.md)
    * [Recraft v3](api-references/image-models/RecraftAI/recraft-v3.md)
  * [Reve](api-references/image-models/reve/README.md)
    * [reve/create-image](api-references/image-models/reve/reve-create-image.md)
    * [reve/edit-image](api-references/image-models/reve/reve-edit-image.md)
    * [reve/remix-edit-image](api-references/image-models/reve/reve-remix-edit-image.md)
  * [Stability AI](api-references/image-models/Stability-AI/README.md)
    * [Stable Diffusion v3 Medium](api-references/image-models/Stability-AI/stable-diffusion-v3-medium.md)
    * [Stable Diffusion v3.5 Large](api-references/image-models/Stability-AI/stable-diffusion-v35-large.md)
  * [Tencent](api-references/image-models/tencent/README.md)
    * [Hunyuan Image V3](api-references/image-models/tencent/hunyuan-image-v3-text-to-image.md)
* [Video Models](api-references/video-models/README.md)
  * [Alibaba Cloud](api-references/video-models/alibaba-cloud/README.md)
    * [Wan 2.1 Plus (Text-to-Video)](api-references/video-models/alibaba-cloud/wan-2.1-plus-text-to-video.md)
    * [Wan 2.1 Turbo (Text-to-Video)](api-references/video-models/alibaba-cloud/wan-2.1-turbo-text-to-video.md)
    * [Wan 2.2 Plus (Text-to-Video)](api-references/video-models/alibaba-cloud/wan-2.2-plus-text-to-video.md)
    * [Wan 2.2 Plus (Image-to-Video)](api-references/video-models/alibaba-cloud/wan-2.2-plus-image-to-video.md)
    * [Wan 2.2 Animate Replace (Image-to-Video)](api-references/video-models/alibaba-cloud/wan-2.2-14b-animate-replace-image-to-video.md)
    * [Wan 2.2 Animate Move (Image-to-Video)](api-references/video-models/alibaba-cloud/wan-2.2-14b-animate-move-image-to-video.md)
    * [Wan 2.2 VACE Fun Reframe (Image-to-Video)](api-references/video-models/alibaba-cloud/wan2.2-vace-fun-a14b-reframe-image-to-video.md)
    * [Wan 2.2 VACE Fun Outpainting (Image-to-Video)](api-references/video-models/alibaba-cloud/wan2.2-vace-fun-a14b-outpainting-image-to-video.md)
    * [Wan 2.2 VACE Fun Inpainting (Image-to-Video)](api-references/video-models/alibaba-cloud/wan2.2-vace-fun-a14b-inpainting-image-to-video.md)
    * [Wan 2.2 VACE Fun Pose (Image-to-Video)](api-references/video-models/alibaba-cloud/wan2.2-vace-fun-a14b-pose-image-to-video.md)
    * [Wan 2.2 VACE Fun Depth (Image-to-Video)](api-references/video-models/alibaba-cloud/wan2.2-vace-fun-a14b-depth-image-to-video.md)
    * [Wan 2.5 Preview (Text-to-Video)](api-references/video-models/alibaba-cloud/wan-2.5-preview-text-to-video.md)
    * [Wan 2.5 Preview (Image-to-Video)](api-references/video-models/alibaba-cloud/wan-2.5-preview-image-to-video.md)
  * [ByteDance](api-references/video-models/bytedance/README.md)
    * [Seedance 1.0 lite (Text-to-Video)](api-references/video-models/bytedance/seedance-1.0-lite-text-to-video.md)
    * [Seedance 1.0 lite (Image-to-Video)](api-references/video-models/bytedance/seedance-1.0-lite-image-to-video.md)
    * [Seedance 1.0 pro (Text-to-Video)](api-references/video-models/bytedance/seedance-1.0-pro-text-to-video.md)
    * [Seedance 1.0 pro (Image-to-Video)](api-references/video-models/bytedance/seedance-1.0-pro-image-to-video.md)
    * [OmniHuman](api-references/video-models/bytedance/omnihuman.md)
  * [Google](api-references/video-models/google/README.md)
    * [Veo 2 (Text-to-Video)](api-references/video-models/google/veo2-text-to-video.md)
    * [Veo 2 (Image-to-Video)](api-references/video-models/google/veo2-image-to-video.md)
    * [Veo 3 (Text-to-Video)](api-references/video-models/google/veo3-text-to-video.md)
    * [Veo 3 (Image-to-Video)](api-references/video-models/google/veo-3-image-to-video.md)
    * [Veo 3 Fast (Text-to-Video)](api-references/video-models/google/veo-3-fast-text-to-video.md)
    * [Veo 3 Fast (Image-to-Video)](api-references/video-models/google/veo-3-fast-image-to-video.md)
    * [Veo 3.1 (Text-to-Video)](api-references/video-models/google/veo-3-1-text-to-video.md)
    * [Veo 3.1 (Image-to-Video)](api-references/video-models/google/veo-3-1-image-to-video.md)
    * [Veo 3.1 (First-Last-Image-to-Video)](api-references/video-models/google/veo-3-1-first-last-image-to-video.md)
    * [Veo 3.1 (Reference-to-Video)](api-references/video-models/google/veo-3-1-reference-to-video.md)
    * [Veo 3.1 Fast (Text-to-Video)](api-references/video-models/google/veo-3-1-text-to-video-fast.md)
    * [Veo 3.1 Fast (Image-to-Video)](api-references/video-models/google/veo-3-1-image-to-video-fast.md)
    * [Veo 3.1 Fast (First-Last-Image-to-Video)](api-references/video-models/google/veo-3-1-first-last-image-to-video-fast.md)
  * [Kling AI](api-references/video-models/Kling-AI/README.md)
    * [v1-standard/image-to-video](api-references/video-models/Kling-AI/v1-standard-image-to-video.md)
    * [v1-standard/text-to-video](api-references/video-models/Kling-AI/v1-standard-text-to-video.md)
    * [v1-pro/image-to-video](api-references/video-models/Kling-AI/v1-pro-image-to-video.md)
    * [v1-pro/text-to-video](api-references/video-models/Kling-AI/v1-pro-text-to-video.md)
    * [v1.6-standard/image-to-video](api-references/video-models/Kling-AI/v1.6-standart-image-to-video.md)
    * [v1.6-standard/text-to-video](api-references/video-models/Kling-AI/v1.6-standard-text-to-video.md)
    * [v1.6-standard/multi-image-to-video](api-references/video-models/kling-ai/v1.6-standard-multi-image-to-video.md)
    * [v1.6-pro/image-to-video](api-references/video-models/Kling-AI/v1.6-pro-image-to-video.md)
    * [v1.6-pro/text-to-video](api-references/video-models/kling-ai/v1.6-pro-text-to-video.md)
    * [v1.6-standard/effects](api-references/video-models/kling-ai/v1.6-standard-effects.md)
    * [v1.6-pro/effects](api-references/video-models/kling-ai/v1.6-pro-effects.md)
    * [v2-master/image-to-video](api-references/video-models/kling-ai/v2-master-image-to-video.md)
    * [v2-master/text-to-video](api-references/video-models/kling-ai/v2-master-text-to-video.md)
    * [v2.1-standard/image-to-video](api-references/video-models/kling-ai/v2.1-standard-image-to-video.md)
    * [v2.1-pro/image-to-video](api-references/video-models/kling-ai/v2.1-pro-image-to-video.md)
    * [v2.1-master-text-to-video](api-references/video-models/kling-ai/v2.1-master-text-to-video.md)
    * [v2.1-master-image-to-video](api-references/video-models/kling-ai/v2.1-master-image-to-video.md)
    * [v2.5-turbo/pro/text-to-video](api-references/video-models/kling-ai/v2.5-turbo-pro-text-to-video.md)
    * [v2.5-turbo/pro/image-to-video](api-references/video-models/kling-ai/v2.5-turbo-pro-image-to-video.md)
  * [Luma AI](api-references/video-models/luma-ai/README.md)
    * [Text-to-Video v2](api-references/video-models/luma-ai/luma-ai-v2.md)
    * [Text-to-Video v1 (legacy)](api-references/video-models/luma-ai/luma-ai-text-to-video-v1-legacy.md)
  * [MiniMax](api-references/video-models/MiniMax/README.md)
    * [video-01](api-references/video-models/MiniMax/video-01.md)
    * [video-01-live2d](api-references/video-models/MiniMax/video-01-live2d.md)
    * [hailuo-02](api-references/video-models/minimax/hailuo-02.md)
  * [OpenAI](api-references/video-models/openai/README.md)
    * [sora-2-t2v](api-references/video-models/openai/sora-2-t2v.md)
    * [sora-2-i2v](api-references/video-models/openai/sora-2-i2v.md)
    * [sora-2-pro-t2v](api-references/video-models/openai/sora-2-pro-t2v.md)
    * [sora-2-pro-i2v](api-references/video-models/openai/sora-2-pro-i2v.md)
  * [PixVerse](api-references/video-models/pixverse/README.md)
    * [v5/text-to-video](api-references/video-models/pixverse/v5-text-to-video.md)
    * [v5/image-to-video](api-references/video-models/pixverse/v5-image-to-video.md)
    * [v5/transition](api-references/video-models/pixverse/v5-transition.md)
  * [Runway](api-references/video-models/runway/README.md)
    * [gen3a\_turbo](api-references/video-models/runway/gen3a_turbo.md)
    * [gen4\_turbo](api-references/video-models/runway/gen4_turbo.md)
    * [gen4\_aleph](api-references/video-models/runway/gen4_aleph.md)
    * [act\_two](api-references/video-models/runway/act_two.md)
  * [Sber AI](api-references/video-models/sber-ai/README.md)
    * [Kandinsky 5 (Text-to-Video)](api-references/video-models/sber-ai/kandinsky5-text-to-video.md)
    * [Kandinsky 5 Distill (Text-to-Video)](api-references/video-models/sber-ai/kandinsky5-distill-text-to-video.md)
  * [Veed](api-references/video-models/veed/README.md)
    * [fabric-1.0](api-references/video-models/veed/fabric-1.0.md)
    * [fabric-1.0-fast](api-references/video-models/veed/fabric-1.0-fast.md)
* [Music Models](api-references/music-models/README.md)
  * [ElevenLabs](api-references/music-models/elevenlabs/README.md)
    * [eleven\_music](api-references/music-models/elevenlabs/eleven_music.md)
  * [Google](api-references/music-models/google/README.md)
    * [Lyria 2](api-references/music-models/google/lyria-2.md)
  * [MiniMax](api-references/music-models/MiniMax/README.md)
    * [minimax-music \[legacy\]](api-references/music-models/MiniMax/minimax-music-\[legacy].md)
    * [music-01](api-references/music-models/MiniMax/music-01.md)
  * [Stability AI](api-references/music-models/Stability-AI/README.md)
    * [stable-audio](api-references/music-models/Stability-AI/stable-audio.md)
* [Voice/Speech Models](api-references/speech-models/README.md)
  * [Speech-to-Text](api-references/speech-models/speech-to-text/README.md)
    * [stt \[legacy\]](api-references/speech-models/speech-to-text/stt-legacy.md)
    * [Assembly AI](api-references/speech-models/speech-to-text/assembly-ai/README.md)
      * [slam-1](api-references/speech-models/speech-to-text/assembly-ai/slam-1.md)
      * [universal](api-references/speech-models/speech-to-text/assembly-ai/universal.md)
    * [Deepgram](api-references/speech-voice-models/stt/Deepgram/README.md)
      * [nova-2](api-references/speech-voice-models/stt/Deepgram/nova-2.md)
    * [OpenAI](api-references/speech-voice-models/stt/OpenAI/README.md)
      * [whisper-base](api-references/speech-voice-models/stt/OpenAI/whisper-base.md)
      * [whisper-large](api-references/speech-voice-models/stt/OpenAI/whisper-large.md)
      * [whisper-medium](api-references/speech-voice-models/stt/OpenAI/whisper-medium.md)
      * [whisper-small](api-references/speech-voice-models/stt/OpenAI/whisper-small.md)
      * [whisper-tiny](api-references/speech-voice-models/stt/OpenAI/whisper-tiny.md)
  * [Text-to-Speech](api-references/speech-models/text-to-speech/README.md)
    * [Alibaba Cloud](api-references/speech-models/text-to-speech/alibaba-cloud/README.md)
      * [qwen3-tts-flash](api-references/speech-models/text-to-speech/alibaba-cloud/qwen3-tts-flash.md)
    * [Deepgram](api-references/speech-voice-models/tts/Deepgram/README.md)
      * [aura](api-references/speech-voice-models/tts/Deepgram/aura.md)
    * [ElevenLabs](api-references/speech-models/text-to-speech/elevenlabs/README.md)
      * [eleven\_multilingual\_v2](api-references/speech-models/text-to-speech/elevenlabs/eleven_multilingual_v2.md)
      * [eleven\_turbo\_v2\_5](api-references/speech-models/text-to-speech/elevenlabs/eleven_turbo_v2_5.md)
    * [Microsoft](api-references/speech-models/text-to-speech/microsoft/README.md)
      * [vibevoice-1.5b](api-references/speech-models/text-to-speech/microsoft/vibevoice-1.5b.md)
      * [vibevoice-7b](api-references/speech-models/text-to-speech/microsoft/vibevoice-7b.md)
    * [OpenAI](api-references/speech-models/text-to-speech/openai/README.md)
      * [TTS-1](api-references/speech-models/text-to-speech/openai/tts-1.md)
      * [TTS-1 HD](api-references/speech-models/text-to-speech/openai/tts-1-hd.md)
      * [gpt-4o-mini-tts](api-references/speech-models/text-to-speech/openai/gpt-4o-mini-tts.md)
  * [Voice Chat](api-references/speech-models/voice-chat/README.md)
    * [ElevenLabs](api-references/speech-models/voice-chat/elevenlabs/README.md)
      * [v3\_alpha](api-references/speech-models/voice-chat/elevenlabs/v3_alpha.md)
    * [MiniMax](api-references/speech-models/voice-chat/minimax/README.md)
      * [Speech 2.5 Turbo Preview](api-references/speech-models/voice-chat/minimax/speech-2.5-turbo-preview.md)
      * [Speech 2.5 HD Preview](api-references/speech-models/voice-chat/minimax/speech-2.5-hd-preview.md)
* [Content Moderation Models](api-references/moderation-safety-models/README.md)
  * [Meta](api-references/moderation-safety-models/Meta/README.md)
    * [Llama-Guard-3-11B-Vision-Turbo](api-references/moderation-safety-models/Meta/Llama-Guard-3-11B-Vision-Turbo.md)
    * [LlamaGuard-2-8b](api-references/moderation-safety-models/Meta/LlamaGuard-2-8b.md)
    * [Meta-Llama-Guard-3-8B](api-references/moderation-safety-models/Meta/Meta-Llama-Guard-3-8B.md)
* [3D-Generating Models](api-references/3d-generating-models/README.md)
  * [Stability AI](api-references/3d-generating-models/Stability-AI/README.md)
    * [triposr](api-references/3d-generating-models/Stability-AI/triposr.md)
    * [Copy of triposr](api-references/3d-generating-models/stability-ai/triposr-1.md)
* [Vision Models](api-references/vision-models/README.md)
  * [Image Analysis](api-references/vision-models/image-analysis.md)
  * [OCR: Optical Character Recognition](api-references/vision-models/ocr-optical-character-recognition/README.md)
    * [Google](api-references/vision-models/ocr-optical-character-recognition/google/README.md)
      * [Google OCR](api-references/vision-models/ocr-optical-character-recognition/google/google-ocr.md)
    * [Mistral AI](api-references/vision-models/ocr-optical-character-recognition/mistral-ai/README.md)
      * [mistral-ocr-latest](api-references/vision-models/ocr-optical-character-recognition/mistral-ai/mistral-ocr-latest.md)
  * [OFR: Optical Feature Recognition](api-references/vision-models/ofr-optical-feature-recognition.md)
* [Embedding Models](api-references/embedding-models/README.md)
  * [Anthropic](api-references/embedding-models/Anthropic/README.md)
    * [voyage-2](api-references/embedding-models/Anthropic/voyage-2.md)
    * [voyage-code-2](api-references/embedding-models/Anthropic/voyage-code-2.md)
    * [voyage-finance-2](api-references/embedding-models/Anthropic/voyage-finance-2.md)
    * [voyage-large-2](api-references/embedding-models/Anthropic/voyage-large-2.md)
    * [voyage-large-2-instruct](api-references/embedding-models/Anthropic/voyage-large-2-instruct.md)
    * [voyage-law-2](api-references/embedding-models/Anthropic/voyage-law-2.md)
    * [voyage-multilingual-2](api-references/embedding-models/Anthropic/voyage-multilingual-2.md)
  * [BAAI](api-references/embedding-models/BAAI/README.md)
    * [bge-base-en](api-references/embedding-models/BAAI/bge-base-en.md)
    * [bge-large-en](api-references/embedding-models/BAAI/bge-large-en.md)
  * [Google](api-references/embedding-models/Google/README.md)
    * [textembedding-gecko](api-references/embedding-models/Google/textembedding-gecko.md)
    * [text-multilingual-embedding-002](api-references/embedding-models/Google/text-multilingual-embedding-002.md)
  * [OpenAI](api-references/embedding-models/OpenAI/README.md)
    * [text-embedding-3-large](api-references/embedding-models/OpenAI/text-embedding-3-large.md)
    * [text-embedding-3-small](api-references/embedding-models/OpenAI/text-embedding-3-small.md)
    * [text-embedding-ada-002](api-references/embedding-models/OpenAI/text-embedding-ada-002.md)
  * [Together AI](api-references/embedding-models/Together-AI/README.md)
    * [m2-bert-80M-retrieval](api-references/embedding-models/Together-AI/m2-bert-80M-retrieval.md)

## Solutions

* [Bagoodex](solutions/bagoodex/README.md)
  * [AI Search Engine](solutions/ai-search-engine/README.md)
    * [Find Links](solutions/ai-search-engine/find-links.md)
    * [Find Images](solutions/ai-search-engine/find-images.md)
    * [Find Videos](solutions/ai-search-engine/find-videos.md)
    * [Find the Weather](solutions/ai-search-engine/find-the-weather.md)
    * [Find a Local Map](solutions/ai-search-engine/find-a-local-map.md)
    * [Get a Knowledge Structure](solutions/ai-search-engine/get-a-knowledge-structure.md)
* [OpenAI](solutions/openai/README.md)
  * [Assistants](solutions/openai/assistants/README.md)
    * [Assistant API](solutions/openai/assistants/assistant-api.md)
    * [Thread API](solutions/openai/assistants/threads.md)
    * [Message API](solutions/openai/assistants/messages.md)
    * [Run and Run Step API](solutions/openai/assistants/runs.md)
    * [Events](solutions/openai/assistants/events.md)
    * [Examples of Using Assistants](solutions/openai/assistants/using-assistants-and-threads.md)

## Use Cases

* [Create Images: Illustrate an Article](use-cases/create-images-illustrate-an-article.md)
* [Animate Images: A Children’s Encyclopedia](use-cases/animate-images-a-childrens-encyclopedia.md)
* [Create an Assistant to Discuss a Specific Document](use-cases/create-an-assistant-to-discuss-a-specific-document.md)
* [Create a 3D Model from an Image](use-cases/create-a-3d-model-from-an-image.md)
* [Create a Looped GIF for a Web Banner](use-cases/create-a-looped-gif-for-a-web-banner.md)
* [Read Text Aloud and Describe Images: Support People with Visual Impairments](use-cases/read-text-aloud-and-describe-images-ai-tool-to-support-people-with-visual-impairments.md)
* [Find Relevant Answers: Semantic Search with Text Embeddings](use-cases/find-relevant-answers-semantic-search-with-text-embeddings.md)
* [Summarize Websites with AI-Powered Chrome Extension](use-cases/summarize-websites-with-ai-powered-chrome-extension.md)

## Capabilities

* [Completion and Chat Completion](capabilities/completion-or-chat-models.md)
* [Streaming Mode](capabilities/streaming-mode.md)
* [Code Generation](capabilities/code-generation.md)
* [Thinking / Reasoning](capabilities/thinking-reasoning.md)
* [Function Calling](capabilities/function-calling.md)
* [Vision in Text Models (Image-To-Text)](capabilities/image-to-text-vision.md)
* [Web Search](capabilities/web-search.md)
* [Features of Anthropic Models](capabilities/anthropic.md)
* [Model comparison](capabilities/models-comparsion.md)

## FAQ

* [Can I use API in Python?](faq/can-i-use-api-in-python.md)
* [Can I use API in NodeJS?](faq/can-i-use-api-in-nodejs.md)
* [What are the Pro Models?](faq/pro-models.md)
* [How to use the Free Tier?](faq/free-tier.md)
* [Is API down or it just me?](faq/is-api-down-or-it-just-me.md)
* [Are my requests cropped?](faq/my-requests-are-cropped.md)
* [Can I call API in the asynchronous mode?](faq/call-api-in-the-asynchronous-mode.md)
* [OpenAI SDK doesn't work?](faq/openai-sdk-doesnt-work.md)
* [qp](faq/qp.md)

***

* [📞 Contact Sales](https://calendly.com/aimlapi/30min)
* [🗯️ Send Feedback](https://forms.aimlapi.com/doc)
* [404 Model is deprecated](404-model-is-deprecated.md)

## Errors and Messages

* [General Info](errors-and-messages/general-info.md)
* [Errors with status code 4xx](errors-and-messages/errors-with-status-code-4xx.md)
* [Errors with status code 5xx](errors-and-messages/errors-with-status-code-5xx.md)

## Glossary

* [Concepts](glossary/concepts.md)

## Integrations

* [Our Integration List](integrations/our-integration-list.md)
* [Agno](integrations/agno.md)
* [AutoGPT](integrations/autogpt.md)
* [Cline](integrations/cline.md)
* [continue.dev](integrations/continue.dev.md)
* [Cursor](integrations/cursor.md)
* [ElizaOS](integrations/elizaos.md)
* [GPT Researcher (gptr)](integrations/gpt-researcher-gptr.md)
* [Langflow](integrations/langflow.md)
* [LiteLLM](integrations/litellm.md)
* [Make](integrations/make.md)
* [n8n](integrations/n8n.md)
* [Roo Code](integrations/roo-code.md)
* [793](integrations/793.md)
* [SillyTavern](integrations/sillytavern.md)
* [Toolhouse](integrations/toolhouse.md)
