---
icon: list-timeline
---

# Find Relevant Answers: Semantic Search with Text Embeddings

## Idea and Step-by-Step Plan

Today, we are going to use [text embeddings](../api-references/embedding-models/) to transform a list of phrases into vectors. When a user asks a question, we will convert it into a vector as well and find the phrases from the list that are semantically closest. This approach is useful, for example, to immediately suggest relevant FAQ sections to the user and reduce the need for full support requests.

So, here's a plan:

1. **Prepare the data:** Create a numbered list of text phrases.
2. **Generate embeddings:** Use a model to embed each phrase into a vector.
3. **Embed the question:** When the user asks something, embed the question text.
4. **Find similar phrases:** Calculate the similarity (e.g., cosine similarity) between the question vector and the list vectors. Show the top 1–3 most similar phrases as the answer.

## Full Walkthrough <a href="#full-walkthrough" id="full-walkthrough"></a>

### 1. Prepare the data

We have compiled the following list of FAQ headings:

```
"How to grow tomatoes at home",
"Learning about birds",
"Best practices for machine learning models",
"How to train a dog",
"Tips for painting landscapes",
"Learning Python for data analysis",
"Everyday Life of a Cynologist"
```

### 2. Generate embeddings

Let's save our headings as a list and pass them to the model. We chose the [text-embedding-3-large](../api-references/embedding-models/OpenAI/text-embedding-3-large.md) model — it has been trained on a large dataset and is powerful enough to build complex semantic connections.&#x20;

Now each of our headings has a corresponding embedding vector.

### 3. Embed the question

Similarly, we process the user's query. We save the embedding vector generated by the model into a separate variable.

### 4. Find similar phrases

We calculate the similarity between the question vector and the list vectors.

There are different metrics and functions you can use for this, such as cosine similarity, dot product, or Euclidean distance.

In this example, we use cosine similarity because it measures the angle between two vectors and is a popular choice for comparing text embeddings, especially when the magnitude of the vectors is less important than their direction.

Please note that to use the cosine similarity function, you need to install the [scikit-learn](https://pypi.org/project/scikit-learn/) library separately. You can install it with the following command:

```shell
pip install scikit-learn
```

## Full Code Example & Results

In this section, you will find the complete Python code for the described use case, along with an example of the program's output.&#x20;

{% hint style="success" %}
Do not forget to replace `<YOUR_AIMLAPI_KEY>` with your actual AI/ML API key from [your account](https://aimlapi.com/app/keys) on our platform.
{% endhint %}

<details>

<summary>Python code</summary>

{% code overflow="wrap" %}
```python
import numpy as np
from openai import OpenAI
from sklearn.metrics.pairwise import cosine_similarity

# Initialize the API client
client = OpenAI(
    base_url="https://api.aimlapi.com/v2",
    api_key="<YOUR_AIMLAPI_KEY>",
)

# Example list of headings
items = [
    "How to grow tomatoes at home",
    "Learning about birds",
    "Best practices for machine learning models",
    "How to train a dog",
    "Tips for painting landscapes",
    "Learning Python for data analysis",
    "Everyday Life of a Cynologist"
]

# Generate embeddings for each phrase in the list
response = client.embeddings.create(
    model="text-embedding-3-large",  # Choose your fighter :)
    input=items
)

item_embeddings = np.array([e.embedding for e in response.data])

# When a user submits a question
query = "How to teach pets new tricks?"

# Generate an embedding for the user's question
query_response = client.embeddings.create(
    model="text-embedding-3-large",
    input=[query]
)
query_embedding = np.array(query_response.data[0].embedding)

# Calculate cosine similarity between the user question and each phrase
similarities = cosine_similarity([query_embedding], item_embeddings)[0]

# Find the indices of the most similar phrases
top_indices = similarities.argsort()[::-1]  # Sort in descending order

print("Query:", query)
print("\nMost similar items:")

for idx in top_indices[:3]:  # Show the top 3 most similar phrases
    print(f"- {items[idx]} (similarity: {similarities[idx]:.3f})")

```
{% endcode %}

</details>

<details>

<summary>Response when using a large embedding model</summary>

{% code overflow="wrap" %}
```json5
Query: How to teach pets new tricks?

Most similar items:
- How to train a dog (similarity: 0.590)
- Everyday Life of a Cynologist (similarity: 0.281)
- Learning about birds (similarity: 0.255)
```
{% endcode %}

</details>

Here is the program output after we switched to the small version of the model, [text-embedding-3-small](../api-references/embedding-models/OpenAI/text-embedding-3-small.md):

<details>

<summary>Response when using a small embedding model</summary>

{% code overflow="wrap" %}
```json5
Query: How to teach pets new tricks?

Most similar items:
- How to train a dog (similarity: 0.534)
- Learning about birds (similarity: 0.322)
- Tips for painting landscapes (similarity: 0.244)
```
{% endcode %}

</details>

Apparently, it was trained a bit less thoroughly and doesn't recognize who cynologists are :person\_shrugging: \
We didn't notice much difference in speed, but the larger version is somewhat more expensive.

## Room for Improvement <a href="#room-for-improvement" id="room-for-improvement"></a>

Naturally, this is a simplified example. You can develop a more comprehensive implementation by introducing features such as:

* **Add a minimum similarity threshold** to filter out irrelevant results,
* **Cache embeddings** for faster lookup without recalculating them each time,
* **Allow partial matches** or fuzzy search for broader results,
* **Handle multiple user questions at once** (batch processing) — and more.
