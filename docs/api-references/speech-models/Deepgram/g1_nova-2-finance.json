{"paths":{"/v1/stt/create":{"post":{"operationId":"VoiceModelsController_createSpeechToText_v1","parameters":[],"requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","properties":{"model":{"enum":["#g1_nova-2-finance"]},"custom_intent":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}}],"description":"A custom intent you want the model to detect within your input audio if present. Submit up to 100."},"custom_topic":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}}],"description":"A custom topic you want the model to detect within your input audio if present. Submit up to 100."},"custom_intent_mode":{"type":"string","enum":["strict","extended"],"description":"Sets how the model will interpret strings submitted to the custom_intent param. When strict, the model will only return intents submitted using the custom_intent param. When extended, the model will return its own detected intents in addition those submitted using the custom_intents param."},"custom_topic_mode":{"type":"string","enum":["strict","extended"],"description":"Sets how the model will interpret strings submitted to the custom_topic param. When strict, the model will only return topics submitted using the custom_topic param. When extended, the model will return its own detected topics in addition to those submitted using the custom_topic param."},"detect_language":{"type":"boolean","description":"Enables language detection to identify the dominant language spoken in the submitted audio."},"detect_entities":{"type":"boolean","description":"When Entity Detection is enabled, the Punctuation feature will be enabled by default."},"detect_topics":{"type":"boolean","description":"Detects the most important and relevant topics that are referenced in speech within the audio"},"diarize":{"type":"boolean","description":"Recognizes speaker changes. Each word in the transcript will be assigned a speaker number starting at 0"},"dictation":{"type":"boolean","description":"Identifies and extracts key entities from content in submitted audio"},"diarize_version":{"type":"string","description":""},"extra":{"type":"string","description":"Arbitrary key-value pairs that are attached to the API response for usage in downstream processing"},"filler_words":{"type":"boolean","description":"Filler Words can help transcribe interruptions in your audio, like “uh” and “um”"},"intents":{"type":"boolean","description":"Recognizes speaker intent throughout a transcript or text"},"keywords":{"type":"string","description":"Keywords can boost or suppress specialized terminology and brands"},"language":{"type":"string","description":"The BCP-47 language tag that hints at the primary spoken language. Depending on the Model and API endpoint you choose only certain languages are available"},"measurements":{"type":"boolean","description":"Spoken measurements will be converted to their corresponding abbreviations"},"multi_channel":{"type":"boolean","description":"Transcribes each audio channel independently"},"numerals":{"type":"boolean","description":"Numerals converts numbers from written format to numerical format"},"paragraphs":{"type":"boolean","description":"Splits audio into paragraphs to improve transcript readability"},"profanity_filter":{"type":"boolean","description":"Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely"},"punctuate":{"type":"boolean","description":"Adds punctuation and capitalization to the transcript"},"search":{"type":"string","description":"Search for terms or phrases in submitted audio"},"sentiment":{"type":"boolean","description":"Recognizes the sentiment throughout a transcript or text"},"smart_format":{"type":"boolean","description":"Applies formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability"},"summarize":{"type":"string","description":"Summarizes content. For Listen API, supports string version option. For Read API, accepts boolean only."},"tag":{"type":"array","items":{"type":"string"},"description":"Labels your requests for the purpose of identification during usage reporting"},"topics":{"type":"boolean","description":"Detects topics throughout a transcript or text"},"utterances":{"type":"boolean","description":"Segments speech into meaningful semantic units"},"utt_split":{"type":"number","description":"Seconds to wait before detecting a pause between words in submitted audio"}},"required":["model"]}}}},"responses":{"201":{"description":"","content":{"application/json":{"schema":{"$ref":"#/components/schemas/Voice.v1.SpeechToTextCreateResponseDTO"}}}}},"tags":["Voice Models"],"security":[{"access-token":[]}]}}},"openapi":"3.0.0","info":{"title":"AI/ML Gateway","description":"","version":"1.0","contact":{}},"tags":[],"servers":[{"url":"https://api.aimlapi.com"}],"components":{"securitySchemes":{"access-token":{"scheme":"bearer","bearerFormat":"<YOUR_AIMLAPI_KEY>","type":"http","in":"header","description":"Bearer key"}},"schemas":{"Image.v1.GenerateImageDTO":{"anyOf":[{"type":"object","properties":{"model":{"type":"string","enum":["dall-e-3","dall-e-2"],"default":"dall-e-2"},"prompt":{"type":"string","maxLength":4000,"description":"The text prompt describing the content, style, or composition of the image to be generated."},"n":{"type":"number","default":1,"description":"The number of images to generate."},"quality":{"type":"string","enum":["standard","hd"],"description":"The quality of the image that will be generated."},"response_format":{"type":"string","nullable":true,"enum":["url","b64_json"],"default":"url","description":"The format in which the generated images are returned."},"size":{"type":"string","nullable":true,"enum":["1024x1024","1024x1792","1792x1024","512x512","256x256"],"default":"1024x1024","description":"The size of the generated image."},"style":{"type":"string","nullable":true,"enum":["vivid","natural"],"description":"The style of the generated images."}},"required":["prompt"],"additionalProperties":false},{"type":"object","properties":{"model":{"type":"string","enum":["flux/schnell"]},"image_size":{"anyOf":[{"type":"object","properties":{"width":{"type":"integer","minimum":64,"maximum":1536,"default":1024},"height":{"type":"integer","minimum":64,"maximum":1536,"default":768}},"description":"For both height and width, the value must be a multiple of 32."},{"type":"string","enum":["square_hd","square","portrait_4_3","portrait_16_9","landscape_4_3","landscape_16_9"],"description":"The size of the generated image."}],"default":"landscape_4_3"},"num_inference_steps":{"type":"integer","minimum":1,"description":"The number of inference steps to perform.","maximum":12},"enable_safety_checker":{"type":"boolean","default":true,"description":"If set to True, the safety checker will be enabled."},"prompt":{"type":"string","maxLength":4000,"description":"The text prompt describing the content, style, or composition of the image to be generated."},"num_images":{"type":"number","minimum":1,"maximum":4,"default":1,"description":"The number of images to generate."},"seed":{"type":"integer","minimum":1,"description":"The same seed and the same prompt given to the same version of the model will output the same image every time."}},"required":["model","prompt"]},{"type":"object","properties":{"model":{"type":"string","enum":["flux-pro"]},"image_size":{"anyOf":[{"type":"object","properties":{"width":{"type":"integer","minimum":256,"maximum":1440,"default":1024},"height":{"type":"integer","minimum":256,"maximum":1440,"default":768}},"description":"For both height and width, the value must be a multiple of 32."},{"type":"string","enum":["square_hd","square","portrait_4_3","portrait_16_9","landscape_4_3","landscape_16_9"],"description":"The size of the generated image."}],"default":"landscape_4_3"},"num_inference_steps":{"type":"integer","minimum":1,"maximum":50,"description":"The number of inference steps to perform."},"guidance_scale":{"type":"number","minimum":1,"maximum":20,"description":"The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."},"safety_tolerance":{"type":"string","enum":["1","2","3","4","5","6"],"default":"2","description":"The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."},"output_format":{"type":"string","enum":["jpeg","png"],"default":"jpeg","description":"The format of the generated image."},"prompt":{"type":"string","maxLength":4000,"description":"The text prompt describing the content, style, or composition of the image to be generated."},"num_images":{"type":"number","minimum":1,"maximum":4,"default":1,"description":"The number of images to generate."},"seed":{"type":"integer","minimum":1,"description":"The same seed and the same prompt given to the same version of the model will output the same image every time."}},"required":["model","prompt"]},{"type":"object","properties":{"model":{"type":"string","enum":["flux-pro/v1.1"]},"image_size":{"anyOf":[{"type":"object","properties":{"width":{"type":"integer","minimum":256,"maximum":1440,"default":1024},"height":{"type":"integer","minimum":256,"maximum":1440,"default":768}},"description":"For both height and width, the value must be a multiple of 32."},{"type":"string","enum":["square_hd","square","portrait_4_3","portrait_16_9","landscape_4_3","landscape_16_9"],"description":"The size of the generated image."}],"default":"landscape_4_3"},"safety_tolerance":{"type":"string","enum":["1","2","3","4","5","6"],"default":"2","description":"The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."},"output_format":{"type":"string","enum":["jpeg","png"],"default":"jpeg","description":"The format of the generated image."},"prompt":{"type":"string","maxLength":4000,"description":"The text prompt describing the content, style, or composition of the image to be generated."},"num_images":{"type":"number","minimum":1,"maximum":4,"default":1,"description":"The number of images to generate."},"seed":{"type":"integer","minimum":1,"description":"The same seed and the same prompt given to the same version of the model will output the same image every time."},"enable_safety_checker":{"type":"boolean","default":true,"description":"If set to True, the safety checker will be enabled."}},"required":["model","prompt"]},{"type":"object","properties":{"model":{"type":"string","enum":["flux-pro/v1.1-ultra"]},"safety_tolerance":{"type":"string","enum":["1","2","3","4","5","6"],"default":"2","description":"The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."},"output_format":{"type":"string","enum":["jpeg","png"],"default":"jpeg","description":"The format of the generated image."},"prompt":{"type":"string","maxLength":4000,"description":"The text prompt describing the content, style, or composition of the image to be generated."},"num_images":{"type":"number","minimum":1,"maximum":4,"default":1,"description":"The number of images to generate."},"seed":{"type":"integer","minimum":1,"description":"The same seed and the same prompt given to the same version of the model will output the same image every time."},"enable_safety_checker":{"type":"boolean","default":true,"description":"If set to True, the safety checker will be enabled."},"aspect_ratio":{"type":"string","enum":["21:9","16:9","4:3","3:2","1:1","2:3","3:4","9:16","9:21"],"default":"16:9","description":"The aspect ratio of the generated image."},"raw":{"type":"boolean","enum":[false],"default":false,"description":"Generate less processed, more natural-looking images."}},"required":["model","prompt"]},{"type":"object","properties":{"model":{"type":"string","enum":["flux-pro/v1.1-ultra-raw"]},"safety_tolerance":{"type":"string","enum":["1","2","3","4","5","6"],"default":"2","description":"The safety tolerance level for the generated image. 1 being the most strict and 5 being the most permissive."},"output_format":{"type":"string","enum":["jpeg","png"],"default":"jpeg","description":"The format of the generated image."},"prompt":{"type":"string","maxLength":4000,"description":"The text prompt describing the content, style, or composition of the image to be generated."},"num_images":{"type":"number","minimum":1,"maximum":4,"default":1,"description":"The number of images to generate."},"seed":{"type":"integer","minimum":1,"description":"The same seed and the same prompt given to the same version of the model will output the same image every time."},"enable_safety_checker":{"type":"boolean","default":true,"description":"If set to True, the safety checker will be enabled."},"aspect_ratio":{"type":"string","enum":["21:9","16:9","4:3","3:2","1:1","2:3","3:4","9:16","9:21"],"default":"16:9","description":"The aspect ratio of the generated image."},"raw":{"type":"boolean","enum":[true],"default":true,"description":"Generate less processed, more natural-looking images."}},"required":["model","prompt"]},{"type":"object","properties":{"model":{"type":"string","enum":["flux/dev"]},"image_size":{"anyOf":[{"type":"object","properties":{"width":{"type":"integer","minimum":512,"maximum":1536,"default":1024},"height":{"type":"integer","minimum":512,"maximum":1536,"default":768}},"description":"For both height and width, the value must be a multiple of 32."},{"type":"string","enum":["square_hd","square","portrait_4_3","portrait_16_9","landscape_4_3","landscape_16_9"],"description":"The size of the generated image."}],"default":"landscape_4_3"},"guidance_scale":{"type":"number","minimum":1,"maximum":20,"description":"The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."},"num_inference_steps":{"type":"integer","minimum":1,"maximum":50,"description":"The number of inference steps to perform."},"enable_safety_checker":{"type":"boolean","default":true,"description":"If set to True, the safety checker will be enabled."},"prompt":{"type":"string","maxLength":4000,"description":"The text prompt describing the content, style, or composition of the image to be generated."},"num_images":{"type":"number","minimum":1,"maximum":4,"default":1,"description":"The number of images to generate."},"seed":{"type":"integer","minimum":1,"description":"The same seed and the same prompt given to the same version of the model will output the same image every time."}},"required":["model","prompt"]},{"type":"object","properties":{"model":{"type":"string","enum":["flux/dev/image-to-image"]},"guidance_scale":{"type":"number","minimum":1,"maximum":20,"description":"The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."},"num_inference_steps":{"type":"integer","minimum":1,"maximum":50,"description":"The number of inference steps to perform."},"enable_safety_checker":{"type":"boolean","default":true,"description":"If set to True, the safety checker will be enabled."},"prompt":{"type":"string","maxLength":4000,"description":"The text prompt describing the content, style, or composition of the image to be generated."},"num_images":{"type":"number","minimum":1,"maximum":4,"default":1,"description":"The number of images to generate."},"seed":{"type":"integer","minimum":1,"description":"The same seed and the same prompt given to the same version of the model will output the same image every time."},"image_url":{"type":"string","format":"uri","description":"The URL of the reference image."},"strength":{"type":"number","default":0.95,"description":"Determines how much the prompt influences the generated image."}},"required":["model","prompt","image_url"]},{"type":"object","properties":{"model":{"type":"string","enum":["stable-diffusion-v3-medium"]},"image_size":{"anyOf":[{"type":"object","properties":{"width":{"type":"integer","minimum":64,"maximum":1536,"default":1024},"height":{"type":"integer","minimum":64,"maximum":1536,"default":768}},"description":"For both height and width, the value must be a multiple of 32."},{"type":"string","enum":["square_hd","square","portrait_4_3","portrait_16_9","landscape_4_3","landscape_16_9"],"description":"The size of the generated image."}],"default":"square_hd"},"negative_prompt":{"type":"string","description":"The description of elements to avoid in the generated image."},"prompt_expansion":{"type":"boolean","description":"If set to True, prompt will be upsampled with more details."},"guidance_scale":{"type":"number","minimum":1,"maximum":20,"description":"The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."},"num_inference_steps":{"type":"integer","minimum":1,"maximum":50,"description":"The number of inference steps to perform."},"enable_safety_checker":{"type":"boolean","default":true,"description":"If set to True, the safety checker will be enabled."},"prompt":{"type":"string","maxLength":4000,"description":"The text prompt describing the content, style, or composition of the image to be generated."},"num_images":{"type":"number","minimum":1,"maximum":4,"default":1,"description":"The number of images to generate."},"seed":{"type":"integer","minimum":1,"description":"The same seed and the same prompt given to the same version of the model will output the same image every time."}},"required":["model","prompt"]},{"type":"object","properties":{"model":{"type":"string","enum":["stable-diffusion-v35-large"]},"image_size":{"anyOf":[{"type":"object","properties":{"width":{"type":"integer","minimum":64,"maximum":1536,"default":1024},"height":{"type":"integer","minimum":64,"maximum":1536,"default":768}},"description":"For both height and width, the value must be a multiple of 32."},{"type":"string","enum":["square_hd","square","portrait_4_3","portrait_16_9","landscape_4_3","landscape_16_9"],"description":"The size of the generated image."}],"default":"square_hd"},"negative_prompt":{"type":"string","description":"The description of elements to avoid in the generated image."},"guidance_scale":{"type":"number","minimum":1,"maximum":20,"description":"The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."},"num_inference_steps":{"type":"integer","minimum":1,"maximum":50,"description":"The number of inference steps to perform."},"enable_safety_checker":{"type":"boolean","default":true,"description":"If set to True, the safety checker will be enabled."},"prompt":{"type":"string","maxLength":4000,"description":"The text prompt describing the content, style, or composition of the image to be generated."},"num_images":{"type":"number","minimum":1,"maximum":4,"default":1,"description":"The number of images to generate."},"seed":{"type":"integer","minimum":1,"description":"The same seed and the same prompt given to the same version of the model will output the same image every time."},"output_format":{"type":"string","enum":["jpeg","png"],"default":"jpeg","description":"The format of the generated image."}},"required":["model","prompt"]},{"type":"object","properties":{"model":{"type":"string","enum":["flux-realism"]},"image_size":{"anyOf":[{"type":"object","properties":{"width":{"type":"integer","minimum":512,"maximum":1536,"default":1024},"height":{"type":"integer","minimum":512,"maximum":1536,"default":768}},"description":"For both height and width, the value must be a multiple of 32."},{"type":"string","enum":["square_hd","square","portrait_4_3","portrait_16_9","landscape_4_3","landscape_16_9"],"description":"The size of the generated image."}],"default":"landscape_4_3"},"guidance_scale":{"type":"number","minimum":1,"maximum":20,"description":"The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt when looking for a related image to show you."},"num_inference_steps":{"type":"integer","minimum":1,"maximum":50,"description":"The number of inference steps to perform."},"enable_safety_checker":{"type":"boolean","default":true,"description":"If set to True, the safety checker will be enabled."},"output_format":{"type":"string","enum":["jpeg","png"],"default":"jpeg","description":"The format of the generated image."},"prompt":{"type":"string","maxLength":4000,"description":"The text prompt describing the content, style, or composition of the image to be generated."},"num_images":{"type":"number","minimum":1,"maximum":4,"default":1,"description":"The number of images to generate."},"seed":{"type":"integer","minimum":1,"description":"The same seed and the same prompt given to the same version of the model will output the same image every time."}},"required":["model","prompt"]},{"type":"object","properties":{"model":{"type":"string","enum":["triposr"]},"image_url":{"type":"string","format":"uri","description":"The URL of the reference image."},"output_format":{"type":"string","enum":["glb","obj"],"default":"glb","description":"The format of the generated image."},"do_remove_background":{"type":"boolean","description":"Enables removing the background from the input image."},"foreground_ratio":{"type":"number","minimum":0.5,"maximum":1,"default":0.9,"description":"Ratio of the foreground image to the original image."},"mc_resolution":{"type":"integer","minimum":32,"maximum":1024,"default":256,"description":"Resolution of the marching cubes. Above 512 is not recommended."}},"required":["model","image_url"]},{"type":"object","properties":{"model":{"type":"string","enum":["recraft-v3"]},"prompt":{"type":"string","maxLength":4000,"description":"The text prompt describing the content, style, or composition of the image to be generated."},"image_size":{"anyOf":[{"type":"object","properties":{"width":{"type":"integer","minimum":64,"maximum":1536,"default":1024},"height":{"type":"integer","minimum":64,"maximum":1536,"default":768}},"description":"For both height and width, the value must be a multiple of 32."},{"type":"string","enum":["square_hd","square","portrait_4_3","portrait_16_9","landscape_4_3","landscape_16_9"],"description":"The size of the generated image."}],"default":"square_hd"},"style":{"type":"string","enum":["any","realistic_image","digital_illustration","vector_illustration","realistic_image/b_and_w","realistic_image/hard_flash","realistic_image/hdr","realistic_image/natural_light","realistic_image/studio_portrait","realistic_image/enterprise","realistic_image/motion_blur","digital_illustration/pixel_art","digital_illustration/hand_drawn","digital_illustration/grain","digital_illustration/infantile_sketch","digital_illustration/2d_art_poster","digital_illustration/handmade_3d","digital_illustration/hand_drawn_outline","digital_illustration/engraving_color","digital_illustration/2d_art_poster_2","vector_illustration/engraving","vector_illustration/line_art","vector_illustration/line_circuit","vector_illustration/linocut"],"default":"realistic_image","description":"The style of the generated images."},"colors":{"type":"array","items":{"type":"object","properties":{"r":{"type":"integer","minimum":0,"maximum":255},"g":{"type":"integer","minimum":0,"maximum":255},"b":{"type":"integer","minimum":0,"maximum":255}},"required":["r","g","b"]},"default":[],"description":"An array of preferred colors."},"num_images":{"type":"number","enum":[1],"default":1,"description":"The number of images to generate."}},"required":["model","prompt"]},{"type":"object","properties":{"model":{"type":"string","enum":["imagen-3.0-generate-002"]},"prompt":{"type":"string","maxLength":400,"description":"The text prompt describing the content, style, or composition of the image to be generated."},"convert_base64_to_url":{"type":"boolean","default":false,"description":"If True, the URL to the image will be returned; otherwise, the file will be provided in base64 format."},"num_images":{"type":"integer","maximum":4,"default":1,"description":"The number of images to generate."},"seed":{"type":"integer","minimum":0,"maximum":4294967295,"description":"The same seed and the same prompt given to the same version of the model will output the same image every time."},"enhance_prompt":{"type":"boolean","default":true,"description":"Optional parameter to use an LLM-based prompt rewriting feature for higher-quality images that better match the original prompt. Disabling it may affect image quality and prompt alignment."},"aspect_ratio":{"type":"string","enum":["1:1","9:16","16:9","3:4","4:3"],"default":"1:1","description":"The aspect ratio of the generated image."},"person_generation":{"type":"string","enum":["dont_allow","allow_adult"],"default":"allow_adult","description":"Allow generation of people."},"safety_setting":{"type":"string","enum":["block_low_and_above","block_medium_and_above","block_only_high"],"default":"block_medium_and_above","description":"Adds a filter level to safety filtering."},"add_watermark":{"type":"boolean","default":false,"description":"Add an invisible watermark to the generated images."}},"required":["model","prompt"]}]},"LlmAssistant.v1.CreateAssistantDTO":{"type":"object","properties":{"model":{"type":"string","enum":["openai/gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","gpt-4o-mini","gpt-4o-mini-2024-07-18","chatgpt-4o-latest","gpt-4-turbo","gpt-4-turbo-2024-04-09","gpt-4","gpt-4-0125-preview","gpt-4-1106-preview","gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106","o1-preview","o1-preview-2024-09-12","o1-mini","o1-mini-2024-09-12","o3-mini","gpt-4.5-preview","gpt-4o-audio-preview","gpt-4o-mini-audio-preview","gpt-4o-search-preview","gpt-4o-mini-search-preview","openai/gpt-4.1-2025-04-14","openai/gpt-4.1-mini-2025-04-14","openai/gpt-4.1-nano-2025-04-14","openai/o4-mini-2025-04-16"]},"description":{"type":"string","nullable":true,"description":"The description of the Assistant. The maximum length is 512 characters."},"instructions":{"type":"string","nullable":true,"description":"The system instructions that the Assistant uses. The maximum length is 256,000 characters. Instructions can indeed be very large and complex, including full action frameworks, example messages, response formatting guidelines, topic restrictions, and stylistic rules."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"name":{"type":"string","nullable":true,"description":"The name of the Assistant. The maximum length is 256 characters."},"reasoning_effort":{"type":"string","nullable":true,"enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models."},"response_format":{"anyOf":[{"type":"string","enum":["auto"]},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}]},{"nullable":true}],"description":"Specifies the format that the model must output."},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."},"tool_resources":{"type":"object","nullable":true,"properties":{"code_interpreter":{"type":"object","properties":{"file_ids":{"type":"array","items":{"nullable":true},"description":"A list of file IDs made available to the code_interpreter tool. There can be a maximum of 20 files associated with the tool."}}},"file_search":{"type":"object","properties":{"vector_store_ids":{"type":"array","items":{"type":"string"},"description":"The vector store attached to this Assistant. There can be a maximum of 1 vector store attached to the Assistant."},"vector_stores":{"type":"array","items":{"type":"object","properties":{"chunking_strategy":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["static"]},"static":{"type":"object","properties":{"chunk_overlap_tokens":{"type":"integer","description":"The number of tokens that overlap between chunks. The default value is 400. Note that the overlap must not exceed half of max_chunk_size_tokens."},"max_chunk_size_tokens":{"type":"integer","description":"The maximum number of tokens in each chunk. The default value is 800. The minimum value is 100 and the maximum value is 4096"}},"required":["chunk_overlap_tokens","max_chunk_size_tokens"]}},"required":["type","static"]}],"description":"The chunking strategy used to chunk the file(s). If not set, will use the auto strategy."},"file_ids":{"type":"array","items":{"type":"string"},"description":"A list of file IDs to add to the vector store. There can be a maximum of 10000 files in a vector store."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."}}},"description":"A helper to create a vector store with file_ids and attach it to this Assistant. There can be a maximum of 1 vector store attached to the Assistant."}}}},"description":"A set of resources that are used by the Assistant's tools. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs."},"tools":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of tool being defined."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["file_search"],"description":"The type of tool being defined."},"file_search":{"type":"object","properties":{"max_num_results":{"type":"integer","minimum":1,"maximum":50,"description":"The maximum number of results the file search tool should output"},"ranking_options":{"type":"object","properties":{"score_threshold":{"type":"number","minimum":0,"maximum":1,"description":"The score threshold for the file search. All values must be a floating point number between 0 and 1"},"ranker":{"type":"string","enum":["auto","default_2024_08_21"],"description":"The ranker to use for the file search. If not specified will use the auto ranker"}},"required":["score_threshold"],"description":"The ranking options for the file search. If not specified, the file search tool will use the auto ranker and a score_threshold of 0."}},"description":"Overrides for the file search tool"}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of tool being defined."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"The parameters the functions accepts, described as a JSON Schema object"},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True"}},"required":["name"]}},"required":["type","function"]}]},"description":"A list of tool enabled on the Assistant. There can be a maximum of 128 tools per Assistant."},"top_p":{"type":"number","nullable":true,"minimum":0,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  \n  We generally recommend altering this or temperature but not both."}},"required":["model"]},"LlmAssistant.v1.CreateAssistantResponseDTO":{"type":"object","properties":{"model":{"type":"string","description":"ID of the model to use."},"description":{"type":"string","nullable":true,"description":"The description of the Assistant. The maximum length is 512 characters."},"instructions":{"type":"string","nullable":true,"description":"The system instructions that the Assistant uses. The maximum length is 256,000 characters. Instructions can indeed be very large and complex, including full action frameworks, example messages, response formatting guidelines, topic restrictions, and stylistic rules."},"metadata":{"nullable":true,"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"name":{"type":"string","nullable":true,"description":"The name of the Assistant. The maximum length is 256 characters."},"reasoning_effort":{"type":"string","nullable":true,"enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models."},"response_format":{"anyOf":[{"type":"string","enum":["auto"]},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}]},{"nullable":true}],"description":"Specifies the format that the model must output."},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."},"tool_resources":{"type":"object","nullable":true,"properties":{"code_interpreter":{"type":"object","properties":{"file_ids":{"type":"array","items":{"nullable":true},"description":"A list of file IDs made available to the code_interpreter tool. There can be a maximum of 20 files associated with the tool."}}},"file_search":{"type":"object","properties":{"vector_store_ids":{"type":"array","items":{"type":"string"},"description":"The vector store attached to this Assistant. There can be a maximum of 1 vector store attached to the Assistant."},"vector_stores":{"type":"array","items":{"type":"object","properties":{"chunking_strategy":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["static"]},"static":{"type":"object","properties":{"chunk_overlap_tokens":{"type":"integer","description":"The number of tokens that overlap between chunks. The default value is 400. Note that the overlap must not exceed half of max_chunk_size_tokens."},"max_chunk_size_tokens":{"type":"integer","description":"The maximum number of tokens in each chunk. The default value is 800. The minimum value is 100 and the maximum value is 4096"}},"required":["chunk_overlap_tokens","max_chunk_size_tokens"]}},"required":["type","static"]}],"description":"The chunking strategy used to chunk the file(s). If not set, will use the auto strategy."},"file_ids":{"type":"array","items":{"type":"string"},"description":"A list of file IDs to add to the vector store. There can be a maximum of 10000 files in a vector store."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."}}},"description":"A helper to create a vector store with file_ids and attach it to this Assistant. There can be a maximum of 1 vector store attached to the Assistant."}}}},"description":"A set of resources that are used by the Assistant's tools. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs."},"tools":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of tool being defined."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["file_search"],"description":"The type of tool being defined."},"file_search":{"type":"object","properties":{"max_num_results":{"type":"integer","minimum":1,"maximum":50,"description":"The maximum number of results the file search tool should output"},"ranking_options":{"type":"object","properties":{"score_threshold":{"type":"number","minimum":0,"maximum":1,"description":"The score threshold for the file search. All values must be a floating point number between 0 and 1"},"ranker":{"type":"string","enum":["auto","default_2024_08_21"],"description":"The ranker to use for the file search. If not specified will use the auto ranker"}},"required":["score_threshold"],"description":"The ranking options for the file search. If not specified, the file search tool will use the auto ranker and a score_threshold of 0."}},"description":"Overrides for the file search tool"}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of tool being defined."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"The parameters the functions accepts, described as a JSON Schema object"},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True"}},"required":["name"]}},"required":["type","function"]}]},"description":"A list of tool enabled on the Assistant. There can be a maximum of 128 tools per Assistant."},"top_p":{"type":"number","nullable":true,"minimum":0,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  \n  We generally recommend altering this or temperature but not both."},"id":{"type":"string","description":"The identifier, which can be referenced in API endpoints."},"created_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when the Assistant was created."},"object":{"type":"string","enum":["assistant"],"description":"The object type."}},"required":["model","id","created_at","object"]},"LlmAssistant.v1.GetAssistantsDTO":{"type":"object","properties":{"limit":{"type":"integer","minimum":1,"maximum":100,"description":"A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20."},"order":{"type":"string","enum":["asc","desc"],"description":"Sort order by the created_at timestamp of the objects. asc for ascending order and desc for descending order."},"before":{"type":"string","description":"A cursor for use in pagination. before is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, starting with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list."},"after":{"type":"string","description":"A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list."}},"additionalProperties":false},"LlmAssistant.v1.GetAssistantsResponseDTO":{"type":"object","properties":{"object":{"type":"string","enum":["list"]},"data":{"type":"array","items":{"type":"object","properties":{"model":{"type":"string","description":"ID of the model to use."},"description":{"type":"string","nullable":true,"description":"The description of the Assistant. The maximum length is 512 characters."},"instructions":{"type":"string","nullable":true,"description":"The system instructions that the Assistant uses. The maximum length is 256,000 characters. Instructions can indeed be very large and complex, including full action frameworks, example messages, response formatting guidelines, topic restrictions, and stylistic rules."},"metadata":{"nullable":true,"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"name":{"type":"string","nullable":true,"description":"The name of the Assistant. The maximum length is 256 characters."},"id":{"type":"string","description":"The identifier, which can be referenced in API endpoints."},"created_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when the Assistant was created."},"object":{"type":"string","enum":["assistant"],"description":"The object type."}},"required":["model","id","created_at","object"]}}},"required":["object","data"]},"LlmAssistant.v1.GetAssistantResponseDTO":{"type":"object","properties":{"model":{"type":"string","description":"ID of the model to use."},"description":{"type":"string","nullable":true,"description":"The description of the Assistant. The maximum length is 512 characters."},"instructions":{"type":"string","nullable":true,"description":"The system instructions that the Assistant uses. The maximum length is 256,000 characters. Instructions can indeed be very large and complex, including full action frameworks, example messages, response formatting guidelines, topic restrictions, and stylistic rules."},"metadata":{"nullable":true,"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"name":{"type":"string","nullable":true,"description":"The name of the Assistant. The maximum length is 256 characters."},"reasoning_effort":{"type":"string","nullable":true,"enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models."},"response_format":{"anyOf":[{"type":"string","enum":["auto"]},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}]},{"nullable":true}],"description":"Specifies the format that the model must output."},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."},"tool_resources":{"type":"object","nullable":true,"properties":{"code_interpreter":{"type":"object","properties":{"file_ids":{"type":"array","items":{"nullable":true},"description":"A list of file IDs made available to the code_interpreter tool. There can be a maximum of 20 files associated with the tool."}}},"file_search":{"type":"object","properties":{"vector_store_ids":{"type":"array","items":{"type":"string"},"description":"The vector store attached to this Assistant. There can be a maximum of 1 vector store attached to the Assistant."},"vector_stores":{"type":"array","items":{"type":"object","properties":{"chunking_strategy":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["static"]},"static":{"type":"object","properties":{"chunk_overlap_tokens":{"type":"integer","description":"The number of tokens that overlap between chunks. The default value is 400. Note that the overlap must not exceed half of max_chunk_size_tokens."},"max_chunk_size_tokens":{"type":"integer","description":"The maximum number of tokens in each chunk. The default value is 800. The minimum value is 100 and the maximum value is 4096"}},"required":["chunk_overlap_tokens","max_chunk_size_tokens"]}},"required":["type","static"]}],"description":"The chunking strategy used to chunk the file(s). If not set, will use the auto strategy."},"file_ids":{"type":"array","items":{"type":"string"},"description":"A list of file IDs to add to the vector store. There can be a maximum of 10000 files in a vector store."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."}}},"description":"A helper to create a vector store with file_ids and attach it to this Assistant. There can be a maximum of 1 vector store attached to the Assistant."}}}},"description":"A set of resources that are used by the Assistant's tools. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs."},"tools":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of tool being defined."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["file_search"],"description":"The type of tool being defined."},"file_search":{"type":"object","properties":{"max_num_results":{"type":"integer","minimum":1,"maximum":50,"description":"The maximum number of results the file search tool should output"},"ranking_options":{"type":"object","properties":{"score_threshold":{"type":"number","minimum":0,"maximum":1,"description":"The score threshold for the file search. All values must be a floating point number between 0 and 1"},"ranker":{"type":"string","enum":["auto","default_2024_08_21"],"description":"The ranker to use for the file search. If not specified will use the auto ranker"}},"required":["score_threshold"],"description":"The ranking options for the file search. If not specified, the file search tool will use the auto ranker and a score_threshold of 0."}},"description":"Overrides for the file search tool"}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of tool being defined."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"The parameters the functions accepts, described as a JSON Schema object"},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True"}},"required":["name"]}},"required":["type","function"]}]},"description":"A list of tool enabled on the Assistant. There can be a maximum of 128 tools per Assistant."},"top_p":{"type":"number","nullable":true,"minimum":0,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  \n  We generally recommend altering this or temperature but not both."},"id":{"type":"string","description":"The identifier, which can be referenced in API endpoints."},"created_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when the Assistant was created."},"object":{"type":"string","enum":["assistant"],"description":"The object type."}},"required":["model","id","created_at","object"]},"LlmAssistant.v1.UpdateAssistantDTO":{"allOf":[{"$ref":"#/components/schemas/LlmAssistant.v1.CreateAssistantDTO"},{"type":"object","properties":{}}]},"LlmAssistant.v1.UpdateAssistantResponseDTO":{"type":"object","properties":{"model":{"type":"string","description":"ID of the model to use."},"description":{"type":"string","nullable":true,"description":"The description of the Assistant. The maximum length is 512 characters."},"instructions":{"type":"string","nullable":true,"description":"The system instructions that the Assistant uses. The maximum length is 256,000 characters. Instructions can indeed be very large and complex, including full action frameworks, example messages, response formatting guidelines, topic restrictions, and stylistic rules."},"metadata":{"nullable":true,"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"name":{"type":"string","nullable":true,"description":"The name of the Assistant. The maximum length is 256 characters."},"reasoning_effort":{"type":"string","nullable":true,"enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models."},"response_format":{"anyOf":[{"type":"string","enum":["auto"]},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}]},{"nullable":true}],"description":"Specifies the format that the model must output."},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."},"tool_resources":{"type":"object","nullable":true,"properties":{"code_interpreter":{"type":"object","properties":{"file_ids":{"type":"array","items":{"nullable":true},"description":"A list of file IDs made available to the code_interpreter tool. There can be a maximum of 20 files associated with the tool."}}},"file_search":{"type":"object","properties":{"vector_store_ids":{"type":"array","items":{"type":"string"},"description":"The vector store attached to this Assistant. There can be a maximum of 1 vector store attached to the Assistant."},"vector_stores":{"type":"array","items":{"type":"object","properties":{"chunking_strategy":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["static"]},"static":{"type":"object","properties":{"chunk_overlap_tokens":{"type":"integer","description":"The number of tokens that overlap between chunks. The default value is 400. Note that the overlap must not exceed half of max_chunk_size_tokens."},"max_chunk_size_tokens":{"type":"integer","description":"The maximum number of tokens in each chunk. The default value is 800. The minimum value is 100 and the maximum value is 4096"}},"required":["chunk_overlap_tokens","max_chunk_size_tokens"]}},"required":["type","static"]}],"description":"The chunking strategy used to chunk the file(s). If not set, will use the auto strategy."},"file_ids":{"type":"array","items":{"type":"string"},"description":"A list of file IDs to add to the vector store. There can be a maximum of 10000 files in a vector store."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."}}},"description":"A helper to create a vector store with file_ids and attach it to this Assistant. There can be a maximum of 1 vector store attached to the Assistant."}}}},"description":"A set of resources that are used by the Assistant's tools. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs."},"tools":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of tool being defined."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["file_search"],"description":"The type of tool being defined."},"file_search":{"type":"object","properties":{"max_num_results":{"type":"integer","minimum":1,"maximum":50,"description":"The maximum number of results the file search tool should output"},"ranking_options":{"type":"object","properties":{"score_threshold":{"type":"number","minimum":0,"maximum":1,"description":"The score threshold for the file search. All values must be a floating point number between 0 and 1"},"ranker":{"type":"string","enum":["auto","default_2024_08_21"],"description":"The ranker to use for the file search. If not specified will use the auto ranker"}},"required":["score_threshold"],"description":"The ranking options for the file search. If not specified, the file search tool will use the auto ranker and a score_threshold of 0."}},"description":"Overrides for the file search tool"}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of tool being defined."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"The parameters the functions accepts, described as a JSON Schema object"},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True"}},"required":["name"]}},"required":["type","function"]}]},"description":"A list of tool enabled on the Assistant. There can be a maximum of 128 tools per Assistant."},"top_p":{"type":"number","nullable":true,"minimum":0,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  \n  We generally recommend altering this or temperature but not both."},"id":{"type":"string","description":"The identifier, which can be referenced in API endpoints."},"created_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when the Assistant was created."},"object":{"type":"string","enum":["assistant"],"description":"The object type."}},"required":["model","id","created_at","object"]},"LlmAssistant.v1.DeleteAssistantResponseDTO":{"type":"object","properties":{"id":{"type":"string"},"object":{"type":"string","enum":["assistant.deleted"]},"deleted":{"type":"boolean"}},"required":["id","object","deleted"]},"LlmThread.v1.GetThreadResponseDTO":{"type":"object","properties":{"id":{"type":"string","description":"The identifier, which can be referenced in API endpoints."},"created_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when the Thread was created."},"metadata":{"nullable":true,"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"object":{"type":"string","enum":["thread"],"description":"The object type."},"tool_resources":{"type":"object","nullable":true,"properties":{"code_interpreter":{"type":"object","properties":{"file_ids":{"type":"array","items":{"type":"string"},"description":"A list of file IDs made available to the code_interpreter tool. There can be a maximum of 20 files associated with the tool."}}},"file_search":{"type":"object","properties":{"vector_store_ids":{"type":"array","items":{"type":"string"},"description":"The vector store attached to this Thread. There can be a maximum of 1 vector store attached to the Thread."}}}},"description":"A set of resources that are made available to the assistant's tools in this Thread. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs."}},"required":["id","created_at","object","tool_resources"]},"LlmThread.v1.CreateThreadDTO":{"type":"object","properties":{"messages":{"type":"array","items":{"type":"object","properties":{"role":{"type":"string","enum":["user","assistant"],"description":"The role of the entity that is creating the Message"},"content":{"anyOf":[{"type":"string","description":"The text contents of the Message"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"image_file":{"type":"object","properties":{"file_id":{"type":"string","description":"The File ID of the image in the Message content."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image if specified by the user. Low uses fewer tokens, you can opt in to high resolution using high."}},"required":["file_id"]},"type":{"type":"string","enum":["image_file"]}},"required":["image_file","type"]},{"type":"object","properties":{"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"The external URL of the image, must be a supported image types: jpeg, jpg, png, gif, webp."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image. Low uses fewer tokens, you can opt in to high resolution using high. Default value is auto."}},"required":["url"]},"type":{"type":"string","enum":["image_url"]}},"required":["image_url","type"]},{"type":"object","properties":{"text":{"type":"string","description":"Text content to be sent to the model"},"type":{"type":"string","enum":["text"]}},"required":["text","type"]}]},"description":"An array of content parts with a defined type, each can be of type text or images can be passed with image_url or image_file"}]},"attachments":{"type":"array","nullable":true,"items":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the file to attach to the Message."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter","file_search"],"description":"The type of tool being defined."}},"required":["type"]},"description":"The tools to which this file should be added."}}},"description":"A list of files attached to the Message, and the tools they should be added to."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."}},"required":["role","content"]},"description":"A list of messages to start the Thread with."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"tool_resources":{"type":"object","properties":{"code_interpreter":{"type":"object","properties":{"file_ids":{"type":"array","items":{"nullable":true},"description":"A list of file IDs made available to the code_interpreter tool. There can be a maximum of 20 files associated with the tool."}}},"file_search":{"type":"object","properties":{"vector_store_ids":{"type":"array","items":{"type":"string"},"description":"The vector store attached to this Assistant. There can be a maximum of 1 vector store attached to the Assistant."},"vector_stores":{"type":"array","items":{"type":"object","properties":{"chunking_strategy":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["static"]},"static":{"type":"object","properties":{"chunk_overlap_tokens":{"type":"integer","description":"The number of tokens that overlap between chunks. The default value is 400. Note that the overlap must not exceed half of max_chunk_size_tokens."},"max_chunk_size_tokens":{"type":"integer","description":"The maximum number of tokens in each chunk. The default value is 800. The minimum value is 100 and the maximum value is 4096"}},"required":["chunk_overlap_tokens","max_chunk_size_tokens"]}},"required":["type","static"]}],"description":"The chunking strategy used to chunk the file(s). If not set, will use the auto strategy."},"file_ids":{"type":"array","items":{"type":"string"},"description":"A list of file IDs to add to the vector store. There can be a maximum of 10000 files in a vector store."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."}}},"description":"A helper to create a vector store with file_ids and attach it to this Assistant. There can be a maximum of 1 vector store attached to the Assistant."}}}},"description":"A set of resources that are made available to the assistant's tools in this Thread. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs."}},"additionalProperties":false},"LlmThread.v1.CreateThreadResponseDTO":{"type":"object","properties":{"id":{"type":"string","description":"The identifier, which can be referenced in API endpoints."},"created_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when the Thread was created."},"metadata":{"nullable":true,"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"object":{"type":"string","enum":["thread"],"description":"The object type."},"tool_resources":{"type":"object","nullable":true,"properties":{"code_interpreter":{"type":"object","properties":{"file_ids":{"type":"array","items":{"type":"string"},"description":"A list of file IDs made available to the code_interpreter tool. There can be a maximum of 20 files associated with the tool."}}},"file_search":{"type":"object","properties":{"vector_store_ids":{"type":"array","items":{"type":"string"},"description":"The vector store attached to this Thread. There can be a maximum of 1 vector store attached to the Thread."}}}},"description":"A set of resources that are made available to the assistant's tools in this Thread. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs."}},"required":["id","created_at","object","tool_resources"]},"LlmThread.v1.UpdateThreadDTO":{"type":"object","properties":{"metadata":{"type":"object","additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"tool_resources":{"type":"object","properties":{"code_interpreter":{"type":"object","properties":{"file_ids":{"type":"array","items":{"nullable":true},"description":"A list of file IDs made available to the code_interpreter tool. There can be a maximum of 20 files associated with the tool."}}},"file_search":{"type":"object","properties":{"vector_store_ids":{"type":"array","items":{"nullable":true},"description":"The vector store attached to this Thread. There can be a maximum of 1 vector store attached to the Thread."}}}},"required":["code_interpreter","file_search"],"description":"A set of resources that are made available to the assistant's tools in this Thread. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs."}},"additionalProperties":false},"LlmThread.v1.UpdateThreadResponseDTO":{"type":"object","properties":{"id":{"type":"string","description":"The identifier, which can be referenced in API endpoints."},"created_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when the Thread was created."},"metadata":{"nullable":true,"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"object":{"type":"string","enum":["thread"],"description":"The object type."},"tool_resources":{"type":"object","nullable":true,"properties":{"code_interpreter":{"type":"object","properties":{"file_ids":{"type":"array","items":{"type":"string"},"description":"A list of file IDs made available to the code_interpreter tool. There can be a maximum of 20 files associated with the tool."}}},"file_search":{"type":"object","properties":{"vector_store_ids":{"type":"array","items":{"type":"string"},"description":"The vector store attached to this Thread. There can be a maximum of 1 vector store attached to the Thread."}}}},"description":"A set of resources that are made available to the assistant's tools in this Thread. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs."}},"required":["id","created_at","object","tool_resources"]},"LlmThread.v1.DeleteThreadResponseDTO":{"type":"object","properties":{"id":{"type":"string"},"object":{"type":"string","enum":["thread.deleted"]},"deleted":{"type":"boolean"}},"required":["id","object","deleted"]},"LlmThreadMessage.v1.GetThreadMessagesDTO":{"type":"object","properties":{"limit":{"type":"integer","minimum":1,"maximum":100,"description":"A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20."},"order":{"type":"string","enum":["asc","desc"],"description":"Sort order by the created_at timestamp of the objects. asc for ascending order and desc for descending order."},"before":{"type":"string","description":"A cursor for use in pagination. before is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, starting with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list."},"after":{"type":"string","description":"A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list."},"run_id":{"type":"string","description":"Filter Messages by the Run ID that generated them."}},"additionalProperties":false},"LlmThreadMessage.v1.GetThreadMessagesResponseDTO":{"type":"object","properties":{"object":{"type":"string","enum":["list"]},"data":{"type":"array","items":{"type":"object","properties":{"role":{"type":"string","enum":["user","assistant"],"description":"The role of the entity that is creating the Message"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"image_file":{"type":"object","properties":{"file_id":{"type":"string","description":"The File ID of the image in the Message content."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image if specified by the user. Low uses fewer tokens, you can opt in to high resolution using high."}},"required":["file_id"]},"type":{"type":"string","enum":["image_file"]}},"required":["image_file","type"]},{"type":"object","properties":{"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"The external URL of the image, must be a supported image types: jpeg, jpg, png, gif, webp."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image. Low uses fewer tokens, you can opt in to high resolution using high. Default value is auto."}},"required":["url"]},"type":{"type":"string","enum":["image_url"]}},"required":["image_url","type"]},{"type":"object","properties":{"text":{"type":"object","properties":{"annotations":{"type":"array","items":{"anyOf":[{"type":"object","properties":{"end_index":{"type":"integer"},"file_citation":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the specific File the citation is from."}},"required":["file_id"]},"start_index":{"type":"integer"},"text":{"type":"string","description":"The text in the message content that needs to be replaced."},"type":{"type":"string","enum":["file_citation"]}},"required":["end_index","file_citation","start_index","text","type"]},{"type":"object","properties":{"end_index":{"type":"integer"},"file_path":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the file that was generated."}},"required":["file_id"]},"start_index":{"type":"integer"},"text":{"type":"string","description":"The text in the message content that needs to be replaced."},"type":{"type":"string","enum":["file_path"]}},"required":["end_index","file_path","start_index","text","type"]}]}},"value":{"type":"string"}},"required":["annotations","value"]},"type":{"type":"string","enum":["text"]}},"required":["text","type"]},{"type":"object","properties":{"refusal":{"type":"string"},"type":{"type":"string","enum":["refusal"]}},"required":["refusal","type"]}]}}],"description":"The content of the message in array of text and/or images."},"attachments":{"type":"array","nullable":true,"items":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the file to attach to the Message."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter","file_search"],"description":"The type of tool being defined."}},"required":["type"]},"description":"The tools to which this file should be added."}}},"description":"A list of files attached to the Message, and the tools they should be added to."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"id":{"type":"string","description":"The identifier, which can be referenced in API endpoints."},"object":{"type":"string","enum":["thread.message"],"description":"The object type."},"status":{"type":"string","enum":["completed","incomplete","in_progress"],"description":"The status of the message."},"created_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when the message was created."},"completed_at":{"type":"integer","nullable":true,"description":"The Unix timestamp (in seconds) for when the message was completed."},"incomplete_at":{"type":"integer","nullable":true,"description":"The Unix timestamp (in seconds) for when the message was marked as incomplete."},"incomplete_details":{"type":"object","nullable":true,"properties":{"reason":{"type":"string","description":"The reason the message is incomplete."}},"required":["reason"],"description":"On an incomplete message, details about why the message is incomplete."},"assistant_id":{"type":"string","nullable":true,"description":"If applicable, the ID of the assistant that authored this message."},"thread_id":{"type":"string","description":"The thread ID that this message belongs to."},"run_id":{"type":"string","nullable":true}},"required":["role","content","id","object","status","created_at","completed_at","incomplete_at","incomplete_details","assistant_id","thread_id","run_id"]}},"first_id":{"type":"string"},"last_id":{"type":"string"},"has_more":{"type":"boolean"}},"required":["object","data","first_id","last_id","has_more"]},"LlmThreadMessage.v1.CreateThreadMessageDTO":{"type":"object","properties":{"role":{"type":"string","enum":["user","assistant"],"description":"The role of the entity that is creating the Message"},"content":{"anyOf":[{"type":"string","description":"The text contents of the Message"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"image_file":{"type":"object","properties":{"file_id":{"type":"string","description":"The File ID of the image in the Message content."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image if specified by the user. Low uses fewer tokens, you can opt in to high resolution using high."}},"required":["file_id"]},"type":{"type":"string","enum":["image_file"]}},"required":["image_file","type"]},{"type":"object","properties":{"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"The external URL of the image, must be a supported image types: jpeg, jpg, png, gif, webp."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image. Low uses fewer tokens, you can opt in to high resolution using high. Default value is auto."}},"required":["url"]},"type":{"type":"string","enum":["image_url"]}},"required":["image_url","type"]},{"type":"object","properties":{"text":{"type":"string","description":"Text content to be sent to the model"},"type":{"type":"string","enum":["text"]}},"required":["text","type"]}]},"description":"An array of content parts with a defined type, each can be of type text or images can be passed with image_url or image_file"}]},"attachments":{"type":"array","nullable":true,"items":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the file to attach to the Message."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter","file_search"],"description":"The type of tool being defined."}},"required":["type"]},"description":"The tools to which this file should be added."}}},"description":"A list of files attached to the Message, and the tools they should be added to."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."}},"required":["role","content"],"additionalProperties":false},"LlmThreadMessage.v1.CreateThreadMessageResponseDTO":{"type":"object","properties":{"role":{"type":"string","enum":["user","assistant"],"description":"The role of the entity that is creating the Message"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"image_file":{"type":"object","properties":{"file_id":{"type":"string","description":"The File ID of the image in the Message content."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image if specified by the user. Low uses fewer tokens, you can opt in to high resolution using high."}},"required":["file_id"]},"type":{"type":"string","enum":["image_file"]}},"required":["image_file","type"]},{"type":"object","properties":{"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"The external URL of the image, must be a supported image types: jpeg, jpg, png, gif, webp."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image. Low uses fewer tokens, you can opt in to high resolution using high. Default value is auto."}},"required":["url"]},"type":{"type":"string","enum":["image_url"]}},"required":["image_url","type"]},{"type":"object","properties":{"text":{"type":"object","properties":{"annotations":{"type":"array","items":{"anyOf":[{"type":"object","properties":{"end_index":{"type":"integer"},"file_citation":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the specific File the citation is from."}},"required":["file_id"]},"start_index":{"type":"integer"},"text":{"type":"string","description":"The text in the message content that needs to be replaced."},"type":{"type":"string","enum":["file_citation"]}},"required":["end_index","file_citation","start_index","text","type"]},{"type":"object","properties":{"end_index":{"type":"integer"},"file_path":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the file that was generated."}},"required":["file_id"]},"start_index":{"type":"integer"},"text":{"type":"string","description":"The text in the message content that needs to be replaced."},"type":{"type":"string","enum":["file_path"]}},"required":["end_index","file_path","start_index","text","type"]}]}},"value":{"type":"string"}},"required":["annotations","value"]},"type":{"type":"string","enum":["text"]}},"required":["text","type"]},{"type":"object","properties":{"refusal":{"type":"string"},"type":{"type":"string","enum":["refusal"]}},"required":["refusal","type"]}]}}],"description":"The content of the message in array of text and/or images."},"attachments":{"type":"array","nullable":true,"items":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the file to attach to the Message."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter","file_search"],"description":"The type of tool being defined."}},"required":["type"]},"description":"The tools to which this file should be added."}}},"description":"A list of files attached to the Message, and the tools they should be added to."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"id":{"type":"string","description":"The identifier, which can be referenced in API endpoints."},"object":{"type":"string","enum":["thread.message"],"description":"The object type."},"status":{"type":"string","enum":["completed","incomplete","in_progress"],"description":"The status of the message."},"created_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when the message was created."},"completed_at":{"type":"integer","nullable":true,"description":"The Unix timestamp (in seconds) for when the message was completed."},"incomplete_at":{"type":"integer","nullable":true,"description":"The Unix timestamp (in seconds) for when the message was marked as incomplete."},"incomplete_details":{"type":"object","nullable":true,"properties":{"reason":{"type":"string","description":"The reason the message is incomplete."}},"required":["reason"],"description":"On an incomplete message, details about why the message is incomplete."},"assistant_id":{"type":"string","nullable":true,"description":"If applicable, the ID of the assistant that authored this message."},"thread_id":{"type":"string","description":"The thread ID that this message belongs to."},"run_id":{"type":"string","nullable":true}},"required":["role","content","id","object","status","created_at","completed_at","incomplete_at","incomplete_details","assistant_id","thread_id","run_id"]},"LlmThreadMessage.v1.GetThreadMessageResponseDTO":{"type":"object","properties":{"role":{"type":"string","enum":["user","assistant"],"description":"The role of the entity that is creating the Message"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"image_file":{"type":"object","properties":{"file_id":{"type":"string","description":"The File ID of the image in the Message content."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image if specified by the user. Low uses fewer tokens, you can opt in to high resolution using high."}},"required":["file_id"]},"type":{"type":"string","enum":["image_file"]}},"required":["image_file","type"]},{"type":"object","properties":{"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"The external URL of the image, must be a supported image types: jpeg, jpg, png, gif, webp."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image. Low uses fewer tokens, you can opt in to high resolution using high. Default value is auto."}},"required":["url"]},"type":{"type":"string","enum":["image_url"]}},"required":["image_url","type"]},{"type":"object","properties":{"text":{"type":"object","properties":{"annotations":{"type":"array","items":{"anyOf":[{"type":"object","properties":{"end_index":{"type":"integer"},"file_citation":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the specific File the citation is from."}},"required":["file_id"]},"start_index":{"type":"integer"},"text":{"type":"string","description":"The text in the message content that needs to be replaced."},"type":{"type":"string","enum":["file_citation"]}},"required":["end_index","file_citation","start_index","text","type"]},{"type":"object","properties":{"end_index":{"type":"integer"},"file_path":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the file that was generated."}},"required":["file_id"]},"start_index":{"type":"integer"},"text":{"type":"string","description":"The text in the message content that needs to be replaced."},"type":{"type":"string","enum":["file_path"]}},"required":["end_index","file_path","start_index","text","type"]}]}},"value":{"type":"string"}},"required":["annotations","value"]},"type":{"type":"string","enum":["text"]}},"required":["text","type"]},{"type":"object","properties":{"refusal":{"type":"string"},"type":{"type":"string","enum":["refusal"]}},"required":["refusal","type"]}]}}],"description":"The content of the message in array of text and/or images."},"attachments":{"type":"array","nullable":true,"items":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the file to attach to the Message."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter","file_search"],"description":"The type of tool being defined."}},"required":["type"]},"description":"The tools to which this file should be added."}}},"description":"A list of files attached to the Message, and the tools they should be added to."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"id":{"type":"string","description":"The identifier, which can be referenced in API endpoints."},"object":{"type":"string","enum":["thread.message"],"description":"The object type."},"status":{"type":"string","enum":["completed","incomplete","in_progress"],"description":"The status of the message."},"created_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when the message was created."},"completed_at":{"type":"integer","nullable":true,"description":"The Unix timestamp (in seconds) for when the message was completed."},"incomplete_at":{"type":"integer","nullable":true,"description":"The Unix timestamp (in seconds) for when the message was marked as incomplete."},"incomplete_details":{"type":"object","nullable":true,"properties":{"reason":{"type":"string","description":"The reason the message is incomplete."}},"required":["reason"],"description":"On an incomplete message, details about why the message is incomplete."},"assistant_id":{"type":"string","nullable":true,"description":"If applicable, the ID of the assistant that authored this message."},"thread_id":{"type":"string","description":"The thread ID that this message belongs to."},"run_id":{"type":"string","nullable":true}},"required":["role","content","id","object","status","created_at","completed_at","incomplete_at","incomplete_details","assistant_id","thread_id","run_id"]},"LlmThreadMessage.v1.UpdateThreadMessageDTO":{"type":"object","properties":{"metadata":{"type":"object","additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."}},"required":["metadata"],"additionalProperties":false},"LlmThreadMessage.v1.UpdateThreadMessageResponseDTO":{"type":"object","properties":{"role":{"type":"string","enum":["user","assistant"],"description":"The role of the entity that is creating the Message"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"image_file":{"type":"object","properties":{"file_id":{"type":"string","description":"The File ID of the image in the Message content."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image if specified by the user. Low uses fewer tokens, you can opt in to high resolution using high."}},"required":["file_id"]},"type":{"type":"string","enum":["image_file"]}},"required":["image_file","type"]},{"type":"object","properties":{"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"The external URL of the image, must be a supported image types: jpeg, jpg, png, gif, webp."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image. Low uses fewer tokens, you can opt in to high resolution using high. Default value is auto."}},"required":["url"]},"type":{"type":"string","enum":["image_url"]}},"required":["image_url","type"]},{"type":"object","properties":{"text":{"type":"object","properties":{"annotations":{"type":"array","items":{"anyOf":[{"type":"object","properties":{"end_index":{"type":"integer"},"file_citation":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the specific File the citation is from."}},"required":["file_id"]},"start_index":{"type":"integer"},"text":{"type":"string","description":"The text in the message content that needs to be replaced."},"type":{"type":"string","enum":["file_citation"]}},"required":["end_index","file_citation","start_index","text","type"]},{"type":"object","properties":{"end_index":{"type":"integer"},"file_path":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the file that was generated."}},"required":["file_id"]},"start_index":{"type":"integer"},"text":{"type":"string","description":"The text in the message content that needs to be replaced."},"type":{"type":"string","enum":["file_path"]}},"required":["end_index","file_path","start_index","text","type"]}]}},"value":{"type":"string"}},"required":["annotations","value"]},"type":{"type":"string","enum":["text"]}},"required":["text","type"]},{"type":"object","properties":{"refusal":{"type":"string"},"type":{"type":"string","enum":["refusal"]}},"required":["refusal","type"]}]}}],"description":"The content of the message in array of text and/or images."},"attachments":{"type":"array","nullable":true,"items":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the file to attach to the Message."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter","file_search"],"description":"The type of tool being defined."}},"required":["type"]},"description":"The tools to which this file should be added."}}},"description":"A list of files attached to the Message, and the tools they should be added to."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"id":{"type":"string","description":"The identifier, which can be referenced in API endpoints."},"object":{"type":"string","enum":["thread.message"],"description":"The object type."},"status":{"type":"string","enum":["completed","incomplete","in_progress"],"description":"The status of the message."},"created_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when the message was created."},"completed_at":{"type":"integer","nullable":true,"description":"The Unix timestamp (in seconds) for when the message was completed."},"incomplete_at":{"type":"integer","nullable":true,"description":"The Unix timestamp (in seconds) for when the message was marked as incomplete."},"incomplete_details":{"type":"object","nullable":true,"properties":{"reason":{"type":"string","description":"The reason the message is incomplete."}},"required":["reason"],"description":"On an incomplete message, details about why the message is incomplete."},"assistant_id":{"type":"string","nullable":true,"description":"If applicable, the ID of the assistant that authored this message."},"thread_id":{"type":"string","description":"The thread ID that this message belongs to."},"run_id":{"type":"string","nullable":true}},"required":["role","content","id","object","status","created_at","completed_at","incomplete_at","incomplete_details","assistant_id","thread_id","run_id"]},"LlmThreadMessage.v1.DeleteThreadMessageResponseDTO":{"type":"object","properties":{"id":{"type":"string"},"object":{"type":"string","enum":["thread.message.deleted"]},"deleted":{"type":"boolean"}},"required":["id","object","deleted"]},"LlmThreadRun.v1.CreateThreadRunDTO":{"type":"object","properties":{"assistant_id":{"type":"string","description":"The ID of the Assistant to use to execute this Run."},"additional_instructions":{"type":"string","nullable":true,"description":"Appends additional instructions at the end of the instructions for the Run. This is useful for modifying the behavior on a per-Run basis without overriding other instructions."},"additional_messages":{"type":"array","nullable":true,"items":{"type":"object","properties":{"role":{"type":"string","enum":["user","assistant"],"description":"The role of the entity that is creating the Message"},"content":{"anyOf":[{"type":"string","description":"The text contents of the Message"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"image_file":{"type":"object","properties":{"file_id":{"type":"string","description":"The File ID of the image in the Message content."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image if specified by the user. Low uses fewer tokens, you can opt in to high resolution using high."}},"required":["file_id"]},"type":{"type":"string","enum":["image_file"]}},"required":["image_file","type"]},{"type":"object","properties":{"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"The external URL of the image, must be a supported image types: jpeg, jpg, png, gif, webp."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image. Low uses fewer tokens, you can opt in to high resolution using high. Default value is auto."}},"required":["url"]},"type":{"type":"string","enum":["image_url"]}},"required":["image_url","type"]},{"type":"object","properties":{"text":{"type":"string","description":"Text content to be sent to the model"},"type":{"type":"string","enum":["text"]}},"required":["text","type"]}]},"description":"An array of content parts with a defined type, each can be of type text or images can be passed with image_url or image_file"}]},"attachments":{"type":"array","nullable":true,"items":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the file to attach to the Message."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter","file_search"],"description":"The type of tool being defined."}},"required":["type"]},"description":"The tools to which this file should be added."}}},"description":"A list of files attached to the Message, and the tools they should be added to."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."}},"required":["role","content"]},"description":"Adds additional Messages to the Thread before creating the Run."},"instructions":{"type":"string","nullable":true,"description":"Overrides the instructions of the Assistant. This is useful for modifying the behavior on a per-Run basis."},"max_completion_tokens":{"type":"integer","nullable":true,"description":"The maximum number of completion tokens that may be used over the course of the Run. The Run will make a best effort to use only the number of completion tokens specified, across multiple turns of the Run. If the Run exceeds the number of completion tokens specified, the Run will end with status incomplete"},"max_prompt_tokens":{"type":"integer","nullable":true,"description":"The maximum number of prompt tokens that may be used over the course of the Run. The Run will make a best effort to use only the number of prompt tokens specified, across multiple turns of the Run. If the Run exceeds the number of prompt tokens specified, the Run will end with status incomplete."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"model":{"type":"string","enum":["openai/gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","gpt-4o-mini","gpt-4o-mini-2024-07-18","chatgpt-4o-latest","gpt-4-turbo","gpt-4-turbo-2024-04-09","gpt-4","gpt-4-0125-preview","gpt-4-1106-preview","gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106","o1-preview","o1-preview-2024-09-12","o1-mini","o1-mini-2024-09-12","o3-mini","gpt-4.5-preview","gpt-4o-audio-preview","gpt-4o-mini-audio-preview","gpt-4o-search-preview","gpt-4o-mini-search-preview","openai/gpt-4.1-2025-04-14","openai/gpt-4.1-mini-2025-04-14","openai/gpt-4.1-nano-2025-04-14","openai/o4-mini-2025-04-16"],"description":"The ID of the model to be used to execute this Run. If a value is provided here, it will override the model associated with the Assistant. If not, the model associated with the Assistant will be used."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models."},"response_format":{"anyOf":[{"type":"string","enum":["auto"]},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}]},{"nullable":true}],"description":"Specifies the format that the model must output."},"stream":{"type":"boolean","nullable":true,"description":"If True, returns a stream of events that happen during the Run as server-sent events."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"]},{"type":"object","properties":{"type":{"type":"string","enum":["function","code_interpreter","file_search"],"description":"The type of the tool. If type is function, the function name must be set."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type"]},{"nullable":true}],"description":"Controls which (if any) tool is called by the model. \n- none means the model will not call any tools and instead generates a message. \n- auto is the default value and means the model can pick between generating a message or calling one or more tools. \n- required means the model must call one or more tools before responding to the user. \nSpecifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool."},"tools":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of tool being defined."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["file_search"],"description":"The type of tool being defined."},"file_search":{"type":"object","properties":{"max_num_results":{"type":"integer","minimum":1,"maximum":50,"description":"The maximum number of results the file search tool should output"},"ranking_options":{"type":"object","properties":{"score_threshold":{"type":"number","minimum":0,"maximum":1,"description":"The score threshold for the file search. All values must be a floating point number between 0 and 1"},"ranker":{"type":"string","enum":["auto","default_2024_08_21"],"description":"The ranker to use for the file search. If not specified will use the auto ranker"}},"required":["score_threshold"],"description":"The ranking options for the file search. If not specified, the file search tool will use the auto ranker and a score_threshold of 0."}},"description":"Overrides for the file search tool"}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of tool being defined."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"The parameters the functions accepts, described as a JSON Schema object"},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True"}},"required":["name"]}},"required":["type","function"]}]},"description":"Override the tools the Assistant can use for this Run. This is useful for modifying the behavior on a per-Run basis."},"top_p":{"type":"number","nullable":true,"minimum":0,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."},"truncation_strategy":{"type":"object","nullable":true,"properties":{"type":{"type":"string","enum":["auto","last_messages"]},"last_messages":{"type":"integer","nullable":true}},"required":["type"],"description":"Controls for how a Thread will be truncated prior to the Run. Use this to control the intial context window of the Run."}},"required":["assistant_id"]},"LlmThreadRun.v1.CreateThreadRunResponseDTO":{"type":"object","properties":{"assistant_id":{"type":"string"},"cancelled_at":{"type":"integer","nullable":true},"completed_at":{"type":"integer","nullable":true},"created_at":{"type":"integer"},"expires_at":{"type":"integer","nullable":true},"failed_at":{"type":"integer","nullable":true},"id":{"type":"string"},"incomplete_details":{"type":"object","nullable":true,"properties":{"reason":{"type":"string"}}},"instructions":{"type":"string"},"last_error":{"type":"object","nullable":true,"properties":{"code":{"type":"string","enum":["server_error","rate_limit_exceeded","invalid_prompt"]},"message":{"type":"string"}},"required":["code","message"]},"max_completion_tokens":{"type":"integer","nullable":true},"max_prompt_tokens":{"type":"integer","nullable":true},"metadata":{"nullable":true},"model":{"type":"string"},"object":{"type":"string","enum":["thread.run"]},"parallel_tool_calls":{"type":"boolean"},"required_action":{"type":"object","nullable":true,"properties":{"submit_tool_outputs":{"type":"object","properties":{"tool_calls":{"type":"array","items":{"type":"object","properties":{"function":{"type":"object","properties":{"arguments":{"type":"string"},"name":{"type":"string"}},"required":["arguments","name"]},"id":{"type":"string"},"type":{"type":"string","enum":["function"]}},"required":["function","id","type"]}}},"required":["tool_calls"]},"type":{"type":"string","enum":["submit_tool_outputs"]}},"required":["submit_tool_outputs","type"]},"response_format":{"anyOf":[{"type":"string","enum":["auto"]},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}]},{"nullable":true}]},"started_at":{"type":"integer","nullable":true},"status":{"type":"string","enum":["queued","in_progress","requires_action","cancelling","cancelled","failed","completed","incomplete","expired"]},"temperature":{"type":"number","nullable":true},"thread_id":{"type":"string"},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"]},{"type":"object","properties":{"type":{"type":"string"},"function":{"type":"object","nullable":true,"properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type"]},{"nullable":true}]},"tools":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of tool being defined."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["file_search"],"description":"The type of tool being defined."},"file_search":{"type":"object","properties":{"max_num_results":{"type":"integer","minimum":1,"maximum":50,"description":"The maximum number of results the file search tool should output"},"ranking_options":{"type":"object","properties":{"score_threshold":{"type":"number","minimum":0,"maximum":1,"description":"The score threshold for the file search. All values must be a floating point number between 0 and 1"},"ranker":{"type":"string","enum":["auto","default_2024_08_21"],"description":"The ranker to use for the file search. If not specified will use the auto ranker"}},"required":["score_threshold"],"description":"The ranking options for the file search. If not specified, the file search tool will use the auto ranker and a score_threshold of 0."}},"description":"Overrides for the file search tool"}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of tool being defined."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"The parameters the functions accepts, described as a JSON Schema object"},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True"}},"required":["name"]}},"required":["type","function"]}]}},"top_p":{"type":"number","nullable":true},"truncation_strategy":{"type":"object","nullable":true,"properties":{"type":{"type":"string"},"last_messages":{"type":"integer","nullable":true}},"required":["type"]},"usage":{"type":"object","nullable":true,"properties":{"completion_tokens":{"type":"integer"},"prompt_tokens":{"type":"integer"},"total_tokens":{"type":"integer"}},"required":["completion_tokens","prompt_tokens","total_tokens"]}},"required":["assistant_id","cancelled_at","completed_at","created_at","expires_at","failed_at","id","incomplete_details","instructions","last_error","max_completion_tokens","max_prompt_tokens","model","object","parallel_tool_calls","required_action","response_format","started_at","status","thread_id"]},"LlmThreadRun.v1.CreateThreadAndRunDTO":{"type":"object","properties":{"assistant_id":{"type":"string","description":"The ID of the Assistant to use to execute this Run."},"instructions":{"type":"string","nullable":true,"description":"Overrides the instructions of the Assistant. This is useful for modifying the behavior on a per-Run basis."},"max_completion_tokens":{"type":"integer","nullable":true,"description":"The maximum number of completion tokens that may be used over the course of the Run. The Run will make a best effort to use only the number of completion tokens specified, across multiple turns of the Run. If the Run exceeds the number of completion tokens specified, the Run will end with status incomplete"},"max_prompt_tokens":{"type":"integer","nullable":true,"description":"The maximum number of prompt tokens that may be used over the course of the Run. The Run will make a best effort to use only the number of prompt tokens specified, across multiple turns of the Run. If the Run exceeds the number of prompt tokens specified, the Run will end with status incomplete."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"model":{"type":"string","nullable":true,"enum":["openai/gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","gpt-4o-mini","gpt-4o-mini-2024-07-18","chatgpt-4o-latest","gpt-4-turbo","gpt-4-turbo-2024-04-09","gpt-4","gpt-4-0125-preview","gpt-4-1106-preview","gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106","o1-preview","o1-preview-2024-09-12","o1-mini","o1-mini-2024-09-12","o3-mini","gpt-4.5-preview","gpt-4o-audio-preview","gpt-4o-mini-audio-preview","gpt-4o-search-preview","gpt-4o-mini-search-preview","openai/gpt-4.1-2025-04-14","openai/gpt-4.1-mini-2025-04-14","openai/gpt-4.1-nano-2025-04-14","openai/o4-mini-2025-04-16"],"description":"The ID of the model to be used to execute this Run. If a value is provided here, it will override the model associated with the Assistant. If not, the model associated with the Assistant will be used."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"response_format":{"anyOf":[{"type":"string","enum":["auto"]},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}]},{"nullable":true}],"description":"Specifies the format that the model must output."},"stream":{"type":"boolean","nullable":true,"description":"If True, returns a stream of events that happen during the Run as server-sent events."},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."},"thread":{"type":"object","properties":{"messages":{"type":"array","items":{"type":"object","properties":{"role":{"type":"string","enum":["user","assistant"],"description":"The role of the entity that is creating the Message"},"content":{"anyOf":[{"type":"string","description":"The text contents of the Message"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"image_file":{"type":"object","properties":{"file_id":{"type":"string","description":"The File ID of the image in the Message content."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image if specified by the user. Low uses fewer tokens, you can opt in to high resolution using high."}},"required":["file_id"]},"type":{"type":"string","enum":["image_file"]}},"required":["image_file","type"]},{"type":"object","properties":{"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"The external URL of the image, must be a supported image types: jpeg, jpg, png, gif, webp."},"detail":{"type":"string","enum":["auto","low","high"],"description":"Specifies the detail level of the image. Low uses fewer tokens, you can opt in to high resolution using high. Default value is auto."}},"required":["url"]},"type":{"type":"string","enum":["image_url"]}},"required":["image_url","type"]},{"type":"object","properties":{"text":{"type":"string","description":"Text content to be sent to the model"},"type":{"type":"string","enum":["text"]}},"required":["text","type"]}]},"description":"An array of content parts with a defined type, each can be of type text or images can be passed with image_url or image_file"}]},"attachments":{"type":"array","nullable":true,"items":{"type":"object","properties":{"file_id":{"type":"string","description":"The ID of the file to attach to the Message."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter","file_search"],"description":"The type of tool being defined."}},"required":["type"]},"description":"The tools to which this file should be added."}}},"description":"A list of files attached to the Message, and the tools they should be added to."},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."}},"required":["role","content"]}},"metadata":{"type":"object","nullable":true,"additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."},"tool_resources":{"type":"object","nullable":true,"properties":{"code_interpreter":{"type":"object","properties":{"file_ids":{"type":"array","items":{"nullable":true},"description":"A list of file IDs made available to the code_interpreter tool. There can be a maximum of 20 files associated with the tool."}}},"file_search":{"type":"object","properties":{"vector_store_ids":{"type":"array","items":{"type":"string"},"description":"The vector store attached to this Assistant. There can be a maximum of 1 vector store attached to the Assistant."},"vector_stores":{"type":"array","items":{"type":"object","properties":{"chunking_strategy":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["static"]},"static":{"type":"object","properties":{"chunk_overlap_tokens":{"type":"integer","description":"The number of tokens that overlap between chunks. The default value is 400. Note that the overlap must not exceed half of max_chunk_size_tokens."},"max_chunk_size_tokens":{"type":"integer","description":"The maximum number of tokens in each chunk. The default value is 800. The minimum value is 100 and the maximum value is 4096"}},"required":["chunk_overlap_tokens","max_chunk_size_tokens"]}},"required":["type","static"]}],"description":"The chunking strategy used to chunk the file(s). If not set, will use the auto strategy."},"file_ids":{"type":"array","items":{"type":"string"},"description":"A list of file IDs to add to the vector store. There can be a maximum of 10000 files in a vector store."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."}}},"description":"A helper to create a vector store with file_ids and attach it to this Assistant. There can be a maximum of 1 vector store attached to the Assistant."}}}},"description":"A set of resources that are made available to the assistant's tools in this Thread. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs."}},"required":["messages"],"description":"Options to create a new Thread. If no Thread is provided when running a request, an empty Thread will be created."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"]},{"type":"object","properties":{"type":{"type":"string","enum":["function","code_interpreter","file_search"],"description":"The type of the tool. If type is function, the function name must be set."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type"]}],"description":"Controls which (if any) tool is called by the model. \n- none means the model will not call any tools and instead generates a message. \n- auto is the default value and means the model can pick between generating a message or calling one or more tools. \n- required means the model must call one or more tools before responding to the user. \nSpecifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool."},"tool_resources":{"type":"object","nullable":true,"properties":{"code_interpreter":{"type":"object","properties":{"file_ids":{"type":"array","items":{"nullable":true},"description":"A list of file IDs made available to the code_interpreter tool. There can be a maximum of 20 files associated with the tool."}}},"file_search":{"type":"object","properties":{"vector_store_ids":{"type":"array","items":{"type":"string"},"description":"The vector store attached to this Assistant. There can be a maximum of 1 vector store attached to the Assistant."},"vector_stores":{"type":"array","items":{"type":"object","properties":{"chunking_strategy":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["static"]},"static":{"type":"object","properties":{"chunk_overlap_tokens":{"type":"integer","description":"The number of tokens that overlap between chunks. The default value is 400. Note that the overlap must not exceed half of max_chunk_size_tokens."},"max_chunk_size_tokens":{"type":"integer","description":"The maximum number of tokens in each chunk. The default value is 800. The minimum value is 100 and the maximum value is 4096"}},"required":["chunk_overlap_tokens","max_chunk_size_tokens"]}},"required":["type","static"]}],"description":"The chunking strategy used to chunk the file(s). If not set, will use the auto strategy."},"file_ids":{"type":"array","items":{"type":"string"},"description":"A list of file IDs to add to the vector store. There can be a maximum of 10000 files in a vector store."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."}}},"description":"A helper to create a vector store with file_ids and attach it to this Assistant. There can be a maximum of 1 vector store attached to the Assistant."}}}},"description":"A set of resources that are used by the Assistant's tools. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs."},"tools":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of tool being defined."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["file_search"],"description":"The type of tool being defined."},"file_search":{"type":"object","properties":{"max_num_results":{"type":"integer","minimum":1,"maximum":50,"description":"The maximum number of results the file search tool should output"},"ranking_options":{"type":"object","properties":{"score_threshold":{"type":"number","minimum":0,"maximum":1,"description":"The score threshold for the file search. All values must be a floating point number between 0 and 1"},"ranker":{"type":"string","enum":["auto","default_2024_08_21"],"description":"The ranker to use for the file search. If not specified will use the auto ranker"}},"required":["score_threshold"],"description":"The ranking options for the file search. If not specified, the file search tool will use the auto ranker and a score_threshold of 0."}},"description":"Overrides for the file search tool"}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of tool being defined."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"The parameters the functions accepts, described as a JSON Schema object"},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True"}},"required":["name"]}},"required":["type","function"]}]},"description":"Override the tools the Assistant can use for this Run. This is useful for modifying the behavior on a per-Run basis."},"top_p":{"type":"number","nullable":true,"minimum":0,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."},"truncation_strategy":{"type":"object","nullable":true,"properties":{"type":{"type":"string","enum":["auto","last_messages"],"description":"The truncation strategy to use for the thread. The default is auto. If set to last_messages, the Thread will be truncated to the n most recent Messages in the Thread. When set to auto, Messages in the middle of the Thread will be dropped to fit the context length of the model, max_prompt_tokens."},"last_messages":{"type":"integer","nullable":true,"description":"The number of most recent Messages from the Thread when constructing the context for the Run."}},"required":["type"],"description":"Controls for how a Thread will be truncated prior to the Run. Use this to control the intial context window of the Run."}},"required":["assistant_id"]},"LlmThreadRun.v1.GetThreadRunsDTO":{"type":"object","properties":{"limit":{"type":"integer","minimum":1,"maximum":100,"description":"A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20."},"order":{"type":"string","enum":["asc","desc"],"description":"Sort order by the created_at timestamp of the objects. asc for ascending order and desc for descending order."},"before":{"type":"string","description":"A cursor for use in pagination. before is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, starting with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list."},"after":{"type":"string","description":"A cursor for use in pagination. after is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list."}},"additionalProperties":false},"LlmThreadRun.v1.GetThreadRunsResponseDTO":{"type":"object","properties":{"object":{"type":"string","enum":["list"]},"data":{"type":"array","items":{"type":"object","properties":{"assistant_id":{"type":"string"},"cancelled_at":{"type":"integer","nullable":true},"completed_at":{"type":"integer","nullable":true},"created_at":{"type":"integer"},"expires_at":{"type":"integer","nullable":true},"failed_at":{"type":"integer","nullable":true},"id":{"type":"string"},"incomplete_details":{"type":"object","nullable":true,"properties":{"reason":{"type":"string"}}},"instructions":{"type":"string"},"last_error":{"type":"object","nullable":true,"properties":{"code":{"type":"string","enum":["server_error","rate_limit_exceeded","invalid_prompt"]},"message":{"type":"string"}},"required":["code","message"]},"max_completion_tokens":{"type":"integer","nullable":true},"max_prompt_tokens":{"type":"integer","nullable":true},"metadata":{"nullable":true},"model":{"type":"string"},"object":{"type":"string","enum":["thread.run"]},"parallel_tool_calls":{"type":"boolean"},"required_action":{"type":"object","nullable":true,"properties":{"submit_tool_outputs":{"type":"object","properties":{"tool_calls":{"type":"array","items":{"type":"object","properties":{"function":{"type":"object","properties":{"arguments":{"type":"string"},"name":{"type":"string"}},"required":["arguments","name"]},"id":{"type":"string"},"type":{"type":"string","enum":["function"]}},"required":["function","id","type"]}}},"required":["tool_calls"]},"type":{"type":"string","enum":["submit_tool_outputs"]}},"required":["submit_tool_outputs","type"]},"response_format":{"anyOf":[{"type":"string","enum":["auto"]},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}]},{"nullable":true}]},"started_at":{"type":"integer","nullable":true},"status":{"type":"string","enum":["queued","in_progress","requires_action","cancelling","cancelled","failed","completed","incomplete","expired"]},"temperature":{"type":"number","nullable":true},"thread_id":{"type":"string"},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"]},{"type":"object","properties":{"type":{"type":"string"},"function":{"type":"object","nullable":true,"properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type"]},{"nullable":true}]},"tools":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of tool being defined."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["file_search"],"description":"The type of tool being defined."},"file_search":{"type":"object","properties":{"max_num_results":{"type":"integer","minimum":1,"maximum":50,"description":"The maximum number of results the file search tool should output"},"ranking_options":{"type":"object","properties":{"score_threshold":{"type":"number","minimum":0,"maximum":1,"description":"The score threshold for the file search. All values must be a floating point number between 0 and 1"},"ranker":{"type":"string","enum":["auto","default_2024_08_21"],"description":"The ranker to use for the file search. If not specified will use the auto ranker"}},"required":["score_threshold"],"description":"The ranking options for the file search. If not specified, the file search tool will use the auto ranker and a score_threshold of 0."}},"description":"Overrides for the file search tool"}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of tool being defined."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"The parameters the functions accepts, described as a JSON Schema object"},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True"}},"required":["name"]}},"required":["type","function"]}]}},"top_p":{"type":"number","nullable":true},"truncation_strategy":{"type":"object","nullable":true,"properties":{"type":{"type":"string"},"last_messages":{"type":"integer","nullable":true}},"required":["type"]},"usage":{"type":"object","nullable":true,"properties":{"completion_tokens":{"type":"integer"},"prompt_tokens":{"type":"integer"},"total_tokens":{"type":"integer"}},"required":["completion_tokens","prompt_tokens","total_tokens"]}},"required":["assistant_id","cancelled_at","completed_at","created_at","expires_at","failed_at","id","incomplete_details","instructions","last_error","max_completion_tokens","max_prompt_tokens","model","object","parallel_tool_calls","required_action","response_format","started_at","status","thread_id"]}},"first_id":{"type":"string"},"last_id":{"type":"string"},"has_more":{"type":"boolean"}},"required":["object","data","first_id","last_id","has_more"]},"LlmThreadRun.v1.GetThreadRunReponsetDTO":{"type":"object","properties":{"assistant_id":{"type":"string"},"cancelled_at":{"type":"integer","nullable":true},"completed_at":{"type":"integer","nullable":true},"created_at":{"type":"integer"},"expires_at":{"type":"integer","nullable":true},"failed_at":{"type":"integer","nullable":true},"id":{"type":"string"},"incomplete_details":{"type":"object","nullable":true,"properties":{"reason":{"type":"string"}}},"instructions":{"type":"string"},"last_error":{"type":"object","nullable":true,"properties":{"code":{"type":"string","enum":["server_error","rate_limit_exceeded","invalid_prompt"]},"message":{"type":"string"}},"required":["code","message"]},"max_completion_tokens":{"type":"integer","nullable":true},"max_prompt_tokens":{"type":"integer","nullable":true},"metadata":{"nullable":true},"model":{"type":"string"},"object":{"type":"string","enum":["thread.run"]},"parallel_tool_calls":{"type":"boolean"},"required_action":{"type":"object","nullable":true,"properties":{"submit_tool_outputs":{"type":"object","properties":{"tool_calls":{"type":"array","items":{"type":"object","properties":{"function":{"type":"object","properties":{"arguments":{"type":"string"},"name":{"type":"string"}},"required":["arguments","name"]},"id":{"type":"string"},"type":{"type":"string","enum":["function"]}},"required":["function","id","type"]}}},"required":["tool_calls"]},"type":{"type":"string","enum":["submit_tool_outputs"]}},"required":["submit_tool_outputs","type"]},"response_format":{"anyOf":[{"type":"string","enum":["auto"]},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}]},{"nullable":true}]},"started_at":{"type":"integer","nullable":true},"status":{"type":"string","enum":["queued","in_progress","requires_action","cancelling","cancelled","failed","completed","incomplete","expired"]},"temperature":{"type":"number","nullable":true},"thread_id":{"type":"string"},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"]},{"type":"object","properties":{"type":{"type":"string"},"function":{"type":"object","nullable":true,"properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type"]},{"nullable":true}]},"tools":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of tool being defined."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["file_search"],"description":"The type of tool being defined."},"file_search":{"type":"object","properties":{"max_num_results":{"type":"integer","minimum":1,"maximum":50,"description":"The maximum number of results the file search tool should output"},"ranking_options":{"type":"object","properties":{"score_threshold":{"type":"number","minimum":0,"maximum":1,"description":"The score threshold for the file search. All values must be a floating point number between 0 and 1"},"ranker":{"type":"string","enum":["auto","default_2024_08_21"],"description":"The ranker to use for the file search. If not specified will use the auto ranker"}},"required":["score_threshold"],"description":"The ranking options for the file search. If not specified, the file search tool will use the auto ranker and a score_threshold of 0."}},"description":"Overrides for the file search tool"}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of tool being defined."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"The parameters the functions accepts, described as a JSON Schema object"},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True"}},"required":["name"]}},"required":["type","function"]}]}},"top_p":{"type":"number","nullable":true},"truncation_strategy":{"type":"object","nullable":true,"properties":{"type":{"type":"string"},"last_messages":{"type":"integer","nullable":true}},"required":["type"]},"usage":{"type":"object","nullable":true,"properties":{"completion_tokens":{"type":"integer"},"prompt_tokens":{"type":"integer"},"total_tokens":{"type":"integer"}},"required":["completion_tokens","prompt_tokens","total_tokens"]}},"required":["assistant_id","cancelled_at","completed_at","created_at","expires_at","failed_at","id","incomplete_details","instructions","last_error","max_completion_tokens","max_prompt_tokens","model","object","parallel_tool_calls","required_action","response_format","started_at","status","thread_id"]},"LlmThreadRun.v1.UpdateThreadRunDTO":{"type":"object","properties":{"metadata":{"type":"object","additionalProperties":{"type":"string"},"description":"Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.\n  Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters."}},"required":["metadata"],"additionalProperties":false},"LlmThreadRun.v1.UpdateThreadRunResponseDTO":{"type":"object","properties":{"assistant_id":{"type":"string"},"cancelled_at":{"type":"integer","nullable":true},"completed_at":{"type":"integer","nullable":true},"created_at":{"type":"integer"},"expires_at":{"type":"integer","nullable":true},"failed_at":{"type":"integer","nullable":true},"id":{"type":"string"},"incomplete_details":{"type":"object","nullable":true,"properties":{"reason":{"type":"string"}}},"instructions":{"type":"string"},"last_error":{"type":"object","nullable":true,"properties":{"code":{"type":"string","enum":["server_error","rate_limit_exceeded","invalid_prompt"]},"message":{"type":"string"}},"required":["code","message"]},"max_completion_tokens":{"type":"integer","nullable":true},"max_prompt_tokens":{"type":"integer","nullable":true},"metadata":{"nullable":true},"model":{"type":"string"},"object":{"type":"string","enum":["thread.run"]},"parallel_tool_calls":{"type":"boolean"},"required_action":{"type":"object","nullable":true,"properties":{"submit_tool_outputs":{"type":"object","properties":{"tool_calls":{"type":"array","items":{"type":"object","properties":{"function":{"type":"object","properties":{"arguments":{"type":"string"},"name":{"type":"string"}},"required":["arguments","name"]},"id":{"type":"string"},"type":{"type":"string","enum":["function"]}},"required":["function","id","type"]}}},"required":["tool_calls"]},"type":{"type":"string","enum":["submit_tool_outputs"]}},"required":["submit_tool_outputs","type"]},"response_format":{"anyOf":[{"type":"string","enum":["auto"]},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}]},{"nullable":true}]},"started_at":{"type":"integer","nullable":true},"status":{"type":"string","enum":["queued","in_progress","requires_action","cancelling","cancelled","failed","completed","incomplete","expired"]},"temperature":{"type":"number","nullable":true},"thread_id":{"type":"string"},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"]},{"type":"object","properties":{"type":{"type":"string"},"function":{"type":"object","nullable":true,"properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type"]},{"nullable":true}]},"tools":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of tool being defined."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["file_search"],"description":"The type of tool being defined."},"file_search":{"type":"object","properties":{"max_num_results":{"type":"integer","minimum":1,"maximum":50,"description":"The maximum number of results the file search tool should output"},"ranking_options":{"type":"object","properties":{"score_threshold":{"type":"number","minimum":0,"maximum":1,"description":"The score threshold for the file search. All values must be a floating point number between 0 and 1"},"ranker":{"type":"string","enum":["auto","default_2024_08_21"],"description":"The ranker to use for the file search. If not specified will use the auto ranker"}},"required":["score_threshold"],"description":"The ranking options for the file search. If not specified, the file search tool will use the auto ranker and a score_threshold of 0."}},"description":"Overrides for the file search tool"}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of tool being defined."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"The parameters the functions accepts, described as a JSON Schema object"},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True"}},"required":["name"]}},"required":["type","function"]}]}},"top_p":{"type":"number","nullable":true},"truncation_strategy":{"type":"object","nullable":true,"properties":{"type":{"type":"string"},"last_messages":{"type":"integer","nullable":true}},"required":["type"]},"usage":{"type":"object","nullable":true,"properties":{"completion_tokens":{"type":"integer"},"prompt_tokens":{"type":"integer"},"total_tokens":{"type":"integer"}},"required":["completion_tokens","prompt_tokens","total_tokens"]}},"required":["assistant_id","cancelled_at","completed_at","created_at","expires_at","failed_at","id","incomplete_details","instructions","last_error","max_completion_tokens","max_prompt_tokens","model","object","parallel_tool_calls","required_action","response_format","started_at","status","thread_id"]},"LlmThreadRun.v1.SubmitToolToRunDTO":{"type":"object","properties":{"tool_outputs":{"type":"array","items":{"type":"object","properties":{"output":{"type":"string","description":"The output of the tool call to be submitted to continue the Run."},"tool_call_id":{"type":"string","description":"The ID of the tool call in the required_action object within the Run object the output is being submitted for."}}},"description":"A list of tools for which the outputs are being submitted."},"stream":{"type":"boolean","nullable":true,"description":"If True, returns a stream of events that happen during the Run as server-sent events."}},"required":["tool_outputs"],"additionalProperties":false},"LlmThreadRun.v1.SubmitToolToRunResponseDTO":{"type":"object","properties":{"assistant_id":{"type":"string"},"cancelled_at":{"type":"integer","nullable":true},"completed_at":{"type":"integer","nullable":true},"created_at":{"type":"integer"},"expires_at":{"type":"integer","nullable":true},"failed_at":{"type":"integer","nullable":true},"id":{"type":"string"},"incomplete_details":{"type":"object","nullable":true,"properties":{"reason":{"type":"string"}}},"instructions":{"type":"string"},"last_error":{"type":"object","nullable":true,"properties":{"code":{"type":"string","enum":["server_error","rate_limit_exceeded","invalid_prompt"]},"message":{"type":"string"}},"required":["code","message"]},"max_completion_tokens":{"type":"integer","nullable":true},"max_prompt_tokens":{"type":"integer","nullable":true},"metadata":{"nullable":true},"model":{"type":"string"},"object":{"type":"string","enum":["thread.run"]},"parallel_tool_calls":{"type":"boolean"},"required_action":{"type":"object","nullable":true,"properties":{"submit_tool_outputs":{"type":"object","properties":{"tool_calls":{"type":"array","items":{"type":"object","properties":{"function":{"type":"object","properties":{"arguments":{"type":"string"},"name":{"type":"string"}},"required":["arguments","name"]},"id":{"type":"string"},"type":{"type":"string","enum":["function"]}},"required":["function","id","type"]}}},"required":["tool_calls"]},"type":{"type":"string","enum":["submit_tool_outputs"]}},"required":["submit_tool_outputs","type"]},"response_format":{"anyOf":[{"type":"string","enum":["auto"]},{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}]},{"nullable":true}]},"started_at":{"type":"integer","nullable":true},"status":{"type":"string","enum":["queued","in_progress","requires_action","cancelling","cancelled","failed","completed","incomplete","expired"]},"temperature":{"type":"number","nullable":true},"thread_id":{"type":"string"},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"]},{"type":"object","properties":{"type":{"type":"string"},"function":{"type":"object","nullable":true,"properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type"]},{"nullable":true}]},"tools":{"type":"array","nullable":true,"items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["code_interpreter"],"description":"The type of tool being defined."}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["file_search"],"description":"The type of tool being defined."},"file_search":{"type":"object","properties":{"max_num_results":{"type":"integer","minimum":1,"maximum":50,"description":"The maximum number of results the file search tool should output"},"ranking_options":{"type":"object","properties":{"score_threshold":{"type":"number","minimum":0,"maximum":1,"description":"The score threshold for the file search. All values must be a floating point number between 0 and 1"},"ranker":{"type":"string","enum":["auto","default_2024_08_21"],"description":"The ranker to use for the file search. If not specified will use the auto ranker"}},"required":["score_threshold"],"description":"The ranking options for the file search. If not specified, the file search tool will use the auto ranker and a score_threshold of 0."}},"description":"Overrides for the file search tool"}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of tool being defined."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"parameters":{"type":"object","additionalProperties":{"nullable":true},"description":"The parameters the functions accepts, described as a JSON Schema object"},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True"}},"required":["name"]}},"required":["type","function"]}]}},"top_p":{"type":"number","nullable":true},"truncation_strategy":{"type":"object","nullable":true,"properties":{"type":{"type":"string"},"last_messages":{"type":"integer","nullable":true}},"required":["type"]},"usage":{"type":"object","nullable":true,"properties":{"completion_tokens":{"type":"integer"},"prompt_tokens":{"type":"integer"},"total_tokens":{"type":"integer"}},"required":["completion_tokens","prompt_tokens","total_tokens"]}},"required":["assistant_id","cancelled_at","completed_at","created_at","expires_at","failed_at","id","incomplete_details","instructions","last_error","max_completion_tokens","max_prompt_tokens","model","object","parallel_tool_calls","required_action","response_format","started_at","status","thread_id"]},"LlmThreadRunSteps.v1.GetRunStepsResponseDTO":{"type":"object","properties":{"object":{"type":"string","enum":["list"]},"data":{"type":"array","items":{"type":"object","properties":{"assistant_id":{"type":"string"},"cancelled_at":{"type":"integer","nullable":true},"completed_at":{"type":"integer","nullable":true},"created_at":{"type":"integer"},"expired_at":{"type":"integer","nullable":true},"failed_at":{"type":"integer","nullable":true},"id":{"type":"string"},"last_error":{"type":"object","nullable":true,"properties":{"code":{"type":"string","enum":["server_error","rate_limit_exceeded"]},"message":{"type":"string"}},"required":["code","message"]},"metadata":{"nullable":true},"object":{"type":"string","enum":["thread.run.step"]},"run_id":{"type":"string"},"status":{"type":"string","enum":["in_progress","cancelled","failed","completed","expired"]},"step_details":{"anyOf":[{"type":"object","properties":{"message_creation":{"type":"object","properties":{"message_id":{"type":"string"}},"required":["message_id"]},"type":{"type":"string","enum":["message_creation"]}},"required":["message_creation","type"]},{"type":"object","properties":{"tool_calls":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"id":{"type":"string"},"code_interpreter":{"type":"object","properties":{"input":{"type":"string"},"outputs":{"type":"array","items":{"anyOf":[{"type":"object","properties":{"logs":{"type":"string"},"type":{"type":"string","enum":["logs"]}},"required":["logs","type"]},{"type":"object","properties":{"image":{"type":"object","properties":{"file_id":{"type":"string"}},"required":["file_id"]},"type":{"type":"string","enum":["image"]}},"required":["image","type"]}]}}},"required":["input","outputs"]},"type":{"type":"string","enum":["code_interpreter"]}},"required":["id","code_interpreter","type"]},{"type":"object","properties":{"id":{"type":"string"},"type":{"type":"string","enum":["file_search"]},"file_search":{"type":"object","properties":{"results":{"type":"array","nullable":true,"items":{"type":"object","properties":{"file_id":{"type":"string"},"file_name":{"type":"string"},"score":{"type":"number"},"content":{"type":"array","nullable":true,"items":{"type":"object","properties":{"text":{"type":"string","nullable":true},"type":{"type":"string","nullable":true}}}}},"required":["file_id","file_name","score"]}},"ranking_options":{"type":"object","properties":{"score_threshold":{"type":"number","minimum":0,"maximum":1},"ranker":{"type":"string"}},"required":["score_threshold","ranker"]}}}},"required":["id","type"]},{"type":"object","properties":{"id":{"type":"string"},"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"},"arguments":{"type":"string"},"ouptut":{"type":"string","nullable":true}},"required":["name"]}},"required":["id","type","function"]}]}},"type":{"type":"string","enum":["tool_calls"]}},"required":["tool_calls","type"]}]},"thread_id":{"type":"string"},"type":{"type":"string","enum":["message_creation","tool_calls"]},"usage":{"type":"object","nullable":true,"properties":{"completion_tokens":{"type":"integer"},"prompt_tokens":{"type":"integer"},"total_tokens":{"type":"integer"}},"required":["completion_tokens","prompt_tokens","total_tokens"]}},"required":["assistant_id","cancelled_at","completed_at","created_at","expired_at","failed_at","id","last_error","object","run_id","status","step_details","thread_id","type","usage"]}},"first_id":{"type":"string"},"last_id":{"type":"string"},"has_more":{"type":"boolean"}},"required":["object","data","first_id","last_id","has_more"]},"LlmThreadRunSteps.v1.GetRunStepResponseDTO":{"type":"object","properties":{"assistant_id":{"type":"string"},"cancelled_at":{"type":"integer","nullable":true},"completed_at":{"type":"integer","nullable":true},"created_at":{"type":"integer"},"expired_at":{"type":"integer","nullable":true},"failed_at":{"type":"integer","nullable":true},"id":{"type":"string"},"last_error":{"type":"object","nullable":true,"properties":{"code":{"type":"string","enum":["server_error","rate_limit_exceeded"]},"message":{"type":"string"}},"required":["code","message"]},"metadata":{"nullable":true},"object":{"type":"string","enum":["thread.run.step"]},"run_id":{"type":"string"},"status":{"type":"string","enum":["in_progress","cancelled","failed","completed","expired"]},"step_details":{"anyOf":[{"type":"object","properties":{"message_creation":{"type":"object","properties":{"message_id":{"type":"string"}},"required":["message_id"]},"type":{"type":"string","enum":["message_creation"]}},"required":["message_creation","type"]},{"type":"object","properties":{"tool_calls":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"id":{"type":"string"},"code_interpreter":{"type":"object","properties":{"input":{"type":"string"},"outputs":{"type":"array","items":{"anyOf":[{"type":"object","properties":{"logs":{"type":"string"},"type":{"type":"string","enum":["logs"]}},"required":["logs","type"]},{"type":"object","properties":{"image":{"type":"object","properties":{"file_id":{"type":"string"}},"required":["file_id"]},"type":{"type":"string","enum":["image"]}},"required":["image","type"]}]}}},"required":["input","outputs"]},"type":{"type":"string","enum":["code_interpreter"]}},"required":["id","code_interpreter","type"]},{"type":"object","properties":{"id":{"type":"string"},"type":{"type":"string","enum":["file_search"]},"file_search":{"type":"object","properties":{"results":{"type":"array","nullable":true,"items":{"type":"object","properties":{"file_id":{"type":"string"},"file_name":{"type":"string"},"score":{"type":"number"},"content":{"type":"array","nullable":true,"items":{"type":"object","properties":{"text":{"type":"string","nullable":true},"type":{"type":"string","nullable":true}}}}},"required":["file_id","file_name","score"]}},"ranking_options":{"type":"object","properties":{"score_threshold":{"type":"number","minimum":0,"maximum":1},"ranker":{"type":"string"}},"required":["score_threshold","ranker"]}}}},"required":["id","type"]},{"type":"object","properties":{"id":{"type":"string"},"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"},"arguments":{"type":"string"},"ouptut":{"type":"string","nullable":true}},"required":["name"]}},"required":["id","type","function"]}]}},"type":{"type":"string","enum":["tool_calls"]}},"required":["tool_calls","type"]}]},"thread_id":{"type":"string"},"type":{"type":"string","enum":["message_creation","tool_calls"]},"usage":{"type":"object","nullable":true,"properties":{"completion_tokens":{"type":"integer"},"prompt_tokens":{"type":"integer"},"total_tokens":{"type":"integer"}},"required":["completion_tokens","prompt_tokens","total_tokens"]}},"required":["assistant_id","cancelled_at","completed_at","created_at","expired_at","failed_at","id","last_error","object","run_id","status","step_details","thread_id","type","usage"]},"Voice.v1.SpeechToTextPayloadDTO":{"anyOf":[{"type":"object","properties":{"model":{"type":"string","enum":["#g1_nova-2-general","#g1_nova-2-meeting","#g1_nova-2-phonecall","#g1_nova-2-voicemail","#g1_nova-2-finance","#g1_nova-2-conversationalai","#g1_nova-2-video","#g1_nova-2-medical","#g1_nova-2-drivethru","#g1_nova-2-automotive","#g1_whisper-large","#g1_whisper-medium","#g1_whisper-small","#g1_whisper-tiny","#g1_whisper-base"]},"custom_intent":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}}],"description":"A custom intent you want the model to detect within your input audio if present. Submit up to 100."},"custom_topic":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}}],"description":"A custom topic you want the model to detect within your input audio if present. Submit up to 100."},"custom_intent_mode":{"type":"string","enum":["strict","extended"],"description":"Sets how the model will interpret strings submitted to the custom_intent param. When strict, the model will only return intents submitted using the custom_intent param. When extended, the model will return its own detected intents in addition those submitted using the custom_intents param."},"custom_topic_mode":{"type":"string","enum":["strict","extended"],"description":"Sets how the model will interpret strings submitted to the custom_topic param. When strict, the model will only return topics submitted using the custom_topic param. When extended, the model will return its own detected topics in addition to those submitted using the custom_topic param."},"detect_language":{"type":"boolean","description":"Enables language detection to identify the dominant language spoken in the submitted audio."},"detect_entities":{"type":"boolean","description":"When Entity Detection is enabled, the Punctuation feature will be enabled by default."},"detect_topics":{"type":"boolean","description":"Detects the most important and relevant topics that are referenced in speech within the audio"},"diarize":{"type":"boolean","description":"Recognizes speaker changes. Each word in the transcript will be assigned a speaker number starting at 0"},"dictation":{"type":"boolean","description":"Identifies and extracts key entities from content in submitted audio"},"diarize_version":{"type":"string","description":""},"extra":{"type":"string","description":"Arbitrary key-value pairs that are attached to the API response for usage in downstream processing"},"filler_words":{"type":"boolean","description":"Filler Words can help transcribe interruptions in your audio, like “uh” and “um”"},"intents":{"type":"boolean","description":"Recognizes speaker intent throughout a transcript or text"},"keywords":{"type":"string","description":"Keywords can boost or suppress specialized terminology and brands"},"language":{"type":"string","description":"The BCP-47 language tag that hints at the primary spoken language. Depending on the Model and API endpoint you choose only certain languages are available"},"measurements":{"type":"boolean","description":"Spoken measurements will be converted to their corresponding abbreviations"},"multi_channel":{"type":"boolean","description":"Transcribes each audio channel independently"},"numerals":{"type":"boolean","description":"Numerals converts numbers from written format to numerical format"},"paragraphs":{"type":"boolean","description":"Splits audio into paragraphs to improve transcript readability"},"profanity_filter":{"type":"boolean","description":"Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely"},"punctuate":{"type":"boolean","description":"Adds punctuation and capitalization to the transcript"},"search":{"type":"string","description":"Search for terms or phrases in submitted audio"},"sentiment":{"type":"boolean","description":"Recognizes the sentiment throughout a transcript or text"},"smart_format":{"type":"boolean","description":"Applies formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability"},"summarize":{"type":"string","description":"Summarizes content. For Listen API, supports string version option. For Read API, accepts boolean only."},"tag":{"type":"array","items":{"type":"string"},"description":"Labels your requests for the purpose of identification during usage reporting"},"topics":{"type":"boolean","description":"Detects topics throughout a transcript or text"},"utterances":{"type":"boolean","description":"Segments speech into meaningful semantic units"},"utt_split":{"type":"number","description":"Seconds to wait before detecting a pause between words in submitted audio"}},"required":["model"]},{"type":"object","properties":{"model":{"type":"string","enum":["#g1_nova-2-general","#g1_nova-2-meeting","#g1_nova-2-phonecall","#g1_nova-2-voicemail","#g1_nova-2-finance","#g1_nova-2-conversationalai","#g1_nova-2-video","#g1_nova-2-medical","#g1_nova-2-drivethru","#g1_nova-2-automotive","#g1_whisper-large","#g1_whisper-medium","#g1_whisper-small","#g1_whisper-tiny","#g1_whisper-base"]},"custom_intent":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}}],"description":"A custom intent you want the model to detect within your input audio if present. Submit up to 100."},"custom_topic":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}}],"description":"A custom topic you want the model to detect within your input audio if present. Submit up to 100."},"custom_intent_mode":{"type":"string","enum":["strict","extended"],"description":"Sets how the model will interpret strings submitted to the custom_intent param. When strict, the model will only return intents submitted using the custom_intent param. When extended, the model will return its own detected intents in addition those submitted using the custom_intents param."},"custom_topic_mode":{"type":"string","enum":["strict","extended"],"description":"Sets how the model will interpret strings submitted to the custom_topic param. When strict, the model will only return topics submitted using the custom_topic param. When extended, the model will return its own detected topics in addition to those submitted using the custom_topic param."},"detect_language":{"type":"boolean","description":"Enables language detection to identify the dominant language spoken in the submitted audio."},"detect_entities":{"type":"boolean","description":"When Entity Detection is enabled, the Punctuation feature will be enabled by default."},"detect_topics":{"type":"boolean","description":"Detects the most important and relevant topics that are referenced in speech within the audio"},"diarize":{"type":"boolean","description":"Recognizes speaker changes. Each word in the transcript will be assigned a speaker number starting at 0"},"dictation":{"type":"boolean","description":"Identifies and extracts key entities from content in submitted audio"},"diarize_version":{"type":"string","description":""},"extra":{"type":"string","description":"Arbitrary key-value pairs that are attached to the API response for usage in downstream processing"},"filler_words":{"type":"boolean","description":"Filler Words can help transcribe interruptions in your audio, like “uh” and “um”"},"intents":{"type":"boolean","description":"Recognizes speaker intent throughout a transcript or text"},"keywords":{"type":"string","description":"Keywords can boost or suppress specialized terminology and brands"},"language":{"type":"string","description":"The BCP-47 language tag that hints at the primary spoken language. Depending on the Model and API endpoint you choose only certain languages are available"},"measurements":{"type":"boolean","description":"Spoken measurements will be converted to their corresponding abbreviations"},"multi_channel":{"type":"boolean","description":"Transcribes each audio channel independently"},"numerals":{"type":"boolean","description":"Numerals converts numbers from written format to numerical format"},"paragraphs":{"type":"boolean","description":"Splits audio into paragraphs to improve transcript readability"},"profanity_filter":{"type":"boolean","description":"Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely"},"punctuate":{"type":"boolean","description":"Adds punctuation and capitalization to the transcript"},"search":{"type":"string","description":"Search for terms or phrases in submitted audio"},"sentiment":{"type":"boolean","description":"Recognizes the sentiment throughout a transcript or text"},"smart_format":{"type":"boolean","description":"Applies formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability"},"summarize":{"type":"string","description":"Summarizes content. For Listen API, supports string version option. For Read API, accepts boolean only."},"tag":{"type":"array","items":{"type":"string"},"description":"Labels your requests for the purpose of identification during usage reporting"},"topics":{"type":"boolean","description":"Detects topics throughout a transcript or text"},"utterances":{"type":"boolean","description":"Segments speech into meaningful semantic units"},"utt_split":{"type":"number","description":"Seconds to wait before detecting a pause between words in submitted audio"},"url":{"type":"string","format":"uri"}},"required":["model","url"]}]},"Voice.v1.SpeechToTextResponse":{"type":"object","properties":{"metadata":{"type":"object","properties":{"transaction_key":{"type":"string"},"request_id":{"type":"string"},"sha256":{"type":"string"},"created":{"type":"string","format":"date-time"},"duration":{"type":"number"},"channels":{"type":"number"},"models":{"type":"array","items":{"type":"string"}},"model_info":{"type":"object","additionalProperties":{"type":"object","properties":{"name":{"type":"string"},"version":{"type":"string"},"arch":{"type":"string"}},"required":["name","version","arch"]}}},"required":["transaction_key","request_id","sha256","created","duration","channels","models","model_info"]}},"required":["metadata"],"additionalProperties":false},"Voice.v1.SpeechToTextCreateResponseDTO":{"type":"object","properties":{"generation_id":{"type":"string","format":"uuid"}},"required":["generation_id"]},"Voice.v1.SpeechToTextGetResponseDTO":{"type":"object","properties":{"status":{"type":"string"},"result":{"type":"object","properties":{"metadata":{"type":"object","properties":{"transaction_key":{"type":"string"},"request_id":{"type":"string"},"sha256":{"type":"string"},"created":{"type":"string","format":"date-time"},"duration":{"type":"number"},"channels":{"type":"number"},"models":{"type":"array","items":{"type":"string"}},"model_info":{"type":"object","additionalProperties":{"type":"object","properties":{"name":{"type":"string"},"version":{"type":"string"},"arch":{"type":"string"}},"required":["name","version","arch"]}}},"required":["transaction_key","request_id","sha256","created","duration","channels","models","model_info"]}},"required":["metadata"],"additionalProperties":false}},"required":["status"]},"Voice.v1.TextToSpeechPayload":{"type":"object","properties":{"model":{"type":"string","enum":["#g1_aura-asteria-en","#g1_aura-hera-en","#g1_aura-luna-en","#g1_aura-stella-en","#g1_aura-athena-en","#g1_aura-zeus-en","#g1_aura-orion-en","#g1_aura-arcas-en","#g1_aura-perseus-en","#g1_aura-angus-en","#g1_aura-orpheus-en","#g1_aura-helios-en"]},"text":{"type":"string","description":"The text content to be converted to speech."},"container":{"type":"string","description":"The file format wrapper for the output audio. The available options depend on the encoding type."},"encoding":{"type":"string","enum":["linear16","mulaw","alaw","mp3","opus","flac","aac"],"default":"linear16","description":"Specifies the expected encoding of your audio output"},"sample_rate":{"type":"string","description":"The sample rate for the output audio. Based on the encoding, different sample rates are supported. For some encodings, the sample rate is not configurable"}},"required":["model","text"],"additionalProperties":false},"Voice.v1.TextToSpeechResponse":{"type":"object","properties":{"metadata":{"type":"object","properties":{"transaction_key":{"type":"string"},"request_id":{"type":"string"},"sha256":{"type":"string"},"created":{"type":"string","format":"date-time"},"duration":{"type":"number"},"channels":{"type":"number"},"models":{"type":"array","items":{"type":"string"}},"model_info":{"type":"object","additionalProperties":{"type":"object","properties":{"name":{"type":"string"},"version":{"type":"string"},"arch":{"type":"string"}},"required":["name","version","arch"]}}},"required":["transaction_key","request_id","sha256","created","duration","channels","models","model_info"]}},"required":["metadata"]},"LlmThreadMessage.v1.CreateMessageDTO":{"type":"object","properties":{"model":{"type":"string","enum":["claude-3-opus-20240229","claude-3-sonnet-20240229","claude-3-haiku-20240307","claude-3-5-sonnet-20240620","claude-3-5-sonnet-20241022","claude-3-5-haiku-20241022","claude-3-7-sonnet-20250219","anthropic/claude-3.5-sonnet-20240620","anthropic/claude-3.5-sonnet-20241022","anthropic/claude-3.5-sonnet","claude-3-5-sonnet-latest","anthropic/claude-3-haiku-20240307","anthropic/claude-3-haiku","claude-3-haiku-latest","anthropic/claude-3-opus-20240229","anthropic/claude-3-opus","claude-3-opus-latest","anthropic/claude-3-sonnet-20240229","anthropic/claude-3-sonnet","claude-3-sonnet-latest","anthropic/claude-3-5-haiku-20241022","anthropic/claude-3-5-haiku","claude-3-5-haiku-latest","claude-3-7-sonnet-latest","anthropic/claude-3.7-sonnet"]},"messages":{"type":"array","items":{"type":"object","properties":{"role":{"type":"string","enum":["user","assistant"]},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"]},"text":{"type":"string"}},"required":["type","text"],"additionalProperties":false},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"source":{"type":"object","properties":{"type":{"type":"string","enum":["base64"]},"media_type":{"type":"string","enum":["image/jpeg","image/png","image/gif","image/webp"]},"data":{"type":"string"}},"required":["type","media_type","data"]}},"required":["type","source"],"additionalProperties":false},{"type":"object","properties":{"type":{"type":"string","enum":["thinking"]},"thinking":{"type":"string"},"signature":{"type":"string"}},"required":["type","thinking","signature"],"additionalProperties":false},{"type":"object","properties":{"type":{"type":"string","enum":["tool_result"]},"tool_use_id":{"type":"string"},"is_error":{"type":"boolean"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"]},"text":{"type":"string"}},"required":["type","text"],"additionalProperties":false},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"source":{"type":"object","properties":{"type":{"type":"string","enum":["base64"]},"media_type":{"type":"string","enum":["image/jpeg","image/png","image/gif","image/webp"]},"data":{"type":"string"}},"required":["type","media_type","data"]}},"required":["type","source"],"additionalProperties":false}]}}]}},"required":["type","tool_use_id"]},{"type":"object","properties":{"id":{"type":"string"},"input":{"type":"object","additionalProperties":{"nullable":true}},"name":{"type":"string"},"type":{"type":"string","enum":["tool_use"]}},"required":["id","input","name","type"]},{"type":"object","properties":{"type":{"type":"string","enum":["redacted_thinking"]},"data":{"type":"string"}},"required":["type","data"]}]},"maxItems":5}]}},"required":["role","content"],"additionalProperties":false},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","default":512,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"description":"An object describing metadata about the request"},"stop_sequences":{"type":"array","items":{"type":"string"},"description":"Custom text sequences that will cause the model to stop generating."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"system":{"type":"string","description":"A system prompt is a way of providing context and instructions to Claude, such as specifying a particular goal or role."},"temperature":{"type":"number","minimum":0,"maximum":1,"default":1,"description":"Amount of randomness injected into the response. Defaults to 1.0. Ranges from 0.0 to 1.0. Use temperature closer to 0.0 for analytical / multiple choice, and closer to 1.0 for creative and generative tasks. Note that even with temperature of 0.0, the results will not be fully deterministic."},"tool_choice":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["any"]}},"required":["type"]},{"type":"object","properties":{"name":{"type":"string"},"type":{"type":"string","enum":["tool"]}},"required":["name","type"]},{"type":"object","properties":{"type":{"type":"string","enum":["none"]}},"required":["type"]}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"tools":{"type":"array","items":{"type":"object","properties":{"name":{"type":"string","description":"Name of the tool."},"description":{"type":"string","description":"Description of what this tool does.\n  Tool descriptions should be as detailed as possible. The more information that the model has about what the tool is and how to use it, the better it will perform. You can use natural language descriptions to reinforce important aspects of the tool input JSON schema."},"input_schema":{"type":"object","properties":{"type":{"type":"string","enum":["object"]},"properties":{"nullable":true}},"required":["type"],"additionalProperties":{"nullable":true},"description":"JSON schema for this tool's input.\n  This defines the shape of the input that your tool accepts and that the model will produce."}},"required":["name","input_schema"],"additionalProperties":false},"description":"Definitions of tools that the model may use.\n  If you include tools in your API request, the model may return tool_use content blocks that represent the model's use of those tools. You can then run those tools using the tool input generated by the model and then optionally return results back to the model using tool_result content blocks.\n  Each tool definition includes:\n      name: Name of the tool.\n      description: Optional, but strongly-recommended description of the tool.\n      input_schema: JSON schema for the tool input shape that the model will produce in tool_use output content blocks."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"top_p":{"type":"number","description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"thinking":{"type":"object","properties":{"budget_tokens":{"type":"integer","minimum":1024,"description":"Determines how many tokens Claude can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality. Must be ≥1024 and less than max_tokens."},"type":{"type":"string","enum":["enabled"]}},"required":["budget_tokens","type"],"description":"Configuration for enabling Claude's extended thinking. When enabled, responses include thinking content blocks showing Claude's thinking process before the final answer. Requires a minimum budget of 1,024 tokens and counts towards your max_tokens limit."}},"required":["model","messages"],"additionalProperties":false},"Bagoodex.v1.FetchDetailsDTO":{"type":"object","properties":{"followup_id":{"type":"string","format":"uuid"}},"required":["followup_id"]},"Bagoodex.v1.FetchImagesResponseDTO":{"type":"array","items":{"type":"object","properties":{"source":{"type":"string","nullable":true},"original":{"type":"string","nullable":true,"format":"uri"},"title":{"type":"string","nullable":true},"source_name":{"type":"string","nullable":true}}}},"Bagoodex.v1.FetchVideosResponseDTO":{"type":"array","items":{"type":"object","properties":{"link":{"type":"string","nullable":true,"format":"uri"},"thumbnail":{"type":"string","nullable":true,"format":"uri"},"title":{"type":"string","nullable":true}}}},"Bagoodex.v1.FetchKnowledgeResponseDTO":{"type":"object","properties":{"title":{"type":"string","nullable":true},"type":{"type":"string","nullable":true},"description":{"type":"string","nullable":true},"born":{"type":"string","nullable":true},"died":{"type":"string","nullable":true}}},"Bagoodex.v1.FetchWeatherResponseDTO":{"type":"object","properties":{"type":{"type":"string","nullable":true},"temperature":{"type":"string","nullable":true},"unit":{"type":"string","nullable":true},"precipitation":{"type":"string","nullable":true},"humidity":{"type":"string","nullable":true},"wind":{"type":"string","nullable":true},"location":{"type":"string","nullable":true},"date":{"type":"string","nullable":true},"weather":{"type":"string","nullable":true},"thumbnail":{"type":"string","nullable":true,"format":"uri"},"forecast":{"type":"array","nullable":true,"items":{"type":"object","properties":{"day":{"type":"string"},"temperature":{"type":"object","properties":{"high":{"type":"string"},"low":{"type":"string"}},"required":["high","low"]},"thumbnail":{"type":"string","format":"uri"},"weather":{"type":"string"},"humidity":{"type":"string"},"precipitation":{"type":"string"},"wind":{"type":"string"}},"required":["day","temperature","thumbnail","weather","humidity","precipitation","wind"]}},"hourly_forecast":{"type":"array","nullable":true,"items":{"type":"object","properties":{"time":{"type":"string"},"thumbnail":{"type":"string","format":"uri"},"weather":{"type":"string"},"temperature":{"type":"string"},"precipitation":{"type":"string"},"humidity":{"type":"string"},"wind":{"type":"string"}},"required":["time","thumbnail","weather","temperature","precipitation","humidity","wind"]}},"precipitation_forecast":{"type":"array","nullable":true,"items":{"type":"object","properties":{"precipitation":{"type":"string"},"day":{"type":"string"},"time":{"type":"string"}},"required":["precipitation","day","time"]}},"wind_forecast":{"type":"array","nullable":true,"items":{"type":"object","properties":{"angle":{"type":"number"},"direction":{"type":"string"},"speed":{"type":"string"},"time":{"type":"string"}},"required":["angle","direction","speed","time"]}},"sources":{"type":"array","nullable":true,"items":{"type":"object","properties":{"title":{"type":"string"},"link":{"type":"string","format":"uri"}},"required":["title","link"]}}}},"Bagoodex.v1.FetchLocalMapResponseDTO":{"type":"object","properties":{"link":{"type":"string","nullable":true},"image":{"type":"string","nullable":true,"format":"uri"},"gps_coordinates":{"type":"object","nullable":true,"properties":{"latitude":{"type":"number"},"longitude":{"type":"number"}},"required":["latitude","longitude"]}}},"Bagoodex.v1.FetchLinksResponseDTO":{"type":"array","items":{"type":"string","format":"uri"}},"LumaAi.v1.FetchGenerationsByIdsPayload":{"type":"object","properties":{"ids":{"anyOf":[{"type":"array","items":{"type":"string","format":"uuid"},"minItems":1},{"type":"string"}],"description":"Array of UUID strings or string with comma-separated UUID strings"}},"required":["ids"],"additionalProperties":false},"LumaAi.v1.CreateGenerationPayload":{"type":"object","properties":{"model":{"type":"string","enum":["ray-1-6"],"default":"ray-1-6"},"aspect_ratio":{"type":"string","enum":["1:1","16:9","9:16","4:3","3:4","21:9","9:21"],"description":"The aspect ratio of the image","example":"16:9"},"expand_prompt":{"type":"boolean","default":false,"description":"Whether to expand the prompt","example":true},"image_end_url":{"type":"string","format":"uri","description":"The URL for the end of the image","example":"https://example.com/image-end.jpg"},"image_url":{"type":"string","format":"uri","description":"The URL of the main image","example":"https://example.com/main-image.jpg"},"user_prompt":{"type":"string","description":"The user-provided prompt for image generation","example":"A beautiful sunset over the ocean"}},"required":["aspect_ratio","user_prompt"]},"LumaAi.v1.ExtendGenerationPayload":{"allOf":[{"$ref":"#/components/schemas/LumaAi.v1.CreateGenerationPayload"},{"type":"object","properties":{}}]},"LumaAi.v2.FetchGenerationPayload":{"type":"object","properties":{"generation_id":{"type":"string","format":"uuid"},"state":{"type":"string","enum":["queued","dreaming","completed"]}},"required":["generation_id"]},"LumaAi.v2.FetchGenerationsPayload":{"type":"object","properties":{"generation_ids":{"anyOf":[{"type":"array","items":{"type":"string","format":"uuid"},"minItems":1},{"type":"string"}]},"status":{"type":"string","enum":["queued","dreaming","completed"]}},"required":["generation_ids"]},"LumaAi.v2.CreateGenerationPayload":{"type":"object","properties":{"generation_type":{"type":"string","nullable":true,"enum":["video"]},"prompt":{"type":"string"},"aspect_ratio":{"type":"string","enum":["1:1","16:9","9:16","4:3","3:4","21:9","9:21"]},"loop":{"type":"boolean","default":false},"keyframes":{"type":"object","nullable":true,"properties":{"frame0":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["generation"]},"id":{"type":"string","format":"uuid"}},"required":["type","id"],"additionalProperties":false},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"url":{"type":"string","format":"uri"}},"required":["type","url"],"additionalProperties":false},{"nullable":true}]},"frame1":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["generation"]},"id":{"type":"string","format":"uuid"}},"required":["type","id"],"additionalProperties":false},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"url":{"type":"string","format":"uri"}},"required":["type","url"],"additionalProperties":false},{"nullable":true}]}}},"callback_url":{"type":"string","nullable":true,"format":"uri"},"model":{"type":"string","enum":["ray-1-6"],"default":"ray-1-6"},"resolution":{"nullable":true},"duration":{"type":"string","nullable":true},"concepts":{"type":"string","nullable":true}},"required":["prompt","aspect_ratio"],"additionalProperties":false},"Runway.v2.GenerateVideoPayloadDTO":{"oneOf":[{"$ref":"#/components/schemas/Runway.v2.gen3Payload"},{"$ref":"#/components/schemas/Runway.v2.gen4Payload"}],"discriminator":{"propertyName":"model","mapping":{"gen3a_turbo":"#/components/schemas/Runway.v2.gen3Payload","runway/gen4_turbo":"#/components/schemas/Runway.v2.gen4Payload"}}},"Runway.v2.gen3Payload":{"type":"object","properties":{"model":{"type":"string","enum":["gen3a_turbo"]},"prompt":{"type":"string","maxLength":1000,"description":"The text description of the scene, subject, or action to generate in the video."},"image_url":{"type":"string","format":"uri","description":"A HTTPS URL or data URI containing an encoded image to be used as the first frame of the generated video."},"last_image_url":{"type":"string","format":"uri","description":"A HTTPS URL or data URI containing an encoded image to be used as the last frame of the generated video."},"duration":{"type":"integer","description":"The length of the output video in seconds.","enum":[5,10]},"ratio":{"type":"string","enum":["16:9","9:16"],"description":"The aspect ratio of the generated video."},"seed":{"type":"integer","minimum":0,"maximum":4294967295,"description":"Varying the seed integer is a way to get different results for the same other request parameters. Using the same value for an identical request will produce similar results. If unspecified, a random number is chosen."}},"required":["model","image_url"]},"Runway.v2.gen4Payload":{"type":"object","properties":{"model":{"type":"string","enum":["runway/gen4_turbo"]},"prompt":{"type":"string","maxLength":1000,"description":"The text description of the scene, subject, or action to generate in the video."},"image_url":{"type":"string","format":"uri","description":"A HTTPS URL or data URI containing an encoded image to be used as the first frame of the generated video."},"duration":{"type":"integer","description":"The length of the output video in seconds.","enum":[5,10]},"ratio":{"type":"string","enum":["16:9","9:16","4:3","3:4","1:1","21:9"],"default":"16:9","description":"The aspect ratio of the generated video."},"seed":{"type":"integer","minimum":0,"maximum":4294967295,"description":"Varying the seed integer is a way to get different results for the same other request parameters. Using the same value for an identical request will produce similar results. If unspecified, a random number is chosen."}},"required":["model","image_url"]},"Runway.v2.GenerateVideoResponsedDTO":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Generation ID","example":"a12b3456-7c89-0de1-23f4-g567d584f98d"},"status":{"type":"string","enum":["queued","generating","completed","error"],"description":"Generation status"}},"required":["id","status"]},"Runway.v2.PollVideoDTO":{"type":"object","properties":{"generation_id":{"type":"string","format":"uuid","description":"Generation ID","example":"a12b3456-7c89-0de1-23f4-g567d584f98d"}},"required":["generation_id"]},"Runway.v2.PollGenerationResponseDTO":{"allOf":[{"$ref":"#/components/schemas/Runway.v2.GenerateVideoResponsedDTO"},{"type":"object","properties":{"video":{"type":"array","items":{"type":"string","nullable":true,"format":"uri"},"description":"If the status is success, this will contain an array of strings. Each string will be a URL that returns an output from the task. URLs expire within 24-48 hours."}},"required":["video"]}]},"Kling.v2.SubmitGenerationPayloadDTO":{"anyOf":[{"$ref":"#/components/schemas/Kling.v2.klingV1StandardTextToVideo"},{"$ref":"#/components/schemas/Kling.v2.klingV1StandardImageToVideo"},{"$ref":"#/components/schemas/Kling.v2.klingV1ProImageToVideo"},{"$ref":"#/components/schemas/Kling.v2.klingV16StandardImageToVideo"},{"$ref":"#/components/schemas/Kling.v2.klingV16ProImageToVideo"},{"$ref":"#/components/schemas/Kling.v2.klingTextToVideoPayload"},{"$ref":"#/components/schemas/Kling.v2.klingV2MasterImageToVideoPayload"},{"type":"object","properties":{"model":{"type":"string","enum":["klingai/kling-video-v1.6-pro-effects","klingai/kling-video-v1.6-standard-effects"]},"duration":{"type":"integer","description":"The length of the output video in seconds.","enum":[5,10]},"image_url":{"anyOf":[{"type":"string","format":"uri"},{"type":"array","items":{"type":"string","format":"uri"}}],"description":"For hug, kiss, and heart_gesture effects, pass an array containing exactly two image URLs. For squish or expansion, only one image URL is required."},"effect_scene":{"type":"string","enum":["hug","kiss","heart_gesture","squish","expansion"],"description":"The effect scene to use for the video generation."}},"required":["model","image_url","effect_scene"]}]},"Kling.v2.klingV1StandardTextToVideo":{"type":"object","properties":{"model":{"type":"string","enum":["kling-video/v1/standard/text-to-video"]},"ratio":{"type":"string","enum":["16:9","9:16","1:1"],"deprecated":true},"aspect_ratio":{"type":"string","enum":["16:9","9:16","1:1"],"description":"The aspect ratio of the generated video."},"camera_control":{"type":"string","enum":["down_back","forward_up","right_turn_forward","left_turn_forward"],"description":"Camera control parameters."},"advanced_camera_control":{"type":"object","properties":{"movement_type":{"type":"string","enum":["horizontal","vertical","pan","tilt","roll","zoom"],"description":"The type of camera movement."},"movement_value":{"type":"integer","minimum":-10,"maximum":10,"description":"The value of the camera movement."}},"required":["movement_type","movement_value"],"description":"Advanced camera control parameters."},"prompt":{"type":"string","description":"The text description of the scene, subject, or action to generate in the video."},"duration":{"type":"integer","description":"The length of the output video in seconds.","enum":[5,10]},"negative_prompt":{"type":"string","description":"The description of elements to avoid in the generated video."},"cfg_scale":{"type":"number","minimum":0,"maximum":1,"description":"The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."}},"required":["model","prompt"]},"Kling.v2.klingV1StandardImageToVideo":{"type":"object","properties":{"model":{"type":"string","enum":["kling-video/v1/standard/image-to-video"]},"tail_image_url":{"type":"string","format":"uri"},"static_mask_url":{"type":"string","format":"uri","description":"URL of the image for Static Brush Application Area (Mask image created by users using the motion brush)."},"dynamic_masks":{"type":"array","items":{"type":"object","properties":{"mask_url":{"type":"string","format":"uri","description":"URL of the image for Dynamic Brush Application Area (Mask image created by users using the motion brush)."},"trajectories":{"type":"array","items":{"type":"object","properties":{"x":{"type":"integer","description":"X coordinate of the motion trajectory."},"y":{"type":"integer","description":"Y coordinate of the motion trajectory."}},"required":["x","y"]},"description":"List of trajectories."}},"required":["mask_url"]},"description":"List of dynamic masks."},"image_url":{"type":"string","format":"uri"},"ratio":{"type":"string","enum":["16:9","9:16","1:1"],"deprecated":true},"aspect_ratio":{"type":"string","enum":["16:9","9:16","1:1"],"deprecated":true},"prompt":{"type":"string","description":"The text description of the scene, subject, or action to generate in the video."},"duration":{"type":"integer","description":"The length of the output video in seconds.","enum":[5,10]},"negative_prompt":{"type":"string","description":"The description of elements to avoid in the generated video."},"cfg_scale":{"type":"number","minimum":0,"maximum":1,"description":"The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."}},"required":["model","image_url","prompt"]},"Kling.v2.klingV1ProImageToVideo":{"type":"object","properties":{"model":{"type":"string","enum":["kling-video/v1/pro/image-to-video"]},"tail_image_url":{"type":"string","format":"uri"},"static_mask_url":{"type":"string","format":"uri","description":"URL of the image for Static Brush Application Area (Mask image created by users using the motion brush)."},"dynamic_masks":{"type":"array","items":{"type":"object","properties":{"mask_url":{"type":"string","format":"uri","description":"URL of the image for Dynamic Brush Application Area (Mask image created by users using the motion brush)."},"trajectories":{"type":"array","items":{"type":"object","properties":{"x":{"type":"integer","description":"X coordinate of the motion trajectory."},"y":{"type":"integer","description":"Y coordinate of the motion trajectory."}},"required":["x","y"]},"description":"List of trajectories."}},"required":["mask_url"]},"description":"List of dynamic masks."},"image_url":{"type":"string","format":"uri"},"ratio":{"type":"string","enum":["16:9","9:16","1:1"],"deprecated":true},"aspect_ratio":{"type":"string","enum":["16:9","9:16","1:1"],"deprecated":true},"prompt":{"type":"string","description":"The text description of the scene, subject, or action to generate in the video."},"duration":{"type":"integer","description":"The length of the output video in seconds.","enum":[5,10]},"negative_prompt":{"type":"string","description":"The description of elements to avoid in the generated video."},"cfg_scale":{"type":"number","minimum":0,"maximum":1,"description":"The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."}},"required":["model","image_url","prompt"]},"Kling.v2.klingV16StandardImageToVideo":{"type":"object","properties":{"model":{"type":"string","enum":["kling-video/v1.6/standard/image-to-video"]},"image_url":{"type":"string","format":"uri"},"ratio":{"type":"string","enum":["16:9","9:16","1:1"],"deprecated":true},"aspect_ratio":{"type":"string","enum":["16:9","9:16","1:1"],"deprecated":true},"prompt":{"type":"string","description":"The text description of the scene, subject, or action to generate in the video."},"duration":{"type":"integer","description":"The length of the output video in seconds.","enum":[5,10]},"negative_prompt":{"type":"string","description":"The description of elements to avoid in the generated video."},"cfg_scale":{"type":"number","minimum":0,"maximum":1,"description":"The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."}},"required":["model","image_url","prompt"]},"Kling.v2.klingV16ProImageToVideo":{"type":"object","properties":{"model":{"type":"string","enum":["kling-video/v1.6/pro/image-to-video"]},"tail_image_url":{"type":"string","format":"uri"},"image_url":{"type":"string","format":"uri"},"ratio":{"type":"string","enum":["16:9","9:16","1:1"],"deprecated":true},"aspect_ratio":{"type":"string","enum":["16:9","9:16","1:1"],"deprecated":true},"prompt":{"type":"string","description":"The text description of the scene, subject, or action to generate in the video."},"duration":{"type":"integer","description":"The length of the output video in seconds.","enum":[5,10]},"negative_prompt":{"type":"string","description":"The description of elements to avoid in the generated video."},"cfg_scale":{"type":"number","minimum":0,"maximum":1,"description":"The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."}},"required":["model","image_url","prompt"]},"Kling.v2.klingTextToVideoPayload":{"type":"object","properties":{"model":{"type":"string","enum":["kling-video/v1/pro/text-to-video","kling-video/v1.6/standard/text-to-video","kling-video/v1.6/pro/text-to-video","klingai/v2-master-text-to-video"]},"ratio":{"type":"string","enum":["16:9","9:16","1:1"],"deprecated":true},"aspect_ratio":{"type":"string","enum":["16:9","9:16","1:1"],"description":"The aspect ratio of the generated video."},"prompt":{"type":"string","description":"The text description of the scene, subject, or action to generate in the video."},"duration":{"type":"integer","description":"The length of the output video in seconds.","enum":[5,10]},"negative_prompt":{"type":"string","description":"The description of elements to avoid in the generated video."},"cfg_scale":{"type":"number","minimum":0,"maximum":1,"description":"The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."}},"required":["model","prompt"]},"Kling.v2.klingV2MasterImageToVideoPayload":{"type":"object","properties":{"model":{"type":"string","enum":["klingai/v2-master-image-to-video"]},"image_url":{"type":"string","format":"uri"},"prompt":{"type":"string","description":"The text description of the scene, subject, or action to generate in the video."},"duration":{"type":"integer","description":"The length of the output video in seconds.","enum":[5,10]},"negative_prompt":{"type":"string","description":"The description of elements to avoid in the generated video."},"cfg_scale":{"type":"number","minimum":0,"maximum":1,"description":"The CFG (Classifier Free Guidance) scale is a measure of how close you want the model to stick to your prompt."}},"required":["model","image_url","prompt"]},"Kling.v2.PollGenerationPayloadDTO":{"type":"object","properties":{"generation_id":{"type":"string"}},"required":["generation_id"]},"Minimax.v2.CreateGenerationVideoDTO":{"oneOf":[{"type":"object","properties":{"model":{"type":"string","enum":["video-01-live2d"],"description":"Model id"},"prompt":{"type":"string","maxLength":2000,"description":"The text description of the scene, subject, or action to generate in the video."},"prompt_optimizer":{"type":"boolean","default":true,"description":"If True, the incoming prompt will be automatically optimized to improve generation quality when needed. For more precise control, set it to False — the model will then follow the instructions more strictly."},"first_frame_image":{"type":"string","format":"uri","description":"The model will use the image passed in this parameter as the first frame to generate a video. \n      Supported formats: \n      - URL of the image;\n      - base64 encoding of the image.\n      Image specifications:\n      - format must be JPG, JPEG, or PNG;\n      - aspect ratio should be greater than 2:5 and less than 5:2; the shorter side must exceed 300 pixels;\n      - file size must not exceed 20MB."}},"required":["model","prompt","first_frame_image"]},{"type":"object","properties":{"model":{"type":"string","enum":["video-01"]},"prompt":{"type":"string","maxLength":2000,"description":"The text description of the scene, subject, or action to generate in the video."},"prompt_optimizer":{"type":"boolean","default":true,"description":"If True, the incoming prompt will be automatically optimized to improve generation quality when needed. For more precise control, set it to False — the model will then follow the instructions more strictly."},"first_frame_image":{"type":"string","format":"uri","description":"The model will use the image passed in this parameter as the first frame to generate a video. \n      Supported formats: \n      - URL of the image;\n      - base64 encoding of the image.\n      Image specifications:\n      - format must be JPG, JPEG, or PNG;\n      - aspect ratio should be greater than 2:5 and less than 5:2; the shorter side must exceed 300 pixels;\n      - file size must not exceed 20MB."}},"required":["model","prompt"]}]},"Minimax.v2.CreateGenerationVideoResponseDTO":{"type":"object","properties":{"generation_id":{"type":"string","description":"Generation ID","example":"222226666699999"},"status":{"type":"string","nullable":true}},"required":["generation_id"]},"Minimax.v2.FetchVideoDTO":{"type":"object","properties":{"generation_id":{"type":"string","description":"Generation ID","example":"222226666699999"}},"required":["generation_id"]},"Minimax.v2.FetchVideoResponseDTO":{"type":"object","properties":{"id":{"type":"string","description":"Generation ID","example":"222226666699999"},"status":{"type":"string","enum":["queued","generating","completed","error"],"description":"Generation status"},"video":{"type":"object","nullable":true,"properties":{"url":{"type":"string","nullable":true,"format":"uri","description":"The URL you get to download the video"}}}},"required":["id","status"]},"Alibaba.v2.SubmitGenerationPayloadDTO":{"type":"object","properties":{"model":{"type":"string","enum":["wan/v2.1/1.3b/text-to-video"]},"prompt":{"type":"string","description":"The text description of the scene, subject, or action to generate in the video.","example":"Mona Lisa puts on glasses with her hands"},"negative_prompt":{"type":"string","description":"The description of elements to avoid in the generated video."},"seed":{"type":"integer","description":"Varying the seed integer is a way to get different results for the same other request parameters. Using the same value for an identical request will produce similar results. If unspecified, a random number is chosen."},"aspect_ratio":{"type":"string","enum":["9:16","16:9"],"default":"16:9","description":"The aspect ratio of the generated video."},"inference_steps":{"type":"integer","default":30,"description":"Number of inference steps for sampling. Higher values give better quality but take longer."},"guidance_scale":{"type":"number","default":5,"description":"Classifier-free guidance scale. Controls prompt adherence / creativity."},"shift":{"type":"number","default":5,"description":"Noise schedule shift parameter. Affects temporal dynamics."},"sampler":{"type":"string","enum":["unipc","dpm+"],"default":"unipc","description":"The sampler to use for generation."},"enable_safety_checker":{"type":"boolean","description":"If set to true, the safety checker will be enabled."},"enable_prompt_expansion":{"type":"boolean","description":"Whether to enable prompt expansion."}},"required":["model","prompt"]},"Alibaba.v2.PollGenerationPayloadDTO":{"type":"object","properties":{"generation_id":{"type":"string"}},"required":["generation_id"]},"Google.v2.SubmitGenerationPayloadDTO":{"anyOf":[{"type":"object","properties":{"model":{"type":"string","enum":["veo2/image-to-video"]},"prompt":{"type":"string","description":"The text description of the scene, subject, or action to generate in the video."},"image_url":{"type":"string","format":"uri"},"aspect_ratio":{"type":"string","enum":["auto","16:9","9:16"],"description":"The aspect ratio of the generated video."},"duration":{"type":"integer","description":"The length of the output video in seconds.","enum":[5,6,7,8]}},"required":["model","prompt","image_url"]},{"type":"object","properties":{"model":{"type":"string","enum":["veo2"]},"prompt":{"type":"string","description":"The text description of the scene, subject, or action to generate in the video."},"aspect_ratio":{"type":"string","enum":["16:9","9:16"],"description":"The aspect ratio of the generated video."},"duration":{"type":"integer","description":"The length of the output video in seconds.","enum":[5,6,7,8]}},"required":["model","prompt"]}]},"Google.v2.PollGenerationPayloadDTO":{"type":"object","properties":{"generation_id":{"type":"string"}},"required":["generation_id"]},"Audio.v2.SubmitGenerationPayloadDTO":{"anyOf":[{"type":"object","properties":{"model":{"type":"string","enum":["stable-audio"]},"prompt":{"type":"string","description":"The prompt to generate audio."},"seconds_start":{"type":"integer","maximum":47,"minimum":1,"description":"The start point of the audio clip to generate."},"seconds_total":{"type":"integer","maximum":47,"minimum":1,"default":30,"description":"The duration of the audio clip to generate."},"steps":{"type":"integer","minimum":1,"maximum":1000,"default":100,"description":"The number of steps to denoise the audio."}},"required":["model","prompt"]},{"type":"object","properties":{"model":{"type":"string","enum":["minimax-music"]},"prompt":{"type":"string","description":"Lyrics with optional formatting. You can use a newline to separate each line of lyrics. You can use two newlines to add a pause between lines. You can use double hash marks (##) at the beginning and end of the lyrics to add accompaniment. Maximum 600 characters."},"reference_audio_url":{"type":"string","format":"uri","description":"Reference song, should contain music and vocals. Must be a .wav or .mp3 file longer than 15 seconds."}},"required":["model","prompt","reference_audio_url"]}]},"Audio.v2.SubmitAudioResponseDTO":{"type":"object","properties":{"id":{"type":"string"},"status":{"type":"string","enum":["queued","completed","generating","error"]}},"required":["id","status"]},"Audio.v2.PollGenerationPayloadDTO":{"type":"object","properties":{"generation_id":{"type":"string"}},"required":["generation_id"]},"Audio.v2.PollAudioResponseDTO":{"type":"object","properties":{"audio_file":{"type":"object","properties":{"url":{"type":"string","format":"uri"}},"required":["url"]},"id":{"type":"string"},"status":{"type":"string","enum":["queued","completed","generating","error"]},"error":{"nullable":true}},"required":["id","status"]},"Minimax.v2.UploadPayloadDTO":{"type":"object","properties":{"purpose":{"type":"string","enum":["song","voice","instrumental"],"description":"1. If purpose is song:\n  - You need to upload a music file containing both acapella (vocals) and accompaniment.\n  - The acapella must be in singing form; normal speech is not supported.\n  - Outputs: voice_id and instrumental_id.\n  2. If purpose is voice:\n  - You need to upload a file containing only acapella in singing form (normal speech audio is not supported).\n  - Output: voice_id.\n  3. If purpose is instrumental:\n  - You need to upload a file containing only accompaniment.\n  - Output: instrumental_id."}},"required":["purpose"]},"Minimax.v2.UploadResponsePayloadDTO":{"type":"object","properties":{"voice_id":{"type":"string","description":"The voice_id will only be returned when the purpose is song or voice.","example":"vocal-2025011003141025-d5ZEMxmp"},"instrumental_id":{"type":"string","description":"The instrumental_id will only be returned when the purpose is song or instrumental.","example":"instrumental-2025011003141125-Akz9eWnD"},"base_resp":{"type":"object","properties":{"status_code":{"type":"integer"},"status_msg":{"type":"string"}},"required":["status_code","status_msg"]}},"required":["base_resp"],"additionalProperties":false},"Minimax.v2.CreateGenerationPayloadDTO":{"allOf":[{"type":"object","properties":{"lyrics":{"type":"string","description":"Lyrics with optional formatting. You can use a newline to separate each line of lyrics. You can use two newlines to add a pause between lines. You can use double hash marks (##) at the beginning and end of the lyrics to add accompaniment. Maximum 600 characters.","example":"##Swift and Boundless \n In the realm of innovation, where visions align, \n\nAIML API's the name, making tech shine. \nIntelligent solutions, breaking the mold, \n\nSwift inference power, bold and untold.\n##"},"model":{"type":"string","enum":["music-01"]},"audio_setting":{"type":"object","properties":{"sample_rate":{"anyOf":[{"type":"number","enum":[16000]},{"type":"number","enum":[24000]},{"type":"number","enum":[32000]},{"type":"number","enum":[44100]}],"description":"The sampling rate of the generated music."},"bitrate":{"anyOf":[{"type":"number","enum":[32000]},{"type":"number","enum":[64000]},{"type":"number","enum":[128000]},{"type":"number","enum":[256000]}],"description":"The bit rate of the generated music."},"format":{"type":"string","enum":["mp3","wav","pcm"],"description":"The format of the generated music."}},"required":["sample_rate","bitrate","format"]}},"required":["lyrics","model"]},{"type":"object","properties":{"refer_voice":{"type":"string","description":"voice_id.\n  At least one of refer_voice or refer_instrumental is required. When only refer_voice is provided, the system can still output music data. The generated music will be an a cappella vocal hum that aligns with the provided refer_voice and the generated lyrics, without any instrumental accompaniment.","example":"vocal-2025010100000000-a0AAAaaa"},"refer_instrumental":{"type":"string","description":"instrumental_id.\n  At least one of refer_voice or refer_instrumental is required. When only refer_instrumental is provided, the system can still output music data. The generated music will be a purely instrumental track that aligns with the provided refer_instrumental, without any vocals.","example":"instrumental-2025010100000000-Aaa0aAaA"}}}]},"Minimax.v2.GenerateAudioResponseDTO":{"type":"object","properties":{"data":{"type":"object","properties":{"status":{"type":"integer","description":"Music generation status. 1: In progress; 2: Completed."},"audio":{"type":"string","description":"Hex-encoded audio data. Currently, can generate music with a duration of no more than 1 minute."}},"required":["status","audio"]},"extra_info":{"type":"object","properties":{"audio_length":{"type":"integer"},"audio_size":{"type":"integer"},"audio_bitrate":{"type":"integer"},"audio_sample_rate":{"type":"integer"}},"required":["audio_length","audio_size","audio_bitrate","audio_sample_rate"]},"trace_id":{"type":"string"},"base_resp":{"type":"object","properties":{"status_code":{"type":"integer"},"status_msg":{"type":"string"}},"required":["status_code","status_msg"]}},"required":["base_resp"]},"Vision.v1.OCRPayloadDTO":{"anyOf":[{"$ref":"#/components/schemas/Vision.v1.OCRGoogleRequestDTO"},{"$ref":"#/components/schemas/Vision.v1.OCRMistralRequestDTO"}]},"Vision.v1.OCRGoogleRequestDTO":{"type":"object","properties":{"model":{"type":"string","enum":["google/gc-document-ai"],"default":"google/gc-document-ai","deprecated":true},"document":{"anyOf":[{"type":"string","format":"uri"},{"type":"string"}],"description":"The document file to be processed by the OCR model."},"mimeType":{"type":"string","enum":["application/pdf","image/gif","image/tiff","image/jpeg","image/png","image/bmp","image/webp","text/html"],"description":"The MIME type of the document."},"pages":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["start"]},"start":{"type":"integer","minimum":1}},"required":["type","start"]},{"type":"object","properties":{"type":{"type":"string","enum":["end"]},"end":{"type":"integer","minimum":1}},"required":["type","end"]},{"type":"object","properties":{"type":{"type":"string","enum":["range"]},"start":{"type":"integer","minimum":1},"end":{"type":"integer","minimum":2}},"required":["type","start","end"]},{"type":"object","properties":{"type":{"type":"string","enum":["indices"]},"indices":{"type":"array","items":{"type":"integer","minimum":1},"maxItems":15}},"required":["type","indices"]}],"description":"Specific pages you wants to process"}},"required":["document"],"additionalProperties":false},"Vision.v1.OCRMistralRequestDTO":{"type":"object","properties":{"model":{"type":"string","enum":["mistral/mistral-ocr-latest"],"default":"mistral/mistral-ocr-latest","description":"Model ID"},"document":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["document_url"],"description":"Type of document."},"document_url":{"type":"string","format":"uri","description":"Document URL."}},"required":["type","document_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["image_url"],"description":"Image URL."},"image_url":{"type":"string","format":"uri","description":"Type of document."}},"required":["type","image_url"]}],"description":"Document to run OCR"},"pages":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"integer"}},{"nullable":true}],"description":"Specific pages you wants to process","example":"\"3\" or \"0-2\" or [0, 3, 4]"},"include_image_base64":{"type":"boolean","nullable":true,"description":"Include base64 images in response"},"image_limit":{"type":"integer","nullable":true,"description":"Max images to extract"},"image_min_size":{"type":"integer","nullable":true,"description":"Minimum height and width of image to extract"}},"required":["document"]},"Vision.v1.VisionRequestDTO":{"type":"object","properties":{"image":{"type":"object","properties":{"source":{"type":"object","properties":{"imageUri":{"type":"string","description":"The URI of the source image."}},"required":["imageUri"],"additionalProperties":false}},"required":["source"],"additionalProperties":false,"description":"The image to be processed."},"features":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["FACE_DETECTION","LANDMARK_DETECTION","LOGO_DETECTION","LABEL_DETECTION","TEXT_DETECTION","DOCUMENT_TEXT_DETECTION","SAFE_SEARCH_DETECTION","IMAGE_PROPERTIES","CROP_HINTS","WEB_DETECTION","PRODUCT_SEARCH","OBJECT_LOCALIZATION"],"description":"The feature type."},"maxResults":{"type":"number","description":"Maximum number of results of this type."},"model":{"type":"string","enum":["builtin/stable","builtin/latest"],"description":"Model to use for the feature."}},"required":["type"],"additionalProperties":false},"description":"Requested features."},"imageContext":{"type":"object","properties":{"latLongRect":{"type":"object","properties":{"minLatLng":{"type":"object","properties":{"latitude":{"type":"number","description":"The latitude in degrees. It must be in the range [-90.0, +90.0]."},"longitude":{"type":"number","description":"The longitude in degrees. It must be in the range [-180.0, +180.0]."}},"required":["latitude","longitude"],"additionalProperties":false,"description":"Min latitude-longitude pair."},"maxLatLng":{"type":"object","properties":{"latitude":{"type":"number","description":"The latitude in degrees. It must be in the range [-90.0, +90.0]."},"longitude":{"type":"number","description":"The longitude in degrees. It must be in the range [-180.0, +180.0]."}},"required":["latitude","longitude"],"additionalProperties":false,"description":"Max latitude-longitude pair."}},"required":["minLatLng","maxLatLng"],"additionalProperties":false,"description":"Rectangle determined by min and max LatLng (latitude-longitude) pairs."},"languageHints":{"type":"array","items":{"type":"string"},"description":"List of languages to use for TEXT_DETECTION. In most cases, an empty value yields the best results since it enables automatic language detection. For languages based on the Latin alphabet, setting languageHints is not needed. In rare cases, when the language of the text in the image is known, setting a hint will help get better results (although it will be a significant hindrance if the hint is wrong)."},"cropHintsParams":{"type":"object","properties":{"aspectRatios":{"type":"array","items":{"type":"number"},"description":"Aspect ratios in floats, representing the ratio of the width to the height of the image. For example, if the desired aspect ratio is 4/3, the corresponding float value should be 1.33333. If not specified, the best possible crop is returned. The number of provided aspect ratios is limited to a maximum of 16; any aspect ratios provided after the 16th are ignored."}},"required":["aspectRatios"],"additionalProperties":false,"description":"Parameters for crop hints annotation request."},"faceRecognitionParams":{"type":"object","properties":{"celebritySet":{"type":"array","items":{"type":"string"}}},"required":["celebritySet"],"additionalProperties":false,"description":"Parameters for face recognition"},"textDetectionParams":{"type":"object","properties":{"enableTextDetectionConfidenceScore":{"type":"boolean","description":"By default, Cloud Vision API only includes confidence score for DOCUMENT_TEXT_DETECTION result. Set the flag to true to include confidence score for TEXT_DETECTION as well."}},"required":["enableTextDetectionConfidenceScore"],"additionalProperties":false,"description":"Parameters for text detection and document text detection."}},"additionalProperties":false,"description":"Additional context that may accompany the image."}},"required":["image","features"]},"Llm.v1.ChatCompletionPayload":{"oneOf":[{"type":"object","properties":{"model":{"type":"string","enum":["meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo","google/gemma-2-27b-it","meta-llama/Llama-Vision-Free","mistralai/Mixtral-8x22B-Instruct-v0.1","Qwen/Qwen2-72B-Instruct","mistralai/Mixtral-8x7B-Instruct-v0.1","nvidia/Llama-3.1-Nemotron-70B-Instruct-HF","NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO","meta-llama/Llama-3.3-70B-Instruct-Turbo","meta-llama/Llama-3.2-3B-Instruct-Turbo","meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo","meta-llama/Llama-Guard-3-11B-Vision-Turbo","Qwen/Qwen2.5-7B-Instruct-Turbo","Qwen/Qwen2.5-Coder-32B-Instruct","meta-llama/Meta-Llama-3-8B-Instruct-Lite","meta-llama/Llama-3-8b-chat-hf","meta-llama/Llama-3-70b-chat-hf","Qwen/Qwen2.5-72B-Instruct-Turbo","Qwen/QwQ-32B","meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo","meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo","meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo","mistralai/Mistral-7B-Instruct-v0.2","meta-llama/LlamaGuard-2-8b","mistralai/Mistral-7B-Instruct-v0.1","mistralai/Mistral-7B-Instruct-v0.3","Gryphe/MythoMax-L2-13b-Lite","meta-llama/Meta-Llama-Guard-3-8B","meta-llama/llama-4-scout","meta-llama/llama-4-maverick","Qwen/Qwen3-235B-A22B-fp8-tput"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message — in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false,"description":"Developer-provided instructions that the model should follow, regardless of messages sent by the user. With o1 models and newer, use developer messages for this purpose instead."},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message — in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"],"description":"The type of the content part."},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data."},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. "}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false,"description":"Messages sent by an end user, containing prompts or additional context information."},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the messages author: tool.The role of the author of the message — in this case, the tool"},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."}},"required":["role","content","tool_call_id"],"additionalProperties":false,"description":""},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false,"deprecated":true},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message — in this case, the Assistant."},"content":{"type":"string","description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","description":"The refusal message by the Assistant."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false,"description":"Messages sent by the model in response to user messages."}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"default":512,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"],"description":"Additional parameters for configuring the streaming mode."},"n":{"type":"integer","minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"temperature":{"type":"number","description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"repetition_penalty":{"type":"number","nullable":true,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"min_p":{"type":"number","minimum":0,"maximum":1,"description":"A number between 0 and 1 that can be used as an alternative to top_p and top_k."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"required":["description","name"],"description":""}},"required":["type","function"]},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"Structured Outputs configuration options, including a JSON Schema."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"]},{"type":"object","properties":{"model":{"type":"string","enum":["claude-3-opus-20240229","claude-3-sonnet-20240229","claude-3-haiku-20240307","claude-3-5-sonnet-20240620","claude-3-5-sonnet-20241022","claude-3-5-haiku-20241022","claude-3-7-sonnet-20250219","anthropic/claude-3.5-sonnet-20240620","anthropic/claude-3.5-sonnet-20241022","anthropic/claude-3.5-sonnet","claude-3-5-sonnet-latest","anthropic/claude-3-haiku-20240307","anthropic/claude-3-haiku","claude-3-haiku-latest","anthropic/claude-3-opus-20240229","anthropic/claude-3-opus","claude-3-opus-latest","anthropic/claude-3-sonnet-20240229","anthropic/claude-3-sonnet","claude-3-sonnet-latest","anthropic/claude-3-5-haiku-20241022","anthropic/claude-3-5-haiku","claude-3-5-haiku-latest","claude-3-7-sonnet-latest","anthropic/claude-3.7-sonnet"]},"messages":{"type":"array","items":{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"],"additionalProperties":false},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"source":{"type":"object","properties":{"type":{"type":"string","enum":["base64"],"description":"The type of the image."},"media_type":{"type":"string","enum":["image/jpeg","image/png","image/gif","image/webp"],"description":"The media type of the image."},"data":{"type":"string","description":"The base64 encoded image data."}},"required":["type","media_type","data"]}},"required":["type","source"]},{"type":"object","properties":{"type":{"type":"string","enum":["tool_result"]},"tool_use_id":{"type":"string"},"is_error":{"type":"boolean"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"],"additionalProperties":false},{"type":"object","properties":{"type":{"type":"string","enum":["image"]},"source":{"type":"object","properties":{"type":{"type":"string","enum":["base64"],"description":"The type of the image."},"media_type":{"type":"string","enum":["image/jpeg","image/png","image/gif","image/webp"],"description":"The media type of the image."},"data":{"type":"string","description":"The base64 encoded image data."}},"required":["type","media_type","data"]}},"required":["type","source"]}]}}]}},"required":["type","tool_use_id"]},{"type":"object","properties":{"id":{"type":"string"},"input":{"type":"object","additionalProperties":{"nullable":true}},"name":{"type":"string"},"type":{"type":"string","enum":["tool_use"]}},"required":["id","input","name","type"]},{"type":"object","properties":{"type":{"type":"string","enum":["thinking"]},"thinking":{"type":"string"},"signature":{"type":"string"}},"required":["type","thinking","signature"],"additionalProperties":false},{"type":"object","properties":{"type":{"type":"string","enum":["redacted_thinking"]},"data":{"type":"string"}},"required":["type","data"]}]}}]},"role":{"type":"string","enum":["user","assistant"],"description":"The role of the messages author"}},"required":["content","role"]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stop_sequences":{"type":"array","items":{"type":"string"},"description":"Custom text sequences that will cause the model to stop generating."},"max_tokens":{"type":"number","minimum":1,"default":512,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"frequency_penalty":{"type":"number","description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"top_p":{"type":"number","description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"top_k":{"type":"number","description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"description":"An object describing metadata about the request"},"temperature":{"type":"number","minimum":0,"maximum":1,"description":"Amount of randomness injected into the response. Defaults to 1.0. Ranges from 0.0 to 1.0. Use temperature closer to 0.0 for analytical / multiple choice, and closer to 1.0 for creative and generative tasks. Note that even with temperature of 0.0, the results will not be fully deterministic."},"tools":{"type":"array","items":{"type":"object","properties":{"name":{"type":"string","description":"Name of the tool."},"description":{"type":"string","description":"Description of what this tool does.\n  Tool descriptions should be as detailed as possible. The more information that the model has about what the tool is and how to use it, the better it will perform. You can use natural language descriptions to reinforce important aspects of the tool input JSON schema."},"input_schema":{"type":"object","properties":{"type":{"type":"string","enum":["object"]},"properties":{"nullable":true}},"required":["type"],"additionalProperties":{"nullable":true},"description":"JSON schema for this tool's input.\n  This defines the shape of the input that your tool accepts and that the model will produce."}},"required":["name","input_schema"],"additionalProperties":false},"description":"Definitions of tools that the model may use.\n  If you include tools in your API request, the model may return tool_use content blocks that represent the model's use of those tools. You can then run those tools using the tool input generated by the model and then optionally return results back to the model using tool_result content blocks.\n  Each tool definition includes:\n      name: Name of the tool.\n      description: Optional, but strongly-recommended description of the tool.\n      input_schema: JSON schema for the tool input shape that the model will produce in tool_use output content blocks."},"tool_choice":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["auto"]}},"required":["type"]},{"type":"object","properties":{"type":{"type":"string","enum":["any"]}},"required":["type"]},{"type":"object","properties":{"name":{"type":"string"},"type":{"type":"string","enum":["tool"]}},"required":["name","type"]},{"type":"object","properties":{"type":{"type":"string","enum":["none"]}},"required":["type"]}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"system":{"type":"string","description":"A system prompt is a way of providing context and instructions to Claude, such as specifying a particular goal or role."},"thinking":{"type":"object","properties":{"budget_tokens":{"type":"integer","minimum":1024,"description":"Determines how many tokens Claude can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality. Must be ≥1024 and less than max_tokens."},"type":{"type":"string","enum":["enabled"]}},"required":["budget_tokens","type"],"description":"Configuration for enabling Claude's extended thinking. When enabled, responses include thinking content blocks showing Claude's thinking process before the final answer. Requires a minimum budget of 1,024 tokens and counts towards your max_tokens limit."}},"required":["model","messages"],"additionalProperties":false},{"type":"object","properties":{"model":{"type":"string","enum":["o1"]},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"default":512,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"max_completion_tokens":{"type":"integer","minimum":1,"default":512,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message — in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message — in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data."},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. "}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. \n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB.\n        - Maximum size per file: Up to 512 MB.\n        - Text and document files: Up to 2 million tokens per file.\n        - Spreadsheets (CSV, XLSX): Up to approximately 50 MB per file, depending on the number and size of rows."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the messages author: tool.The role of the author of the message — in this case, the tool"},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message — in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"function_call":{"nullable":true,"description":"Deprecated and replaced by tool_calls. The name and arguments of a function that should be called, as generated by the model.","deprecated":true},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message — in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","enum":[false],"default":false,"deprecated":true},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"],"description":"Additional parameters for configuring the streaming mode.","deprecated":true},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."},"required":{"type":"array","items":{"type":"string"}}},"required":["name"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."}},"required":["model","messages"]},{"type":"object","properties":{"model":{"type":"string","enum":["openai/gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","gpt-4o-mini","gpt-4o-mini-2024-07-18","chatgpt-4o-latest","gpt-4-turbo","gpt-4-turbo-2024-04-09","gpt-4","gpt-4-0125-preview","gpt-4-1106-preview","gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106","o1-preview","o1-preview-2024-09-12","o1-mini","o1-mini-2024-09-12","o3-mini","gpt-4.5-preview","gpt-4o-audio-preview","gpt-4o-mini-audio-preview","gpt-4o-search-preview","gpt-4o-mini-search-preview","openai/gpt-4.1-2025-04-14","openai/gpt-4.1-mini-2025-04-14","openai/gpt-4.1-nano-2025-04-14","openai/o4-mini-2025-04-16"]},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"default":512,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"max_completion_tokens":{"type":"integer","minimum":1,"default":512,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message — in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message — in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data."},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. "}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. \n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB.\n        - Maximum size per file: Up to 512 MB.\n        - Text and document files: Up to 2 million tokens per file.\n        - Spreadsheets (CSV, XLSX): Up to approximately 50 MB per file, depending on the number and size of rows."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the messages author: tool.The role of the author of the message — in this case, the tool"},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message — in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"function_call":{"nullable":true,"description":"Deprecated and replaced by tool_calls. The name and arguments of a function that should be called, as generated by the model.","deprecated":true},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message — in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."},"required":{"type":"array","items":{"type":"string"}}},"required":["name"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."}},"required":["model","messages"]},{"type":"object","properties":{"model":{"type":"string","enum":["google/gemini-1.5-flash","google/gemini-1.5-pro","google/gemini-2.0-flash-exp","google/gemini-2.0-flash-thinking-exp-01-21","google/gemini-2.5-pro-exp-03-25","google/gemini-2.0-flash","google/gemini-2.5-pro-preview-05-06","google/gemini-2.5-pro-preview","google/gemini-2.5-flash-preview"]},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"max_tokens":{"type":"number","minimum":1,"default":512,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message — in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false,"description":"Developer-provided instructions that the model should follow, regardless of messages sent by the user. With o1 models and newer, use developer messages for this purpose instead."},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message — in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data."},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. "}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false,"description":"Messages sent by an end user, containing prompts or additional context information."},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the messages author: tool.The role of the author of the message — in this case, the tool"},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."}},"required":["role","content","tool_call_id"],"additionalProperties":false,"description":""},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message — in this case, the Assistant."},"content":{"type":"string","description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"additionalProperties":false,"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","description":"The refusal message by the Assistant."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false,"description":"Messages sent by the model in response to user messages."}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."}},"required":["name"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"],"description":""}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."}},"required":["model","messages"]},{"type":"object","properties":{"model":{"type":"string","enum":["mistralai/mistral-tiny","x-ai/grok-beta","mistralai/mistral-nemo","neversleep/llama-3.1-lumimaid-70b","anthracite-org/magnum-v4-72b","nvidia/llama-3.1-nemotron-70b-instruct","cohere/command-r-plus","ai21/jamba-1-5-mini","mistralai/codestral-2501","google/gemma-3-1b-it","google/gemma-3-4b-it","google/gemma-3-12b-it","google/gemma-3-27b-it","x-ai/grok-3-beta","x-ai/grok-3-mini-beta","deepseek/deepseek-prover-v2"]},"top_k":{"type":"integer","minimum":0,"description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","minimum":0,"maximum":2,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"min_p":{"type":"number","minimum":0,"maximum":1,"description":"A number between 0 and 1 that can be used as an alternative to top_p and top_k."},"top_a":{"type":"number","minimum":0,"maximum":1,"description":"Alternate top sampling parameter."},"reasoning":{"type":"object","properties":{"effort":{"type":"string","enum":["low","medium","high"],"description":"Reasoning effort setting"},"max_tokens":{"type":"integer","minimum":1,"description":"Max tokens of reasoning content. Cannot be used simultaneously with effort."},"exclude":{"type":"boolean","description":"Whether to exclude reasoning from the response"}},"description":"Configuration for model reasoning/thinking tokens"},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"default":512,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"max_completion_tokens":{"type":"integer","minimum":1,"default":512,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message — in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message — in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data."},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. "}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. \n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB.\n        - Maximum size per file: Up to 512 MB.\n        - Text and document files: Up to 2 million tokens per file.\n        - Spreadsheets (CSV, XLSX): Up to approximately 50 MB per file, depending on the number and size of rows."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the messages author: tool.The role of the author of the message — in this case, the tool"},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message — in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"function_call":{"nullable":true,"description":"Deprecated and replaced by tool_calls. The name and arguments of a function that should be called, as generated by the model.","deprecated":true},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message — in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."},"required":{"type":"array","items":{"type":"string"}}},"required":["name"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."}},"required":["model","messages"]},{"type":"object","properties":{"model":{"type":"string","enum":["MiniMax-Text-01","abab6.5s-chat"]},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"]},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}]},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"]},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data."}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}]}}]},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false,"description":"Messages sent by an end user, containing prompts or additional context information."},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the messages author: tool.The role of the author of the message — in this case, the tool"},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."}},"required":["role","content","tool_call_id"],"additionalProperties":false,"description":""},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"]},"content":{"type":"string"},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]}},"refusal":{"type":"string"},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false,"description":"Messages sent by the model in response to user messages."}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"max_tokens":{"type":"number","minimum":1,"default":256,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"temperature":{"type":"number","minimum":0,"maximum":1,"default":0.1,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.01,"maximum":1,"default":0.95,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"mask_sensitive_info":{"type":"boolean","default":false,"description":"Mask (replace with ***) content in the output that involves private information, including but not limited to email, domain, link, ID number, home address, etc. Defaults to False, i.e. enable masking."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."}},"required":["name"]}},"required":["type","function"]},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false}],"description":"An object specifying the format that the model must output."}},"required":["model","messages"]},{"type":"object","properties":{"model":{"type":"string","enum":["deepseek-chat","deepseek-reasoner","deepseek/deepseek-chat","deepseek/deepseek-chat-v3-0324","deepseek/deepseek-r1"]},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"default":512,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"max_completion_tokens":{"type":"integer","minimum":1,"default":512,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message — in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message — in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data."},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. "}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. \n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB.\n        - Maximum size per file: Up to 512 MB.\n        - Text and document files: Up to 2 million tokens per file.\n        - Spreadsheets (CSV, XLSX): Up to approximately 50 MB per file, depending on the number and size of rows."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the messages author: tool.The role of the author of the message — in this case, the tool"},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message — in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"function_call":{"nullable":true,"description":"Deprecated and replaced by tool_calls. The name and arguments of a function that should be called, as generated by the model.","deprecated":true},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message — in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."},"required":{"type":"array","items":{"type":"string"}}},"required":["name"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."}},"required":["model","messages"]},{"type":"object","properties":{"model":{"type":"string","enum":["qwen-max","qwen-plus","qwen-turbo","qwen-max-2025-01-25"]},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"default":512,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message — in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message — in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data."},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. "}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. \n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB.\n        - Maximum size per file: Up to 512 MB.\n        - Text and document files: Up to 2 million tokens per file.\n        - Spreadsheets (CSV, XLSX): Up to approximately 50 MB per file, depending on the number and size of rows."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the messages author: tool.The role of the author of the message — in this case, the tool"},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message — in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"function_call":{"nullable":true,"description":"Deprecated and replaced by tool_calls. The name and arguments of a function that should be called, as generated by the model.","deprecated":true},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message — in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."},"required":{"type":"array","items":{"type":"string"}}},"required":["name"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."}},"required":["model","messages"]},{"type":"object","properties":{"model":{"type":"string","enum":["bagoodex/bagoodex-search-v1"]},"messages":{"type":"array","items":{"type":"object","properties":{"role":{"type":"string","enum":["user","assistant"]},"content":{"type":"string"}},"required":["role","content"]}},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"default":512,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."}],"description":"An object specifying the format that the model must output."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}]},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."},"required":{"type":"array","items":{"type":"string"}}},"required":["name"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"deprecated":true},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"]},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"]}],"deprecated":true},"user":{"type":"string"},"best_of":{"type":"integer","nullable":true,"minimum":1},"use_beam_search":{"type":"boolean","nullable":true},"top_k":{"type":"integer","nullable":true},"min_p":{"type":"number","nullable":true},"repetition_penalty":{"type":"number","nullable":true},"length_penalty":{"type":"number","nullable":true},"early_stopping":{"type":"boolean","nullable":true},"ignore_eos":{"type":"boolean","nullable":true},"min_tokens":{"type":"integer","nullable":true},"stop_token_ids":{"type":"array","nullable":true,"items":{"type":"integer"}},"skip_special_tokens":{"type":"boolean","nullable":true},"spaces_between_special_tokens":{"nullable":true},"echo":{"type":"boolean","nullable":true,"description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"add_generation_prompt":{"type":"boolean","nullable":true,"description":"If True, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model."},"add_special_tokens":{"type":"boolean","nullable":true,"description":"If True, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to False (as is the default)."},"documents":{"type":"array","nullable":true,"items":{"type":"object","additionalProperties":{"type":"string"}},"description":"'A list of dicts representing documents that will be accessible to the model if it is performing RAG (retrieval-augmented generation). If the template does not support RAG, this argument will have no effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys."},"chat_template":{"type":"string","nullable":true,"description":"A Jinja template to use for this conversion. If this is not passed, the model's default chat template will be used instead."},"chat_template_kwargs":{"type":"object","nullable":true,"additionalProperties":{"nullable":true},"description":"Additional kwargs to pass to the template renderer. Will be accessible by the chat template"},"include_stop_str_in_output":{"type":"boolean","nullable":true,"description":"Whether to include the stop string in the output. This is only applied when the stop or stop_token_ids is set"},"guided_json":{"anyOf":[{"type":"string"},{"type":"object","additionalProperties":{"nullable":true}},{"nullable":true}],"description":"If specified, the output will follow the JSON schema."},"guided_regex":{"type":"string","nullable":true,"description":"If specified, the output will follow the regex pattern."},"guided_choice":{"type":"array","nullable":true,"items":{"type":"string"},"description":"If specified, the output will be exactly one of the choices."},"guided_grammar":{"type":"string","nullable":true,"description":"If specified, the output will follow the context free grammar."},"guided_decoding_backend":{"type":"string","nullable":true,"enum":["outlines","lm-format-enforcer"],"description":"If specified, will override the default guided decoding backend of the server for this specific request. If set, must be either 'outlines' / 'lm-format-enforcer'"},"guided_whitespace_pattern":{"type":"string","nullable":true,"description":"If specified, will override the default whitespace pattern for guided json decoding."},"ip":{"type":"string","format":"ip","description":"IP from which a request is executed"}},"required":["model","messages"]}]},"Llm.v2.CompleteChatPayload":{"type":"object","properties":{"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","nullable":true,"additionalProperties":{"type":"number","minimum":-100,"maximum":100},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."},"logprobs":{"type":"boolean","nullable":true,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"top_logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":20,"description":"An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to True if this parameter is used."},"max_tokens":{"type":"number","minimum":1,"default":512,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"max_completion_tokens":{"type":"integer","minimum":1,"default":512,"description":"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens."},"n":{"type":"integer","nullable":true,"minimum":1,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"prediction":{"type":"object","properties":{"type":{"type":"string","enum":["content"],"description":"The type of the predicted content you want to provide."},"content":{"anyOf":[{"type":"string","description":"The content used for a Predicted Output. This is often the text of a file you are regenerating with minor changes."},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},"description":"An array of content parts with a defined type. Supported options differ based on the model being used to generate the response. Can contain text inputs."}],"description":"The content that should be matched when generating a model response. If generated tokens would match this content, the entire model response can be returned much more quickly."}},"required":["type","content"],"description":"Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"seed":{"type":"integer","minimum":1,"description":"This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result."},"messages":{"type":"array","items":{"oneOf":[{"type":"object","properties":{"role":{"type":"string","enum":["system"],"description":"The role of the author of the message — in this case, the system."},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the system message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["user"],"description":"The role of the author of the message — in this case, the user"},"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["image_url"]},"image_url":{"type":"object","properties":{"url":{"type":"string","format":"uri","description":"Either a URL of the image or the base64 encoded image data."},"detail":{"type":"string","enum":["low","high","auto"],"description":"Specifies the detail level of the image. "}},"required":["url"]}},"required":["type","image_url"]},{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"type":{"type":"string","enum":["input_audio"],"description":"The type of the content part."},"input_audio":{"type":"object","properties":{"data":{"type":"string","description":"Base64 encoded audio data."},"format":{"type":"string","enum":["wav","mp3"],"description":"The format of the encoded audio data. Currently supports \"wav\" and \"mp3\"."}},"required":["data","format"]}},"required":["type","input_audio"]},{"type":"object","properties":{"type":{"type":"string","enum":["file"],"description":"The type of the content part."},"file":{"type":"object","properties":{"file_data":{"type":"string","description":"The file data, encoded in base64 and passed to the model as a string. \n        - Maximum number of files: Up to 20 files can be attached to a single GPT application or Assistant. This limit applies throughout the application's lifetime.\n        - Maximum total file storage per user: 10 GB.\n        - Maximum size per file: Up to 512 MB.\n        - Text and document files: Up to 2 million tokens per file.\n        - Spreadsheets (CSV, XLSX): Up to approximately 50 MB per file, depending on the number and size of rows."},"filename":{"type":"string","description":"The file name specified by the user. This name can be used to reference the file when interacting with the model, especially if multiple files are uploaded."}}}},"required":["type","file"]}]}}],"description":"The contents of the user message."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["tool"],"description":"The role of the messages author: tool.The role of the author of the message — in this case, the tool"},"content":{"type":"string","description":"The contents of the tool message."},"tool_call_id":{"type":"string","description":"Tool call that this message is responding to."},"name":{"type":"string","nullable":true,"description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role","content","tool_call_id"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["function"]},"content":{"type":"string"},"name":{"type":"string"}},"required":["role","content","name"],"additionalProperties":false},{"type":"object","properties":{"role":{"type":"string","enum":["assistant"],"description":"The role of the author of the message — in this case, the Assistant."},"content":{"anyOf":[{"type":"string","description":"The contents of the Assistant message."},{"type":"array","items":{"anyOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]},{"type":"object","properties":{"refusal":{"type":"string","description":"The refusal message generated by the model."},"type":{"type":"string","enum":["refusal"],"description":"The type of the content part."}},"required":["refusal","type"]}]},"description":"An array of content parts with a defined type. Can be one or more of type text, or exactly one of type refusal."}],"description":"The contents of the Assistant message. Required unless tool_calls or function_call is specified."},"tool_calls":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the tool call."},"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"name":{"type":"string","description":"The name of the function to call."},"arguments":{"type":"string","description":"The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."}},"required":["name","arguments"],"description":"The function that the model called."}},"required":["id","type","function"]},"description":"The tool calls generated by the model, such as function calls."},"refusal":{"type":"string","nullable":true,"description":"The refusal message by the Assistant."},"audio":{"type":"object","nullable":true,"properties":{"id":{"type":"string","description":"Unique identifier for a previous audio response from the model."}},"required":["id"],"description":"Data about a previous audio response from the model."},"function_call":{"nullable":true,"description":"Deprecated and replaced by tool_calls. The name and arguments of a function that should be called, as generated by the model.","deprecated":true},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["role"],"additionalProperties":false},{"type":"object","properties":{"content":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of the content part."},"text":{"type":"string","description":"The text content."}},"required":["type","text"]}}],"description":"The contents of the developer message."},"role":{"type":"string","enum":["developer"],"description":"The role of the author of the message — in this case, the developer."},"name":{"type":"string","description":"An optional name for the participant. Provides the model information to differentiate between participants of the same role."}},"required":["content","role"],"additionalProperties":false}]},"description":"A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, documents (txt, pdf), images, and audio."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"stream_options":{"type":"object","properties":{"include_usage":{"type":"boolean"}},"required":["include_usage"]},"top_p":{"type":"number","minimum":0.1,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"temperature":{"type":"number","minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"stop":{"anyOf":[{"type":"string"},{"type":"array","items":{"type":"string"}},{"nullable":true}],"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"tools":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["function"],"description":"The type of the tool. Currently, only function is supported."},"function":{"type":"object","properties":{"description":{"type":"string","description":"A description of what the function does, used by the model to choose when and how to call the function."},"name":{"type":"string","description":"The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"parameters":{"nullable":true,"description":"The parameters the functions accepts, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the function call. If set to True, the model will follow the exact schema defined in the parameters field. Only a subset of JSON Schema is supported when strict is True."},"required":{"type":"array","items":{"type":"string"}}},"required":["name"],"additionalProperties":false}},"required":["type","function"],"additionalProperties":false},"description":"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported."},"tool_choice":{"anyOf":[{"type":"string","enum":["none","auto","required"],"description":"none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools."},{"type":"object","properties":{"type":{"type":"string","enum":["function"]},"function":{"type":"object","properties":{"name":{"type":"string"}},"required":["name"]}},"required":["type","function"],"description":"Specifies a tool the model should use. Use to force the model to call a specific function."}],"description":"Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}} forces the model to call that tool.\n  none is the default when no tools are present. auto is the default if tools are present."},"parallel_tool_calls":{"type":"boolean","description":"Whether to enable parallel function calling during tool use."},"reasoning_effort":{"type":"string","enum":["low","medium","high"],"description":"Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response."},"response_format":{"oneOf":[{"type":"object","properties":{"type":{"type":"string","enum":["text"],"description":"The type of response format being defined. Always text."}},"required":["type"],"additionalProperties":false,"description":"Default response format. Used to generate text responses."},{"type":"object","properties":{"type":{"type":"string","enum":["json_object"],"description":"The type of response format being defined. Always json_object."}},"required":["type"],"additionalProperties":false,"description":"An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so."},{"type":"object","properties":{"type":{"type":"string","enum":["json_schema"],"description":"The type of response format being defined. Always json_schema."},"json_schema":{"type":"object","properties":{"name":{"type":"string","description":"The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."},"schema":{"type":"object","additionalProperties":{"nullable":true},"description":"The schema for the response format, described as a JSON Schema object."},"strict":{"type":"boolean","nullable":true,"description":"Whether to enable strict schema adherence when generating the output. If set to True, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is True."},"description":{"type":"string","description":"A description of what the response format is for, used by the model to determine how to respond in the format."}},"required":["name"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}},"required":["type","json_schema"],"additionalProperties":false,"description":"JSON Schema response format. Used to generate structured JSON responses."}],"description":"An object specifying the format that the model must output."},"audio":{"type":"object","nullable":true,"properties":{"format":{"type":"string","enum":["wav","mp3","flac","opus","pcm16"],"description":"Specifies the output audio format. Must be one of wav, mp3, flac, opus, or pcm16."},"voice":{"type":"string","enum":["alloy","ash","ballad","coral","echo","fable","nova","onyx","sage","shimmer"],"description":"The voice the model uses to respond. Supported voices are alloy, ash, ballad, coral, echo, fable, nova, onyx, sage, and shimmer."}},"required":["format","voice"],"description":"Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]."},"modalities":{"type":"array","nullable":true,"items":{"type":"string","enum":["text","audio"]},"description":"Output types that you would like the model to generate. Most models are capable of generating text, which is the default:\n  \n  [\"text\"]\n  \n  The gpt-4o-audio-preview model can also be used to generate audio. To request that this model generate both text and audio responses, you can use:\n  \n  [\"text\", \"audio\"]"},"web_search_options":{"type":"object","properties":{"search_context_size":{"type":"string","enum":["low","medium","high"],"description":"High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default."},"user_location":{"type":"object","nullable":true,"properties":{"approximate":{"type":"object","properties":{"city":{"type":"string","description":"Free text input for the city of the user, e.g. San Francisco."},"country":{"type":"string","description":"The two-letter ISO country code of the user, e.g. US."},"region":{"type":"string","description":"Free text input for the region of the user, e.g. California."},"timezone":{"type":"string","description":"The IANA timezone of the user, e.g. America/Los_Angeles."}},"description":"Approximate location parameters for the search."},"type":{"type":"string","enum":["approximate"],"description":"The type of location approximation. Always approximate."}},"required":["approximate","type"],"description":"Approximate location parameters for the search."}},"description":"This tool searches the web for relevant results to use in a response."},"model":{"type":"string","enum":["meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo","google/gemma-2-27b-it","meta-llama/Llama-Vision-Free","mistralai/Mixtral-8x22B-Instruct-v0.1","Qwen/Qwen2-72B-Instruct","mistralai/Mixtral-8x7B-Instruct-v0.1","nvidia/Llama-3.1-Nemotron-70B-Instruct-HF","NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO","meta-llama/Llama-3.3-70B-Instruct-Turbo","meta-llama/Llama-3.2-3B-Instruct-Turbo","meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo","meta-llama/Llama-Guard-3-11B-Vision-Turbo","Qwen/Qwen2.5-7B-Instruct-Turbo","Qwen/Qwen2.5-Coder-32B-Instruct","meta-llama/Meta-Llama-3-8B-Instruct-Lite","meta-llama/Llama-3-8b-chat-hf","meta-llama/Llama-3-70b-chat-hf","Qwen/Qwen2.5-72B-Instruct-Turbo","Qwen/QwQ-32B","meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo","meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo","meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo","mistralai/Mistral-7B-Instruct-v0.2","meta-llama/LlamaGuard-2-8b","mistralai/Mistral-7B-Instruct-v0.1","mistralai/Mistral-7B-Instruct-v0.3","Gryphe/MythoMax-L2-13b-Lite","meta-llama/Meta-Llama-Guard-3-8B","meta-llama/llama-4-scout","meta-llama/llama-4-maverick","Qwen/Qwen3-235B-A22B-fp8-tput","o1","openai/gpt-4o","gpt-4o-2024-08-06","gpt-4o-2024-05-13","gpt-4o-mini","gpt-4o-mini-2024-07-18","chatgpt-4o-latest","gpt-4-turbo","gpt-4-turbo-2024-04-09","gpt-4","gpt-4-0125-preview","gpt-4-1106-preview","gpt-3.5-turbo","gpt-3.5-turbo-0125","gpt-3.5-turbo-1106","o1-preview","o1-preview-2024-09-12","o1-mini","o1-mini-2024-09-12","o3-mini","gpt-4.5-preview","gpt-4o-audio-preview","gpt-4o-mini-audio-preview","gpt-4o-search-preview","gpt-4o-mini-search-preview","openai/gpt-4.1-2025-04-14","openai/gpt-4.1-mini-2025-04-14","openai/gpt-4.1-nano-2025-04-14","openai/o4-mini-2025-04-16","claude-3-opus-20240229","claude-3-sonnet-20240229","claude-3-haiku-20240307","claude-3-5-sonnet-20240620","claude-3-5-sonnet-20241022","claude-3-5-haiku-20241022","claude-3-7-sonnet-20250219","anthropic/claude-3.5-sonnet-20240620","anthropic/claude-3.5-sonnet-20241022","anthropic/claude-3.5-sonnet","claude-3-5-sonnet-latest","anthropic/claude-3-haiku-20240307","anthropic/claude-3-haiku","claude-3-haiku-latest","anthropic/claude-3-opus-20240229","anthropic/claude-3-opus","claude-3-opus-latest","anthropic/claude-3-sonnet-20240229","anthropic/claude-3-sonnet","claude-3-sonnet-latest","anthropic/claude-3-5-haiku-20241022","anthropic/claude-3-5-haiku","claude-3-5-haiku-latest","claude-3-7-sonnet-latest","anthropic/claude-3.7-sonnet","google/gemini-1.5-flash","google/gemini-1.5-pro","google/gemini-2.0-flash-exp","google/gemini-2.0-flash-thinking-exp-01-21","google/gemini-2.5-pro-exp-03-25","google/gemini-2.0-flash","google/gemini-2.5-pro-preview-05-06","google/gemini-2.5-pro-preview","google/gemini-2.5-flash-preview","mistralai/mistral-tiny","x-ai/grok-beta","mistralai/mistral-nemo","neversleep/llama-3.1-lumimaid-70b","anthracite-org/magnum-v4-72b","nvidia/llama-3.1-nemotron-70b-instruct","cohere/command-r-plus","ai21/jamba-1-5-mini","mistralai/codestral-2501","google/gemma-3-1b-it","google/gemma-3-4b-it","google/gemma-3-12b-it","google/gemma-3-27b-it","x-ai/grok-3-beta","x-ai/grok-3-mini-beta","deepseek/deepseek-prover-v2","MiniMax-Text-01","abab6.5s-chat","deepseek-chat","deepseek-reasoner","deepseek/deepseek-chat","deepseek/deepseek-chat-v3-0324","deepseek/deepseek-r1","qwen-max","qwen-plus","qwen-turbo","qwen-max-2025-01-25","bagoodex/bagoodex-search-v1"]}},"required":["messages","model"]},"Llm.v1.CompleteLanguageDTO":{"oneOf":[{"type":"object","properties":{"prompt":{"type":"string"},"model":{"type":"string","enum":["gpt-3.5-turbo-instruct","mistralai/Mixtral-8x7B-v0.1","mistralai/Mistral-7B-v0.1","NousResearch/Nous-Hermes-13b","togethercomputer/llama-2-7b","huggyllama/llama-7b","WizardLM/WizardLM-70B-V1.0","huggyllama/llama-65b","togethercomputer/llama-2-13b","togethercomputer/llama-2-70b","huggyllama/llama-13b","huggyllama/llama-30b","EleutherAI/llemma_7b","meta-llama/Llama-3-70b-hf","meta-llama/Meta-Llama-3.1-8B-Reference","meta-llama/Meta-Llama-3.1-70B-Reference"]},"max_tokens":{"type":"number","minimum":1,"maximum":8000,"description":"The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API."},"stop":{"type":"array","items":{"type":"string"},"description":"Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."},"temperature":{"type":"number","nullable":true,"minimum":0,"maximum":2,"description":"What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."},"top_p":{"type":"number","nullable":true,"minimum":0,"maximum":1,"description":"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n  We generally recommend altering this or temperature but not both."},"top_k":{"type":"number","minimum":1,"description":"Only sample from the top K options for each subsequent token. Used to remove \"long tail\" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature."},"repetition_penalty":{"type":"number","nullable":true,"minimum":0,"description":"A number that controls the diversity of generated text by reducing the likelihood of repeated sequences. Higher values decrease repetition."},"stream":{"type":"boolean","default":false,"description":"If set to True, the model response data will be streamed to the client as it is generated using server-sent events."},"logprobs":{"type":"number","nullable":true,"minimum":0,"maximum":1,"description":"Whether to return log probabilities of the output tokens or not. If True, returns the log probabilities of each output token returned in the content of message."},"echo":{"type":"boolean","description":"If True, the response will contain the prompt. Can be used with logprobs to return prompt logprobs."},"n":{"type":"number","minimum":1,"maximum":128,"description":"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs."},"safety_model":{"type":"string"},"min_p":{"type":"number","nullable":true,"description":"A number between 0 and 1 that can be used as an alternative to top_p and top_k."},"presence_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."},"frequency_penalty":{"type":"number","nullable":true,"minimum":-2,"maximum":2,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."},"logit_bias":{"type":"object","additionalProperties":{"type":"number"},"description":"Modify the likelihood of specified tokens appearing in the completion.\n  \n  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."}},"required":["prompt","model"]}]},"Embedding.v1.CreateEmbeddingsDTO":{"oneOf":[{"type":"object","properties":{"model":{"type":"string","enum":["text-embedding-3-small","text-embedding-3-large","text-embedding-ada-002"]},"input":{"anyOf":[{"type":"string","minLength":1},{"type":"array","items":{"type":"string"},"minItems":1}],"description":"Input text to embed, encoded as a string or array of tokens."},"encoding_format":{"type":"string","nullable":true,"enum":["float","base64"],"default":"float","description":"The format in which to return the embeddings."},"dimensions":{"type":"number","nullable":true,"description":"The number of dimensions the resulting output embeddings should have."}},"required":["model","input"],"additionalProperties":false},{"type":"object","properties":{"model":{"type":"string","enum":["voyage-large-2-instruct","voyage-finance-2","voyage-multilingual-2","voyage-law-2","voyage-code-2","voyage-large-2","voyage-2"]},"input":{"anyOf":[{"type":"string","minLength":1,"maxLength":8000},{"type":"array","items":{"type":"string","maxLength":800}}],"description":"Input text to embed, encoded as a string or array of tokens."},"input_type":{"type":"string","enum":["document"],"default":"document","description":"The type of input data for the model."}},"required":["model","input"]},{"type":"object","properties":{"model":{"type":"string","enum":["togethercomputer/m2-bert-80M-32k-retrieval","BAAI/bge-base-en-v1.5","togethercomputer/m2-bert-80M-2k-retrieval","BAAI/bge-large-en-v1.5","togethercomputer/m2-bert-80M-8k-retrieval"]},"input":{"type":"string","minLength":1,"maxLength":3000,"description":"Input text to embed, encoded as a string or array of tokens."}},"required":["model","input"]},{"type":"object","properties":{"model":{"type":"string","enum":["textembedding-gecko@003","textembedding-gecko-multilingual@001","text-multilingual-embedding-002"]},"input":{"anyOf":[{"type":"string","minLength":1},{"type":"array","items":{"type":"string"},"minItems":1}],"description":"Input text to embed, encoded as a string or array of tokens."},"dimensions":{"type":"number","nullable":true,"description":"The number of dimensions the resulting output embeddings should have."},"auto_truncate":{"type":"boolean","default":true,"description":"If enabled, this parameter automatically truncates the input text to fit within the model’s maximum token limit. It helps ensure that longer texts are processed without errors."},"task_type":{"type":"string","enum":["RETRIEVAL_QUERY","RETRIEVAL_DOCUMENT","SEMANTIC_SIMILARITY","CLASSIFICATION","CLUSTERING","QUESTION_ANSWERING","FACT_VERIFICATION"],"description":"Optional task type for which the embeddings will be used"},"title":{"type":"string","description":"An optional title for the text. Only applicable when task_type is RETRIEVAL_DOCUMENT.\n  \n  Note: Specifying a title for RETRIEVAL_DOCUMENT provides better quality embeddings for retrieval."}},"required":["model","input"]}]},"FileApi.v1.UploadFileDTO":{"type":"object","properties":{"purpose":{"type":"string","enum":["assistants"],"description":"The intended purpose of the uploaded file"}},"required":["purpose"]},"FileApi.v1.UploadFileResponseDTO":{"type":"object","properties":{"bytes":{"type":"integer","description":"The size of the file, in bytes"},"created_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when the file was created."},"filename":{"type":"string","description":"The name of the file."},"expires_at":{"type":"integer","nullable":true,"description":"The Unix timestamp (in seconds) for when the file will expire."},"id":{"type":"string","description":"The file identifier, which can be referenced in the API endpoints"},"object":{"type":"string","enum":["file"],"description":"The object type"},"purpose":{"type":"string","enum":["assistants"],"description":"The intended purpose of the file"}},"required":["bytes","created_at","filename","id","object","purpose"]},"FileApi.v1.GetFileResponseDTO":{"type":"object","properties":{"bytes":{"type":"integer","description":"The size of the file, in bytes"},"created_at":{"type":"integer","description":"The Unix timestamp (in seconds) for when the file was created."},"filename":{"type":"string","description":"The name of the file."},"expires_at":{"type":"integer","nullable":true,"description":"The Unix timestamp (in seconds) for when the file will expire."},"id":{"type":"string","description":"The file identifier, which can be referenced in the API endpoints"},"object":{"type":"string","enum":["file"],"description":"The object type"},"purpose":{"type":"string","enum":["assistants"],"description":"The intended purpose of the file"}},"required":["bytes","created_at","filename","id","object","purpose"]},"FileApi.v1.DeleteFileResponseDTO":{"type":"object","properties":{"id":{"type":"string"},"object":{"type":"string"},"deleted":{"type":"boolean"}},"required":["id","object","deleted"]}}}}